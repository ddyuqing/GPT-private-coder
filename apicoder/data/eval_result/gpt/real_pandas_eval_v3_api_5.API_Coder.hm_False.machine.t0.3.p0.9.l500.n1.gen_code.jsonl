{"task_id": "PandasEval/0", "success": true, "generated_code": "import pandas as pd\n\ndef compute_mean_along_rows(df):\n    # You can specify a new column named `mean_along_rows` that contains the mean of each row. You also need to compute the mean along the rows, so use axis=1.\n    # Finally, return the dataframe with the new column. \n    df['mean_along_rows'] = df.mean(axis=1)\n    return df", "error": "", "token_usage": [[355, 89]], "time_usage": [2.8971822261810303]}
{"task_id": "PandasEval/1", "success": true, "generated_code": "# [start]\n# isin(self, values, level=None) -> 'np.ndarray': Return a boolean array where the index values are in `values`.\n# query(self, expr: 'str', inplace: 'bool' = False, **kwargs): Query the columns of a DataFrame with a boolean expression.\n# notna(self) -> 'np.ndarray': Detect existing (non-missing) values.\n# isna(self) -> 'np.ndarray': Detect missing values.\n# tolist(self) -> 'list[Scalar]': Return a list of the values.\n# [end]\nimport pandas as pd\n\ndef select_rows_from_column(df, col_name, values):\n    # How do I select rows from a DataFrame df based on column values?\n    # Return rows whose column value named `col_name` is in an iterable `values`\n    return df[df[col_name].isin(values)]", "error": "", "token_usage": [[273, 185]], "time_usage": [5.033973932266235]}
{"task_id": "PandasEval/2", "success": true, "generated_code": "# [start]\n# rename(self, index=None, *, axis=None, copy=True, inplace=False, level=None, errors='ignore'): Alter Series index labels or name.\n# reorder_categories(self, new_categories, ordered=None, inplace=<no_default>): Reorder categories as specified in new_categories.\n# assign(self, **kwargs) -> 'DataFrame': Assign new columns to a DataFrame.\n# drop(self, labels, errors: 'str_t' = 'raise') -> 'Index': Make new Index with passed list of labels deleted.\n# rename_categories(self, new_categories, inplace=<no_default>): Rename categories.\n# [end]\nimport pandas as pd\n\ndef change_col_names_of_df(df, origin_names, new_names):\n    # How do I change the column labels of df\uff1f\n    # And return the dataframe that has been renamed\n    return df.rename(columns=dict(zip(origin_names, new_names)))", "error": "", "token_usage": [[272, 187]], "time_usage": [6.768479347229004]}
{"task_id": "PandasEval/3", "success": true, "generated_code": "# [start]\n# drop(self, labels, errors: 'str_t' = 'raise') -> 'Index': Make new Index with passed list of labels deleted.\n# delete(self: '_IndexT', loc) -> '_IndexT': Make new Index with passed location(-s) deleted.\n# lstrip(self, to_strip=None): Remove leading characters.\n# insert(self, loc, column, value, allow_duplicates: 'bool' = False) -> 'None': Insert column into DataFrame at specified location.\n# drop_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Return Index with duplicate values removed.\n# [end]\nimport pandas as pd\n\ndef delete_column(df, column_name):\n    \"\"\"\n    Deleting a column from a Pandas DataFrame.\n    \n    Args:\n        df (DataFrame): The DataFrame from which the column needs to be deleted.\n        column_name (str): The name of the column to be deleted.\n    \n    Returns:\n        DataFrame: The DataFrame after deleting the specified column.\n    \"\"\"\n    df.drop(column_name, axis=1, inplace=True)\n    return df", "error": "", "token_usage": [[275, 239]], "time_usage": [6.234367370605469]}
{"task_id": "PandasEval/4", "success": true, "generated_code": "# [start]\n# assign(self, **kwargs) -> 'DataFrame': Assign new columns to a DataFrame.\n# pivot(data: 'DataFrame', index: 'IndexLabel | None' = None, columns: 'IndexLabel | None' = None, values: 'IndexLabel | None' = None) -> 'DataFrame': Return reshaped DataFrame organized by given index / column values.\n# select_column(self, key: 'str', column: 'str', start: 'int | None' = None, stop: 'int | None' = None): return a single column from the table.\n# nlargest(self, n, columns, keep: 'str' = 'first') -> 'DataFrame': Return the first `n` rows ordered by `columns` in descending order.\n# query(self, expr: 'str', inplace: 'bool' = False, **kwargs): Query the columns of a DataFrame with a boolean expression.\n# [end]\nimport pandas as pd\n\ndef select_multiple_columns(df, columns):\n    # How do I select the given columns and return the new DataFrame?\n    return df[columns]", "error": "", "token_usage": [[324, 232]], "time_usage": [5.869344234466553]}
{"task_id": "PandasEval/5", "success": true, "generated_code": "# [start]\n# len(self): Compute the length of each element in the Series/Index.\n# take(self, indices: 'Sequence[int]', allow_fill: 'bool' = False, fill_value: 'Any' = None): Take elements from an array.\n# count(self, value, /): Return number of occurrences of value.\n# asof(self, where, subset=None): Return the last row(s) without any NaNs before `where`.\n# ge(self, other, level=None, fill_value=None, axis=0): Return Greater than or equal to of series and other, element-wise (binary operator `ge`).\n# [end]\nimport pandas as pd\n\ndef get_row_count(df):\n    \"\"\"\n    Return the row count of df\n    \"\"\"\n    return len(df)", "error": "", "token_usage": [[258, 165]], "time_usage": [4.541266679763794]}
{"task_id": "PandasEval/6", "success": true, "generated_code": "# [start]\n# tolist(self) -> 'list[Scalar]': Return a list of the values.\n# to_list(self) -> 'list[Scalar]': Return a list of the values.\n# to_arrays(data, columns: 'Index | None', dtype: 'DtypeObj | None' = None) -> 'tuple[list[ArrayLike], Index]': Return list of arrays, columns.\n# boxplot(self, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, backend=None, **kwargs): Make a box plot from DataFrame columns.\n# select_dtypes(self, include=None, exclude=None) -> 'DataFrame': Return a subset of the DataFrame's columns based on the column dtypes.\n# [end]\nimport pandas as pd\n\ndef get_list_from_dataframe(df):\n    # I want to get a list of the column headers from a Pandas DataFrame. \n    # The DataFrame will come from user input, so I won't know how many columns there will be or what they will be called.\n    # Return a list of the column headers.\n    return df.columns.tolist()", "error": "", "token_usage": [[338, 246]], "time_usage": [6.517936944961548]}
{"task_id": "PandasEval/7", "success": true, "generated_code": "# [start]\n# assign(self, **kwargs) -> 'DataFrame': Assign new columns to a DataFrame.\n# insert(self, loc, column, value, allow_duplicates: 'bool' = False) -> 'None': Insert column into DataFrame at specified location.\n# add(self, other, axis='columns', level=None, fill_value=None): Get Addition of dataframe and other, element-wise (binary operator `add`).\n# pivot(self, index=None, columns=None, values=None) -> 'DataFrame': Return reshaped DataFrame organized by given index / column values.\n# rename(self, name, inplace=False): Alter Index or MultiIndex name.\n# [end]\nimport pandas as pd\n\ndef add_column_to_dataframe(df, column_name, column_data):\n    # How to add a new column to an existing DataFrame?\n    # I would like to add a new column data with the column name, to the existing dataframe\n    df[column_name] = column_data\n    return df", "error": "", "token_usage": [[289, 204]], "time_usage": [5.617398023605347]}
{"task_id": "PandasEval/8", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# eval(self, expr: 'str', inplace: 'bool' = False, **kwargs): Evaluate a string describing operations on DataFrame columns.\n# to_sql(self, name: 'str', con, schema=None, if_exists: 'str' = 'fail', index: 'bool_t' = True, index_label=None, chunksize=None, dtype: 'DtypeArg | None' = None, method=None) -> 'None': Write records stored in a DataFrame to a SQL database.\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# [end]\nimport pandas as pd\n\ndef change_all_cols_type(df):\n    # Change all columns type of DataFrame to numeric\n    # And return the new DataFrame\n    # The code is:\n    df = df.apply(pd.to_numeric, errors='coerce')\n    return df", "error": "", "token_usage": [[396, 316]], "time_usage": [8.805537223815918]}
{"task_id": "PandasEval/9", "success": true, "generated_code": "# [start]\n# dropna(self: '_IndexT', how: 'str_t' = 'any') -> '_IndexT': Return Index without NA/NaN values.\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# fillna(self, value=None, downcast=None): Fill NA/NaN values with the specified value.\n# isna(self) -> 'np.ndarray': Detect missing values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef drop_rows_col_nan(df, col_name):\n    # How to drop rows of Pandas DataFrame whose value in a certain column is NaN\n    df.dropna(subset=[col_name], inplace=True)\n    return df", "error": "", "token_usage": [[242, 160]], "time_usage": [4.795800685882568]}
{"task_id": "PandasEval/11", "success": true, "generated_code": "from typing import List\nimport pandas as pd\nimport numpy as np\n\ndef append_in_dataframe(df, list_to_append, column_name_list) -> pd.DataFrame:\n    \"\"\"    \n    Params:\n        df: The dataframe to append to.\n        list_to_append: The list to append.\n        column_name_list: The column names of the list to append.\n\n    Returns:\n        The dataframe with the list appended.\n    \"\"\"\n    # Convert the list to a DataFrame\n    new_df = pd.DataFrame(list_to_append, columns=column_name_list)\n    \n    # Append the new DataFrame to the original DataFrame\n    df = pd.concat([df, new_df], ignore_index=True)\n    \n    return df", "error": "", "token_usage": [[490, 146]], "time_usage": [4.302330017089844]}
{"task_id": "PandasEval/12", "success": true, "generated_code": "# [start]\n# type(value=None, freq=None, ordinal=None, year=None, month=None, quarter=None, day=None, hour=None, minute=None, second=None): Represents a period of time.\n# to_numeric(arg, errors='raise', downcast=None): Convert argument to a numeric type.\n# date(*args, **kwargs): Return date object with same year, month and day.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# to_julian_date(self) -> numpy.float64: Convert TimeStamp to a Julian Date.\n# [end]\nimport pandas as pd\n\ndef extract_the_last_year(df, column_name):\n    \"\"\"\n    Extracts the last year (YY) of a fiscal date string in the format of YYYY-YY.\n    \n    Args:\n        df (DataFrame): The DataFrame containing the date strings.\n        column_name (str): The column name of the DataFrame that contains the date strings.\n    \n    Returns:\n        Series: The numerical Series object of the last year.\n    \"\"\"\n    # I am trying to extract the last year (YY) of a fiscal date string in the format of YYYY-YY.\n    # e.g The last year of this '1999-00' would be 2000.\n    # I need a logic to include a case where if it is the end of the century then my apply method should add to the first two digits.\n    return df[column_name].apply(lambda x: int(x.split('-')[0][:2]) + int(x.split('-')[1]))", "error": "", "token_usage": [[380, 355]], "time_usage": [10.178362369537354]}
{"task_id": "PandasEval/13", "success": true, "generated_code": "# [start]\n# head(self: 'FrameOrSeries', n: 'int' = 5) -> 'FrameOrSeries': Return the first `n` rows.\n# nlargest(self, n, columns, keep: 'str' = 'first') -> 'DataFrame': Return the first `n` rows ordered by `columns` in descending order.\n# nsmallest(self, n, columns, keep: 'str' = 'first') -> 'DataFrame': Return the first `n` rows ordered by `columns` in ascending order.\n# tail(self: 'FrameOrSeries', n: 'int' = 5) -> 'FrameOrSeries': Return the last `n` rows.\n# SelectNFrame(obj, n: 'int', keep: 'str', columns): Implement n largest/smallest for DataFrame.\n# [end]\nimport pandas as pd\n\ndef get_last_n_rows(df, n):\n    # How to get the last N rows of a pandas DataFrame?\n    return df.tail(n)", "error": "", "token_usage": [[305, 213]], "time_usage": [6.033011198043823]}
{"task_id": "PandasEval/14", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# count(self, value, /): Return number of occurrences of value.\n# argmin(self, axis=None, skipna=True, *args, **kwargs) -> 'int': Return int position of the smallest value in the Series.\n# to_list(self) -> 'list[Scalar]': Return a list of the values.\n# func(self) -> 'bool': return if the index is monotonic increasing (only equal or increasing) values.\n# [end]\nimport pandas as pd\n\ndef get_values_at_nth_rows(df, n, column_name):\n    \"\"\"\n    how do I get the value at an nth row of a given column name in Pandas?\n    return the value\n    \"\"\"\n    return df[column_name].iloc[n]", "error": "", "token_usage": [[297, 208]], "time_usage": [5.808481693267822]}
{"task_id": "PandasEval/15", "success": true, "generated_code": "# [start]\n# combine(self, other: 'DataFrame', func, fill_value=None, overwrite: 'bool' = True) -> 'DataFrame': Perform column-wise combine with another DataFrame.\n# reindex(self, labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None): Conform Series/DataFrame to new index with optional filling logic.\n# join(self, other: 'FrameOrSeriesUnion', on: 'IndexLabel | None' = None, how: 'str' = 'left', lsuffix: 'str' = '', rsuffix: 'str' = '', sort: 'bool' = False) -> 'DataFrame': Join columns of another DataFrame.\n# combine_first(self, other: 'DataFrame') -> 'DataFrame': Update null elements with value in the same location in `other`.\n# transform(self, func: 'AggFuncType', axis: 'Axis' = 0, *args, **kwargs) -> 'DataFrame': Call ``func`` on self producing a DataFrame with transformed values.\n# [end]\nimport pandas as pd\n\ndef creating_df_with_same_as_other(df_original):\n    # creating a new dataframe of all same with df_original one, but no any rows\n    new_df = pd.DataFrame(columns=df_original.columns)\n    return new_df", "error": "", "token_usage": [[373, 284]], "time_usage": [7.5038902759552]}
{"task_id": "PandasEval/20", "success": true, "generated_code": "new_df = df.groupby(['Country', 'Item_Code'])[['Y1961', 'Y1962', 'Y1963']].sum()", "error": "", "token_usage": [[516, 33]], "time_usage": [1.5166301727294922]}
{"task_id": "PandasEval/10", "success": true, "generated_code": "my_series = pd.Series([56, 24, 421, 90])", "error": "", "token_usage": [[419, 21]], "time_usage": [1.2003633975982666]}
{"task_id": "PandasEval/16", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -> 'FrameOrSeriesUnion': Invoke function on values of Series.\n# swapaxes(self: 'FrameOrSeries', axis1, axis2, copy=True) -> 'FrameOrSeries': Interchange axes and swap values axes appropriately.\n# transform(self, func: 'AggFuncType', axis: 'Axis' = 0, *args, **kwargs) -> 'FrameOrSeriesUnion': Call ``func`` on self producing a Series with transformed values.\n# to_clipboard(obj, excel=True, sep=None, **kwargs): Attempt to write text representation of object to the system clipboard The clipboard can be then pasted into Excel for example.\n# preprocess_pandas(func): Decorate a function to convert all data series arguments to `np.\n# [end]\nimport pandas as pd\n\ndata = {'col_0': ['a', 'a', 'a', 'a', 'b','b','b'], 'col_1': [-2, -7, 6, 8, -5, 2, 6]}\ndf = pd.DataFrame(data)\n# What I want is to clip the values of `col_1` between -2 to 2 if `col_0` is `a`.\n# # Using `clip` function in pandas.\ndf.loc[df['col_0']=='a','col_1'] = df.loc[df['col_0']=='a','col_1'].clip(-2, 2)", "error": "", "token_usage": [[415, 339]], "time_usage": [9.354309320449829]}
{"task_id": "PandasEval/17", "success": true, "generated_code": "df = df.where(df > df.mean()).dropna()", "error": "", "token_usage": [[409, 16]], "time_usage": [0.9647305011749268]}
{"task_id": "PandasEval/18", "success": true, "generated_code": "target_series = target_series.append(source_series, ignore_index=True)", "error": "", "token_usage": [[353, 17]], "time_usage": [1.1088216304779053]}
{"task_id": "PandasEval/19", "success": true, "generated_code": "# Selecting rows where column x2 is NaN \nnan_df = df[df['x2'].isna()]", "error": "", "token_usage": [[349, 27]], "time_usage": [1.2920606136322021]}
{"task_id": "PandasEval/21", "success": true, "generated_code": "df = pd.DataFrame(a, columns=['one', 'two'])\ndf['two'] = df['two'].astype(float)", "error": "", "token_usage": [[460, 29]], "time_usage": [1.4701712131500244]}
{"task_id": "PandasEval/22", "success": true, "generated_code": "# [start]\n# astype(self, dtype: 'Dtype | None' = None, copy=True): Change the dtype of a SparseArray.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# update_dtype(self, dtype): Convert the SparseDtype to a new dtype.\n# Float32Dtype(): An ExtensionDtype for float32 data.\n# [end]\nimport pandas as pd\nimport numpy as np\n\nmy_df = pd.DataFrame({'col1': [1,2,3], 'col2': [1.0,2.0,3.0]})\n# I need to change the dtype of multiple columns but the dataframe has different kind of dtypes. \n# Some columns dtypes are float64 whereas some columns are int64\n# I need to change all float64 to float32.\ncols = ['col1', 'col2']\nmy_df[cols] = my_df[cols].astype(np.float32)", "error": "", "token_usage": [[351, 277]], "time_usage": [8.078561305999756]}
{"task_id": "PandasEval/23", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# pivot(self, index=None, columns=None, values=None) -> 'DataFrame': Return reshaped DataFrame organized by given index / column values.\n# fillna(self, value=None, method=None, limit=None): Fill missing values with `value`.\n# unstack(self, level=-1, fill_value=None) -> 'DataFrame': Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n# drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') -> 'Series': Return Series with specified index labels removed.\n# [end]\nimport pandas as pd\ndf = pd.DataFrame({'col1': [1,2,3], 'col2': ['Jimmy','Tom','Jimmy']})\n# I have a dataframe that has two columns, the second column is one of only a few values. \n# I want to return a dataframe where only the rows where that col2 had a specific value 'Jimmy' are included.\nnew_df = df[df['col2'] == 'Jimmy']", "error": "", "token_usage": [[363, 275]], "time_usage": [8.05302882194519]}
{"task_id": "PandasEval/24", "success": true, "generated_code": "# [start]\n# iterrows(self) -> 'Iterable[tuple[Hashable, Series]]': Iterate over DataFrame rows as (index, Series) pairs.\n# items(self) -> 'Iterable[tuple[Hashable, Series]]': Iterate over (column name, Series) pairs.\n# iteritems(self) -> 'Iterable[tuple[Hashable, Series]]': Iterate over (column name, Series) pairs.\n# to_arrays(data, columns: 'Index | None', dtype: 'DtypeObj | None' = None) -> 'tuple[list[ArrayLike], Index]': Return list of arrays, columns.\n# arrays_to_mgr(arrays, arr_names: 'Index', index, columns, *, dtype: 'DtypeObj | None' = None, verify_integrity: 'bool' = True, typ: 'str | None' = None, consolidate: 'bool' = True) -> 'Manager': Segregate Series based on type and coerce into matrices.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'MSRA': [10, 11, 12], 'THU': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n# (for index, row in DataFrame.iterrows) is a generator which yields both the index and row (as a Series)\n# for each row in the DataFrame, we need put the row['MSRA'] (as key) and row['THU'] (as value) into a rows_dict\nrows_dict = {} # {MSRA: THU, ...}\n\nfor index, row in df.iterrows():\n    rows_dict[row['MSRA']] = row['THU']", "error": "", "token_usage": [[436, 361]], "time_usage": [10.479564905166626]}
{"task_id": "PandasEval/25", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# pivot(self, index=None, columns=None, values=None) -> 'DataFrame': Return reshaped DataFrame organized by given index / column values.\n# select_dtypes(self, include=None, exclude=None) -> 'DataFrame': Return a subset of the DataFrame's columns based on the column dtypes.\n# frame_apply(obj: 'DataFrame', func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type: 'str | None' = None, args=None, kwargs=None) -> 'FrameApply': construct and return a row or column based frame apply object.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1000, 765, 800], 'B': [10, 5, 7]})\n# I have a dataframe in pandas where each column has different value range.\n# Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?\nnormalized_df = df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))", "error": "", "token_usage": [[401, 323]], "time_usage": [8.459865093231201]}
{"task_id": "PandasEval/26", "success": true, "generated_code": "# [start]\n# astype(self, dtype: 'Dtype', copy: 'bool' = True) -> 'ArrayLike': Coerce this type to another dtype.\n# from_dict(data, orient: 'str' = 'columns', dtype: 'Dtype | None' = None, columns=None) -> 'DataFrame': Construct DataFrame from dict of array-like or dicts.\n# to_arrays(data, columns: 'Index | None', dtype: 'DtypeObj | None' = None) -> 'tuple[list[ArrayLike], Index]': Return list of arrays, columns.\n# dict_to_mgr(data: 'dict', index, columns, *, dtype: 'DtypeObj | None' = None, typ: 'str' = 'block', copy: 'bool' = True) -> 'Manager': Segregate Series based on type and coerce into matrices.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# [end]\nimport pandas as pd\n\n# I want to create a dataframe with one of the column as a list or array.\ndf = pd.DataFrame({'Name':['Juda','Pri']})\nemails = {'a@a.com','b@b.com'}\ndf['Email'] = ''\n# After you assign a list like or array like value to the columns, the column should be considered as type object\n# Now I want to assign the emails to first row and the 'Email' column\n\n# Complete the code\ndf.at[0, 'Email'] = emails\n\n# Print the updated dataframe\nprint(df)", "error": "", "token_usage": [[421, 350]], "time_usage": [9.34939432144165]}
{"task_id": "PandasEval/28", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# astype(self: 'FrameOrSeries', dtype, copy: 'bool_t' = True, errors: 'str' = 'raise') -> 'FrameOrSeries': Cast a pandas object to a specified dtype ``dtype``.\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# frame_apply(obj: 'DataFrame', func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type: 'str | None' = None, args=None, kwargs=None) -> 'FrameApply': construct and return a row or column based frame apply object.\n# [end]\nimport pandas as pd\n\ndef is_df_exist(df):\n    # In my code, I have several variables which can either contain a pandas DataFrame or nothing at all.\n    # Let's say I want to test and see if a certain DataFrame has been created yet or not.\n    if isinstance(df, pd.DataFrame):\n        return True\n    else:\n        return False", "error": "", "token_usage": [[428, 349]], "time_usage": [9.544384002685547]}
{"task_id": "PandasEval/29", "success": true, "generated_code": "n_df = df[df['line_num'] != 0]", "error": "", "token_usage": [[328, 17]], "time_usage": [1.1292517185211182]}
{"task_id": "PandasEval/30", "success": true, "generated_code": "# [start]\n# drop(self, labels, errors: 'str_t' = 'raise') -> 'Index': Make new Index with passed list of labels deleted.\n# drop_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Return Index with duplicate values removed.\n# droplevel(self, level=0): Return index with requested level(s) removed.\n# delete(self: '_IndexT', loc) -> '_IndexT': Make new Index with passed location(-s) deleted.\n# dropna(self: '_IndexT', how: 'str_t' = 'any') -> '_IndexT': Return Index without NA/NaN values.\n# [end]\nimport pandas as pd\n\nweb_stats = {'Day': [1, 2, 3, 4, 2, 6],\n             'Visitors': [43, 43, 34, 23, 43, 23],\n             'Bounce_Rate': [3, 2, 4, 3, 5, 5]}\ndf = pd.DataFrame(web_stats)\n# I would like to drop all data in a pandas dataframe\n# Using df.index to drop all rows\n\ndf.drop(df.index, inplace=True)", "error": "", "token_usage": [[355, 265]], "time_usage": [6.977933645248413]}
{"task_id": "PandasEval/31", "success": true, "generated_code": "# [start]\n# add(self, other, level=None, fill_value=None, axis=0): Return Addition of series and other, element-wise (binary operator `add`).\n# multiply(self, other, level=None, fill_value=None, axis=0): Return Multiplication of series and other, element-wise (binary operator `mul`).\n# rmul(self, other, level=None, fill_value=None, axis=0): Return Multiplication of series and other, element-wise (binary operator `rmul`).\n# mul(self, other, level=None, fill_value=None, axis=0): Return Multiplication of series and other, element-wise (binary operator `mul`).\n# area(self, x=None, y=None, **kwargs): Draw a stacked area plot.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\ndf['C'] = df['A'] + df['B']", "error": "", "token_usage": [[320, 215]], "time_usage": [5.817580938339233]}
{"task_id": "PandasEval/32", "success": true, "generated_code": "# [start]\n# drop(self, labels=None, axis: 'Axis' = 0, index=None, columns=None, level: 'Level | None' = None, inplace: 'bool' = False, errors: 'str' = 'raise'): Drop specified labels from rows or columns.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# remove(self, key: 'str', where=None, start=None, stop=None): Remove pandas object partially by specifying the where condition.\n# dropna(self, axis: 'Axis' = 0, how: 'str' = 'any', thresh=None, subset=None, inplace: 'bool' = False): Remove missing values.\n# shift(self, periods: 'int' = 1, fill_value: 'object' = None) -> 'ExtensionArray': Shift values by desired number.\n# [end]\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': [1, 4, 7, np.nan], 'B': [np.nan, 2, 5, np.nan], 'C': [np.nan, np.nan, 3, 6]})\n# Move next value to first empty row pandas\n# how do i move each value from a column to the first empty \"row/cell\" in pandas?\n# use sorted to align non NULL data at the top, use dropna to drop all rows with all NaN\nnew_df = df.apply(lambda x: sorted(x.dropna().tolist()) + [np.nan] * (len(x) - len(x.dropna())), axis=0)", "error": "", "token_usage": [[437, 372]], "time_usage": [9.954257011413574]}
{"task_id": "PandasEval/33", "success": true, "generated_code": "# [start]\n# map(self, mapper): Map categories using input correspondence (dict, Series, or function).\n# lower(self): Convert strings in the Series/Index to lowercase.\n# upper(self): Convert strings in the Series/Index to uppercase.\n# translate(self, table): Map all characters in the string through the given mapping table.\n# boxplot_frame(self, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, backend=None, **kwargs): Make a box plot from DataFrame columns.\n# [end]\nimport pandas as pd\n\ndef make_dataframe_column_headers_lowercase(data):\n    # I want to make all column headers in my pandas data frame lower case\n    data.columns = data.columns.str.lower()\n    return data", "error": "", "token_usage": [[252, 167]], "time_usage": [5.107562780380249]}
{"task_id": "PandasEval/35", "success": true, "generated_code": "first_value = df['a'].nlargest(1).iloc[0]", "error": "", "token_usage": [[369, 20]], "time_usage": [1.1605682373046875]}
{"task_id": "PandasEval/36", "success": true, "generated_code": "# [start]\n# unique(self): Return the ``Categorical`` which ``categories`` and ``codes`` are unique.\n# factorize_array(values: 'np.ndarray', na_sentinel: 'int' = -1, size_hint: 'int | None' = None, na_value=None, mask: 'np.ndarray | None' = None) -> 'tuple[np.ndarray, np.ndarray]': Factorize an array-like to codes and uniques.\n# BooleanArray(values: 'np.ndarray', mask: 'np.ndarray', copy: 'bool' = False): Array of boolean (True/False) data with missing values.\n# isin(self, values, level=None) -> 'np.ndarray': Return a boolean array where the index values are in `values`.\n# round(self: 'T', decimals: 'int' = 0, *args, **kwargs) -> 'T': Round each value in the array a to the given number of decimals.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(0,10,size=100).reshape(10,10))\n# I have a Pandas dataframe and I want to find all the unique values in that dataframe...irrespective of row/columns. \n# If I have a 10 x 10 dataframe, and suppose they have 84 unique values, I need to find them - Not the count.\n# Using xx.values.ravel to get the flattened array of the dataframe\n# Getting the unique values by numpy.unique\nunique_ndarray = np.unique(df.values.ravel())\n# Printing the unique values\nprint(unique_ndarray)", "error": "", "token_usage": [[421, 340]], "time_usage": [8.444373846054077]}
{"task_id": "PandasEval/37", "success": true, "generated_code": "last_df = df.sort_values('date', ascending=True).groupby('id').last()", "error": "", "token_usage": [[581, 23]], "time_usage": [1.219059705734253]}
{"task_id": "PandasEval/38", "success": true, "generated_code": "def drop2rows_zero(df):\n    # i want to drop 2 rows in the dataframe if zero comes in the column\n    # if 0 comes on odd index drop previous row as well as current row using pandas\n    # Assuming your dataframe is indexed starting from 0\n    # Rows with column2 = 0 and on odd index\n    idx = df[(df['column2'] == 0) & (df.index % 2 == 1)].index\n    # The rows above them\n    idx = idx.append(idx-1)\n    # A new dataframe with those rows removed\n    df = df.drop(idx)\n    return df", "error": "", "token_usage": [[373, 140]], "time_usage": [4.234294891357422]}
{"task_id": "PandasEval/39", "success": true, "generated_code": "# [start]\n# shift(self, periods=1, freq=None): Shift index by desired number of time frequency increments.\n# pct_change(self: 'FrameOrSeries', periods=1, fill_method='pad', limit=None, freq=None, **kwargs) -> 'FrameOrSeries': Percentage change between the current and a prior element.\n# tshift(self: 'FrameOrSeries', periods: 'int' = 1, freq=None, axis: 'Axis' = 0) -> 'FrameOrSeries': Shift the time index, using the index's frequency if available.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# to_timedelta(arg, unit=None, errors='raise'): Convert argument to timedelta.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef shift_column_up_by_one(df):\n    # Shift column in pandas dataframe up by one?\n    # In detail, in 'gdp' column, shift up by one and return dataframe with the changed gdp column.\n    df['gdp'] = df['gdp'].shift(-1)\n    return df", "error": "", "token_usage": [[347, 269]], "time_usage": [7.429225444793701]}
{"task_id": "PandasEval/40", "success": true, "generated_code": "new_df = df.select_dtypes(include=['float64'])", "error": "", "token_usage": [[427, 16]], "time_usage": [0.986405611038208]}
{"task_id": "PandasEval/41", "success": false, "generated_code": "", "error": "[[task_id:PandasEval/41]:generate code: think fix suggestion]:Prompt:[ here is a uncomplete python code with some useful api information in its comments: \n```python# [start]\n# merge_ordered(left: 'DataFrame', right: 'DataFrame', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_by=None, right_by=None, fill_method: 'str | None' = None, suffixes: 'Suffixes' = ('_x', '_y'), how: 'str' = 'outer') -> 'DataFrame': Perform merge with optional filling/interpolation.\n# combine(self, other: 'DataFrame', func, fill_value=None, overwrite: 'bool' = True) -> 'DataFrame': Perform column-wise combine with another DataFrame.\n# combine_first(self, other: 'DataFrame') -> 'DataFrame': Update null elements with value in the same location in `other`.\n# merge(left: 'DataFrame | Series', right: 'DataFrame | Series', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'DataFrame': Merge DataFrame or named Series objects with a database-style join.\n# union_categoricals(to_union, sort_categories: bool = False, ignore_order: bool = False): Combine list-like of Categorical-like, unioning categories.\n# [end]\nimport pandas as pd\nimport numpy as np\ndef merge_df(df1, df2):\n    # How to merge two dataframes with different column names but same number of rows?\n    # I have two different data frames in pandas. Example:\n    # df1=a b  df2= c\n    # 0 1       1 \n    # 1 2       2 \n    # 2 3       3 \n    # I want to merge them so\n    # df1= a b c  \n    #  0 1 1\n    #  1 2 2\n    #  2 3 3\n    # In order to merge two dataframes you can use this two examples. Both returns the same goal\n    # Using merge plus additional arguments instructing it to use the indexes\n    # Specially, we can set left_index and right_index to True\n    ```\nplease complete the code directly in side ''.\nWatch!!! output the whole part of completion only include function def, comment, completion.\nWatch!!! DO NOT output any other information.\nWatch!!! DO NOT change the original code (include comment).\nWatch!!! you should wrap the code between '```python' and '```', for example:\n```python\n<code>\n```.\n ] \u5c1d\u8bd5\u6b21\u6570\u5230\u8fbe\u4e0a\u9650.", "token_usage": null, "time_usage": [107.40038919448853]}
{"task_id": "PandasEval/42", "success": true, "generated_code": "# [start]\n# delete(self, loc): Make new Index with passed location(-s) deleted.\n# remove_unused_categories(self, *args, **kwargs): Remove categories which are not used.\n# remove_categories(self, *args, **kwargs): Remove the specified categories.\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# select_column(self, key: 'str', column: 'str', start: 'int | None' = None, stop: 'int | None' = None): return a single column from the table.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 3],'B': [100, 300, 500],'C': list('abc')})\n\n# How can I delete multiple columns in one pass?\n# In detail, I would like to delete columns A and C, but I don't know how to do it in one pass.\nnew_df = df.drop(['A', 'C'], axis=1)", "error": "", "token_usage": [[309, 223]], "time_usage": [6.256938934326172]}
{"task_id": "PandasEval/43", "success": true, "generated_code": "# [start]\n# value_counts(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, dropna: 'bool' = True): Return a Series containing counts of unique values.\n# count(self, value, /): Return number of occurrences of value.\n# count_not_none(*args) -> 'int': Returns the count of arguments that are not None.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# sort_values(self, by, axis: 'Axis' = 0, ascending=True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'last', ignore_index: 'bool' = False, key: 'ValueKeyFunc' = None): Sort by the values along either axis.\n# [end]\nimport pandas as pd\n\ndef get_value_counts(df):\n    # I want to get the counts of unique values of the dataframe. count_values implements this however I want to use its output somewhere else. \n    # How can I convert .count_values output to a pandas dataframe.\n    # Use rename_axis('unique_values') for name ('counts') of column from index and reset_index\n    # return the final dataframe\n    counts = df.value_counts().rename_axis('unique_values').reset_index(name='counts')\n    return counts", "error": "", "token_usage": [[405, 330]], "time_usage": [8.415569067001343]}
{"task_id": "PandasEval/44", "success": true, "generated_code": "data.columns = ['a', 'b', 'c']", "error": "", "token_usage": [[285, 16]], "time_usage": [1.158876895904541]}
{"task_id": "PandasEval/45", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -> 'FrameOrSeriesUnion': Invoke function on values of Series.\n# lower(self): Convert strings in the Series/Index to lowercase.\n# to_frame(self, name=None) -> 'DataFrame': Convert Series to DataFrame.\n# map(self, mapper, na_action=None): Map values using input correspondence (a dict, Series, or function).\n# upper(self): Convert strings in the Series/Index to uppercase.\n# [end]\nimport pandas as pd\n\ndef make_df_all_cols_lower(data):\n    # I want to make all column headers in my pandas data frame lower case\n    # Return the changed dataframe\n    return data.columns.str.lower().to_frame().T", "error": "", "token_usage": [[262, 176]], "time_usage": [4.78750205039978]}
{"task_id": "PandasEval/46", "success": true, "generated_code": "# [start]\n# sample(self: 'FrameOrSeries', n=None, frac: 'float | None' = None, replace: 'bool_t' = False, weights=None, random_state=None, axis: 'Axis | None' = None, ignore_index: 'bool_t' = False) -> 'FrameOrSeries': Return a random sample of items from an axis of object.\n# groupby(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, dropna: 'bool' = True) -> 'DataFrameGroupBy': Group DataFrame using a mapper or by a Series of columns.\n# boxplot_frame_groupby(grouped, subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharex=False, sharey=True, backend=None, **kwargs): Make box plots from DataFrameGroupBy data.\n# Selection(table: 'Table', where=None, start: 'int | None' = None, stop: 'int | None' = None): Carries out a selection operation on a tables.\n# boxplot_frame(self, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, backend=None, **kwargs): Make a box plot from DataFrame columns.\n# [end]\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame(\n    {\"x\": np.arange(1_000 * 100), \"section\": np.repeat(np.arange(100), 1_000)}\n)\n\n# Say i have a dataframe with 100,000 entries and want to split it into 100 sections of 1000 entries.\n# How do i take a random sample of say size 50 of just one of the 100 sections. \n# the data set is already ordered such that the first 1000 results are the first section the next section the next and so on.\n# You could add a \"section\" column to your data then perform a groupby and sample(n=50):\nsample = df.groupby('section').sample(n=50)", "error": "", "token_usage": [[582, 496]], "time_usage": [12.71953558921814]}
{"task_id": "PandasEval/47", "success": true, "generated_code": "df['Name'] = df['Name'].replace('\\d+', '', regex=True)", "error": "", "token_usage": [[430, 21]], "time_usage": [1.0863969326019287]}
{"task_id": "PandasEval/48", "success": true, "generated_code": "new_df = df.loc[df.groupby('Mt')['num'].idxmax()]", "error": "", "token_usage": [[540, 19]], "time_usage": [1.1705865859985352]}
{"task_id": "PandasEval/49", "success": true, "generated_code": "# [start]\n# replace(self, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=<class 'object'>, fold=None): Implements datetime.\n# DatetimeIndex(data=None, freq=<no_default>, tz=None, normalize: 'bool' = False, closed=None, ambiguous='raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, dtype: 'Dtype | None' = None, copy: 'bool' = False, name: 'Hashable' = None) -> 'DatetimeIndex': Immutable ndarray-like of datetime64 data.\n# to_period(self, freq: 'Frequency | None' = None, axis: 'Axis' = 0, copy: 'bool' = True) -> 'DataFrame': Convert DataFrame from DatetimeIndex to PeriodIndex.\n# apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -> 'FrameOrSeriesUnion': Invoke function on values of Series.\n# to_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, format: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_format: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Series | DatetimeScalar | NaTType | None': Convert argument to datetime.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({\n'date': [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"friday\"],\n'value': [1, 2, 3, 4]\n})\n\n# transfer column date to datetime type\n# when there is a string that is not capable of beeing turned into datetime format, skip that row,\n# use errors='coerce' for this\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')", "error": "", "token_usage": [[560, 474]], "time_usage": [12.61719298362732]}
{"task_id": "PandasEval/50", "success": true, "generated_code": "# [start]\n# dropna(self, how: 'str' = 'any') -> 'MultiIndex': Return Index without NA/NaN values.\n# isin(self, values) -> 'DataFrame': Whether each element in the DataFrame is contained in values.\n# notna(self) -> 'DataFrame': Detect existing (non-missing) values.\n# isna(self) -> 'DataFrame': Detect missing values.\n# isnull(self) -> 'DataFrame': Detect missing values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef if_any_value_is_nan(df):\n    # How to check if any value is NaN in a Pandas DataFrame? Return the result.\n    return df.isnull().values.any()", "error": "", "token_usage": [[241, 152]], "time_usage": [4.1762590408325195]}
{"task_id": "PandasEval/51", "success": true, "generated_code": "# [start]\n# rename_axis(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False): Set the name of the axis for the index or columns.\n# rename(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore'): Alter axes labels.\n# dot(self, other): Compute the dot product between the Series and the columns of other.\n# pivot(data: 'DataFrame', index: 'IndexLabel | None' = None, columns: 'IndexLabel | None' = None, values: 'IndexLabel | None' = None) -> 'DataFrame': Return reshaped DataFrame organized by given index / column values.\n# validate_axis_style_args(data, args, kwargs, arg_name, method_name): Argument handler for mixed index, columns / axis functions In an attempt to handle both `.\n# [end]\nimport pandas as pd\n\ndef sorting_columns_based_on_column_name(df):\n    # Sorting columns in pandas dataframe based on column name\n    # Note that axis is one\n    df = df.sort_index(axis=1)\n    return df", "error": "", "token_usage": [[322, 238]], "time_usage": [6.229182004928589]}
{"task_id": "PandasEval/52", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# isin(self, values) -> 'Series': Whether elements in Series are contained in `values`.\n# select_column(self, key: 'str', column: 'str', start: 'int | None' = None, stop: 'int | None' = None): return a single column from the table.\n# dropna(self, axis=0, inplace=False, how=None): Return a new Series with missing values removed.\n# isna(self) -> 'np.ndarray': Detect missing values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef get_value_when_condition(df):\n    # How can I get the values of column `A` when column `B`=3?\n    return df.loc[df['B'] == 3, 'A']", "error": "", "token_usage": [[298, 215]], "time_usage": [5.783891439437866]}
{"task_id": "PandasEval/53", "success": true, "generated_code": "# [start]\n# mean(self, axis=0, *args, **kwargs): Mean of non-NA/null values.\n# std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return sample standard deviation over requested axis.\n# mad(self, axis=None, skipna=None, level=None): Return the mean absolute deviation of the values over the requested axis.\n# sem(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return unbiased standard error of the mean over requested axis.\n# corr(self, other, method='pearson', min_periods=None) -> 'float': Compute correlation with `other` Series, excluding missing values.\n# [end]\nimport pandas as pd\n\ndef get_mean_in_column(df, col_name):\n    # return the column average/mean\n    return df[col_name].mean()", "error": "", "token_usage": [[285, 195]], "time_usage": [5.600010395050049]}
{"task_id": "PandasEval/54", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# combine(self, other: 'DataFrame', func, fill_value=None, overwrite: 'bool' = True) -> 'DataFrame': Perform column-wise combine with another DataFrame.\n# combine_first(self, other: 'DataFrame') -> 'DataFrame': Update null elements with value in the same location in `other`.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# append(self, to_append, ignore_index: 'bool' = False, verify_integrity: 'bool' = False): Concatenate two or more Series.\n# [end]\nimport pandas as pd\n\ndef combine_df(df1, df2):\n    \"\"\"\n    How do I combine two dataframes with ignore index? Return the concated dataframe.\n    \"\"\"\n    return pd.concat([df1, df2], ignore_index=True)", "error": "", "token_usage": [[327, 247]], "time_usage": [7.175862789154053]}
{"task_id": "PandasEval/55", "success": true, "generated_code": "repeated_x = pd.concat([x] * 5)", "error": "", "token_usage": [[417, 17]], "time_usage": [1.0414421558380127]}
{"task_id": "PandasEval/56", "success": true, "generated_code": "# [start]\n# astype(self: 'FrameOrSeries', dtype, copy: 'bool_t' = True, errors: 'str' = 'raise') -> 'FrameOrSeries': Cast a pandas object to a specified dtype ``dtype``.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# from_dict(data, orient: 'str' = 'columns', dtype: 'Dtype | None' = None, columns=None) -> 'DataFrame': Construct DataFrame from dict of array-like or dicts.\n# SQLTable(name: 'str', pandas_sql_engine, frame=None, index=True, if_exists='fail', prefix='pandas', index_label=None, schema=None, keys=None, dtype: 'DtypeArg | None' = None): For mapping Pandas tables to SQL tables.\n# itertuples(self, index: 'bool' = True, name: 'str | None' = 'Pandas') -> 'Iterable[tuple[Any, ...]]': Iterate over DataFrame rows as namedtuples.\n# [end]\nimport pandas as pd\n\ndef dataframe2list_of_dict(df):\n    # Pandas DataFrame to List of Dictionaries\n    # Use df.to_dict() to solve it and return the result\n    return df.to_dict()", "error": "", "token_usage": [[383, 291]], "time_usage": [8.17725419998169]}
{"task_id": "PandasEval/57", "success": true, "generated_code": "# [start]\n# strftime(self, date_format: 'str') -> 'np.ndarray': Convert to Index using specified date_format.\n# to_period(self, freq: 'Frequency | None' = None, axis: 'Axis' = 0, copy: 'bool' = True) -> 'DataFrame': Convert DataFrame from DatetimeIndex to PeriodIndex.\n# to_julian_date(self) -> 'Float64Index': Convert Datetime Array to float64 ndarray of Julian Dates.\n# to_timestamp(self, freq=None, how='start') -> 'DatetimeIndex': Cast to DatetimeArray/Index.\n# to_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, format: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_format: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Series | DatetimeScalar | NaTType | None': Convert argument to datetime.\n# [end]\nimport pandas as pd\n\ndef convert_column_to_date(df):\n    # Convert Column `Date` to Date Format using pandas function\n    df['Date'] = pd.to_datetime(df['Date'])\n    # return the converted dataframe\n    return df", "error": "", "token_usage": [[384, 302]], "time_usage": [8.728306770324707]}
{"task_id": "PandasEval/58", "success": true, "generated_code": "# [start]\n# value_counts(self, subset: 'Sequence[Hashable] | None' = None, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, dropna: 'bool' = True): Return a Series containing counts of unique rows in the DataFrame.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# count(self, axis: 'Axis' = 0, level: 'Level | None' = None, numeric_only: 'bool' = False): Count non-NA cells for each column or row.\n# ge(self, other, level=None, fill_value=None, axis=0): Return Greater than or equal to of series and other, element-wise (binary operator `ge`).\n# items(self) -> 'Iterable[tuple[Hashable, Any]]': Lazily iterate over (index, value) tuples.\n# [end]\nimport pandas as pd\n\ndef counting_consecutive_positive_values(y):\n    # Counting consecutive positive values in Python/pandas array\n    # I'm trying to count consecutive up days in equity return data; so if a positive day is 1 and a negative is 0, a list y=[0,0,1,1,1,0,0,1,0,1,1] should return z=[0,0,1,2,3,0,0,1,0,1,2].\n    # Initialize variables\n    count = 0\n    result = []\n    \n    # Iterate over the elements in the array\n    for value in y:\n        # If the value is positive, increment the count\n        if value > 0:\n            count += 1\n        # If the value is negative or zero, reset the count\n        else:\n            count = 0\n        # Append the current count to the result list\n        result.append(count)\n    \n    # Return the result\n    return result", "error": "", "token_usage": [[444, 447]], "time_usage": [12.942306756973267]}
{"task_id": "PandasEval/59", "success": true, "generated_code": "# [start]\n# take(self, indices: 'Sequence[int]', allow_fill: 'bool' = False, fill_value: 'Any' = None): Take elements from an array.\n# reindex(self, labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None): Conform Series/DataFrame to new index with optional filling logic.\n# drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') -> 'Series': Return Series with specified index labels removed.\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# fillna(self, value: 'object | ArrayLike | None' = None, method: 'FillnaOptions | None' = None, axis=None, inplace=False, limit=None, downcast=None) -> 'Series | None': Fill NA/NaN values using the specified method.\n# [end]\nimport pandas as pd\n\ndef insert_row_at_arbitrary_in_dataframe(df, row_to_insert):\n    \"\"\"\n    Inserts a row into a dataframe at a specified row with no ingore index, and sort & reset the index with drop=True. \n    Returns the new dataframe.\n    \"\"\"\n    df = df.append(row_to_insert, ignore_index=True)\n    df = df.sort_index().reset_index(drop=True)\n    return df", "error": "", "token_usage": [[408, 339]], "time_usage": [10.084228038787842]}
{"task_id": "PandasEval/60", "success": true, "generated_code": "# [start]\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# to_frame(self, name=None) -> 'DataFrame': Convert Series to DataFrame.\n# get_sheet_data(self, sheet, convert_float: 'bool') -> 'list[list[Scalar]]': Parse an ODF Table into a list of lists.\n# describe_ndframe(*, obj: 'FrameOrSeries', include: 'str | Sequence[str] | None', exclude: 'str | Sequence[str] | None', datetime_is_numeric: 'bool', percentiles: 'Sequence[float] | None') -> 'FrameOrSeries': Describe series or dataframe.\n# from_dict(data, orient: 'str' = 'columns', dtype: 'Dtype | None' = None, columns=None) -> 'DataFrame': Construct DataFrame from dict of array-like or dicts.\n# [end]\nimport pandas as pd\n\ndef get_data_frame_from_list(list_of_lists):\n    # list_of_lists format: [header, [row1], [row2], ...]\n    # header format: [column1, column2, ...]\n    # row format: [value1, value2, ...]\n    # How to convert list to dataframe?\n    # Return the dataframe\n    return pd.DataFrame(list_of_lists[1:], columns=list_of_lists[0])", "error": "", "token_usage": [[402, 321]], "time_usage": [8.78473424911499]}
{"task_id": "PandasEval/61", "success": true, "generated_code": "merged_df = df1.merge(df2, left_index=True, right_index=True)", "error": "", "token_usage": [[661, 21]], "time_usage": [1.1497657299041748]}
{"task_id": "PandasEval/62", "success": true, "generated_code": "# [start]\n# astype(self, dtype, copy: 'bool' = True) -> 'Index': Create an Index with values cast to dtypes.\n# append(self, other: 'Index | Sequence[Index]') -> 'Index': Append a collection of Index options together.\n# to_markdown(self, buf: 'IO[str] | None' = None, mode: 'str' = 'wt', index: 'bool' = True, storage_options: 'StorageOptions' = None, **kwargs) -> 'str | None': Print Series in Markdown-friendly format.\n# to_excel(self, excel_writer, sheet_name: 'str' = 'Sheet1', na_rep: 'str' = '', float_format: 'str | None' = None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None, storage_options: 'StorageOptions' = None) -> 'None': Write object to an Excel sheet.\n# format(self, name: 'bool' = False, formatter: 'Callable | None' = None, na_rep: 'str' = 'NaT', date_format: 'str | None' = None) -> 'list[str]': Render a string representation of the Index.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [0, 1], 'b': [5, 3]})\n# How to obtain pandas DataFrame without index\n# I want to print the whole dataframe, but I don't want to print the index\ndf_string = df.to_string(index=False)", "error": "", "token_usage": [[446, 354]], "time_usage": [8.975764274597168]}
{"task_id": "PandasEval/63", "success": true, "generated_code": "# [start]\n# dropna(self, how: 'str' = 'any') -> 'MultiIndex': Return Index without NA/NaN values.\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# fillna(self, value=None, downcast=None): Fill NA/NaN values with the specified value.\n# delete(self, loc): Make new Index with passed location(-s) deleted.\n# update(self, *args, **kwargs) -> 'None': Update self.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef drop_all_nan_rows(df):\n    # We will drop all Nan rows.\n    # Return the changed dataframe.\n    return df.dropna(how='all')", "error": "", "token_usage": [[250, 162]], "time_usage": [4.6281468868255615]}
{"task_id": "PandasEval/64", "success": true, "generated_code": "# [start]\n# select_column(self, key: 'str', column: 'str', start: 'int | None' = None, stop: 'int | None' = None): return a single column from the table.\n# isin(self, values) -> 'DataFrame': Whether each element in the DataFrame is contained in values.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# value_counts(self, dropna: 'bool' = True): Returns a Series containing counts of unique values.\n# all(self, axis=None, *args, **kwargs): Tests whether all elements evaluate True.\n# [end]\nimport pandas as pd\n\ndef is_contain_particular_value(series, value):\n    # How to determine whether a Pandas Column contains a particular value?\n    # Return the result\n    return series.isin([value]).any()", "error": "", "token_usage": [[304, 216]], "time_usage": [6.066393613815308]}
{"task_id": "PandasEval/65", "success": true, "generated_code": "# [start]\n# rename(self, name, inplace=False): Alter Index or MultiIndex name.\n# to_frame(self, name=None) -> 'DataFrame': Convert Series to DataFrame.\n# droplevel(self: 'FrameOrSeries', level, axis=0) -> 'FrameOrSeries': Return Series/DataFrame with requested index / column level(s) removed.\n# insert(self, loc, column, value, allow_duplicates: 'bool' = False) -> 'None': Insert column into DataFrame at specified location.\n# drop(self, labels, errors: 'str_t' = 'raise') -> 'Index': Make new Index with passed list of labels deleted.\n# [end]\nimport pandas as pd\n\ndef rename_column(df, old_name, new_name):\n    # How would I rename the only one column header?\n    df.rename(columns={old_name: new_name}, inplace=True)\n    return df", "error": "", "token_usage": [[275, 188]], "time_usage": [5.411567211151123]}
{"task_id": "PandasEval/66", "success": true, "generated_code": "# [start]\n# drop_duplicates(self, keep='first', inplace=False) -> 'Series | None': Return Series with duplicate values removed.\n# duplicated(self, keep='first') -> 'Series': Indicate duplicate Series values.\n# repeat(self, repeats: 'int', axis=None) -> 'MultiIndex': Repeat elements of a MultiIndex.\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# update(self, other) -> 'None': Modify Series in place using values from passed Series.\n# [end]\nimport pandas as pd\n\ndef remove_duplicates_by_column(df, col1, col2):\n    # I have a dataframe with repeat values in column `col1`. I want to drop duplicates, keeping the row with the last value in column `col2`.\n    # How would I do that?\n    df.drop_duplicates(subset=col1, keep='last', inplace=True)\n    return df", "error": "", "token_usage": [[288, 204]], "time_usage": [5.911406755447388]}
{"task_id": "PandasEval/67", "success": true, "generated_code": "# [start]\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# astype(self: 'FrameOrSeries', dtype, copy: 'bool_t' = True, errors: 'str' = 'raise') -> 'FrameOrSeries': Cast a pandas object to a specified dtype ``dtype``.\n# DataFrameInfo(data: 'DataFrame', memory_usage: 'bool | str | None' = None): Class storing dataframe-specific info.\n# from_dict(data, orient: 'str' = 'columns', dtype: 'Dtype | None' = None, columns=None) -> 'DataFrame': Construct DataFrame from dict of array-like or dicts.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# [end]\nimport pandas as pd\n\ndef create_empty_df(col_names):\n    \"\"\"\n    Pandas create empty DataFrame with only column names\n    Return: DataFrame\n    \"\"\"\n    return pd.DataFrame(columns=col_names)", "error": "", "token_usage": [[356, 269]], "time_usage": [7.315773010253906]}
{"task_id": "PandasEval/68", "success": true, "generated_code": "# [start]\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# uname_result(system, node, release, version, machine, processor): uname_result(system, node, release, version, machine, processor).\n# SelectNFrame(obj, n: 'int', keep: 'str', columns): Implement n largest/smallest for DataFrame.\n# sparse(data=None): DataFrame accessor for sparse data.\n# SparseFrameAccessor(data=None): DataFrame accessor for sparse data.\n# [end]\nimport pandas as pd\n\ndef delete_first_n_rows(df, n):\n    # Delete first n rows of a dataframe\n    # Input:\n    #   df: DataFrame\n    #   n: int\n    # Return:\n    #   DataFrame\n    return df[n:]", "error": "", "token_usage": [[299, 206]], "time_usage": [5.647598743438721]}
{"task_id": "PandasEval/69", "success": true, "generated_code": "# [start]\n# insert(self, loc, column, value, allow_duplicates: 'bool' = False) -> 'None': Insert column into DataFrame at specified location.\n# duplicated(self, keep='first') -> 'np.ndarray': Indicate duplicate index values.\n# drop_duplicates(self, subset: 'Hashable | Sequence[Hashable] | None' = None, keep: \"Literal['first'] | Literal['last'] | Literal[False]\" = 'first', inplace: 'bool' = False, ignore_index: 'bool' = False) -> 'DataFrame | None': Return DataFrame with duplicate rows removed.\n# delete(self, loc) -> 'MultiIndex': Make new index with passed location deleted.\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# [end]\nimport pandas as pd\n\ndef remove_duplicates_by_col_names(df):\n    \"\"\"\n    Here's a one solution to remove columns based on duplicate column names:\n    Return the duplicated dataframe\n    \"\"\"\n    duplicated_columns = df.columns[df.columns.duplicated()]\n    return df[duplicated_columns]", "error": "", "token_usage": [[314, 234]], "time_usage": [6.447090148925781]}
{"task_id": "PandasEval/70", "success": true, "generated_code": "# [start]\n# astype(self, dtype, copy: 'bool' = True): Create an Index with values cast to dtypes.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# to_string(self, buf=None, na_rep='NaN', float_format=None, header=True, index=True, length=False, dtype=False, name=False, max_rows=None, min_rows=None): Render a string representation of the Series.\n# map(self, mapper, na_action=None): Map values using input correspondence (a dict, Series, or function).\n# from_records(data, index=None, exclude=None, columns=None, coerce_float: 'bool' = False, nrows: 'int | None' = None) -> 'DataFrame': Convert structured or record ndarray to DataFrame.\n# [end]\nimport pandas as pd\n\ndef convert_bool_to_int(df, col_name):\n    # How can I map True/False to 1/0 in a Pandas DataFrame?\n    # return the dataframe with the column converted to int\n    df[col_name] = df[col_name].astype(int)\n    return df", "error": "", "token_usage": [[348, 267]], "time_usage": [7.444119453430176]}
{"task_id": "PandasEval/71", "success": true, "generated_code": "# [start]\n# to_frame(self, index: 'bool' = True, name: 'Hashable' = None) -> 'DataFrame': Create a DataFrame with a column containing the Index.\n# abs(self: 'FrameOrSeries') -> 'FrameOrSeries': Return a Series/DataFrame with absolute numeric value of each element.\n# func(self) -> 'int': Return the number of bytes in the underlying data.\n# fget(self) -> 'int': Return the number of elements in the underlying data.\n# len(self): Compute the length of each element in the Series/Index.\n# [end]\nimport pandas as pd\n\ndef get_number_columns(df):\n    # How do I retrieve the number of columns in a Pandas data frame?\n    # Return the number of columns in the dataframe\n    return len(df.columns)", "error": "", "token_usage": [[265, 173]], "time_usage": [4.871596097946167]}
{"task_id": "PandasEval/72", "success": true, "generated_code": "# [start]\n# isna(self) -> 'np.ndarray': Detect missing values.\n# notna(self) -> 'np.ndarray': Detect existing (non-missing) values.\n# isnull(self) -> 'np.ndarray': Detect missing values.\n# notnull(self) -> 'np.ndarray': Detect existing (non-missing) values.\n# tolist(self): Return a list of the values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef find_columns_name_lists(df):\n    # How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?\n    # Return a list of the column names containing NaNs\n    return df.columns[df.isna().any()].tolist()", "error": "", "token_usage": [[246, 160]], "time_usage": [4.674122333526611]}
{"task_id": "PandasEval/73", "success": true, "generated_code": "# [start]\n# head(self: 'FrameOrSeries', n: 'int' = 5) -> 'FrameOrSeries': Return the first `n` rows.\n# nlargest(self, n, columns, keep: 'str' = 'first') -> 'DataFrame': Return the first `n` rows ordered by `columns` in descending order.\n# nsmallest(self, n, columns, keep: 'str' = 'first') -> 'DataFrame': Return the first `n` rows ordered by `columns` in ascending order.\n# tail(self: 'FrameOrSeries', n: 'int' = 5) -> 'FrameOrSeries': Return the last `n` rows.\n# SelectNFrame(obj, n: 'int', keep: 'str', columns): Implement n largest/smallest for DataFrame.\n# [end]\nimport pandas as pd\n\nN = 2\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n# How to get the last N rows of a pandas DataFrame?\nresult = df.tail(N)", "error": "", "token_usage": [[341, 249]], "time_usage": [6.581518888473511]}
{"task_id": "PandasEval/74", "success": true, "generated_code": "# [start]\n# replace(self, to_replace=None, value=None, inplace: 'bool' = False, limit=None, regex: 'bool' = False, method: 'str' = 'pad'): Replace values given in `to_replace` with `value`.\n# fillna(self, value=None, method=None, limit=None): Fill missing values with `value`.\n# remove(self, key: 'str', where=None, start=None, stop=None): Remove pandas object partially by specifying the where condition.\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# update(self, other, join: 'str' = 'left', overwrite: 'bool' = True, filter_func=None, errors: 'str' = 'ignore') -> 'None': Modify in place using non-NA values from another DataFrame.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef replacing_blank_with_nan(df):\n    # replace field that's entirely space (or empty) with NaN using regex\n    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n    # return the result\n    return df", "error": "", "token_usage": [[326, 250]], "time_usage": [6.737383604049683]}
{"task_id": "PandasEval/75", "success": true, "generated_code": "# [start]\n# fillna(self, value=None, method=None, limit=None): Fill missing values with `value`.\n# shift(self, periods: 'int' = 1, fill_value: 'object' = None) -> 'ExtensionArray': Shift values by desired number.\n# take(self, indices: 'Sequence[int]', allow_fill: 'bool' = False, fill_value: 'Any' = None): Take elements from an array.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef fill_none_with_zero(df, col_names):\n    # Pandas dataframe fillna() only some columns in place\n    # This function fills all columns with 0\n    # Return the changed dataframe\n    df[col_names] = df[col_names].fillna(0)\n    return df", "error": "", "token_usage": [[339, 259]], "time_usage": [7.140437364578247]}
{"task_id": "PandasEval/76", "success": true, "generated_code": "# [start]\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# assign(self, **kwargs) -> 'DataFrame': Assign new columns to a DataFrame.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# pivot_table(data: 'DataFrame', values=None, index=None, columns=None, aggfunc: 'AggFuncType' = 'mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True) -> 'DataFrame': Create a spreadsheet-style pivot table as a DataFrame.\n# transpose(self, *args, copy: 'bool' = False) -> 'DataFrame': Transpose index and columns.\n# [end]\nimport pandas as pd\n\ndef concat_df(df1, df2):\n    # Given that all the dataframes have the same columns, you can simply concat them:\n    return pd.concat([df1, df2])", "error": "", "token_usage": [[368, 273]], "time_usage": [7.339470386505127]}
{"task_id": "PandasEval/77", "success": true, "generated_code": "# [start]\n# extract(self, pat: 'str', flags: 'int' = 0, expand: 'bool' = True) -> 'FrameOrSeriesUnion | Index': Extract capture groups in the regex `pat` as columns in a DataFrame.\n# extractall(self, pat, flags=0): Extract capture groups in the regex `pat` as columns in DataFrame.\n# extract_array(obj: 'object', extract_numpy: 'bool' = False, extract_range: 'bool' = False) -> 'Any | ArrayLike': Extract the ndarray or ExtensionArray from a Series or Index.\n# select(self, key: 'str', where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, auto_close: 'bool' = False): Retrieve pandas object stored in file, optionally based on where criteria.\n# arrays_to_mgr(arrays, arr_names: 'Index', index, columns, *, dtype: 'DtypeObj | None' = None, verify_integrity: 'bool' = True, typ: 'str | None' = None, consolidate: 'bool' = True) -> 'Manager': Segregate Series based on type and coerce into matrices.\n# [end]\nimport pandas as pd\n\ndef extract_first_and_last_df(df):\n    # Extract first and last row of a dataframe in pandas\n    # Return the dataframe with the first and last row\n    return pd.concat([df.head(1), df.tail(1)])", "error": "", "token_usage": [[392, 309]], "time_usage": [8.648460388183594]}
{"task_id": "PandasEval/78", "success": true, "generated_code": "def display_rows_with_gt_1_nan(df):\n    # Return the dataframe with the rows with one or more NaN values\n    return df[df.isnull().sum(axis=1) > 0]", "error": "", "token_usage": [[233, 45]], "time_usage": [1.8901236057281494]}
{"task_id": "PandasEval/79", "success": true, "generated_code": "# [start]\n# tolist(self) -> 'list[Scalar]': Return a list of the values.\n# to_list(self) -> 'list[Scalar]': Return a list of the values.\n# to_arrays(data, columns: 'Index | None', dtype: 'DtypeObj | None' = None) -> 'tuple[list[ArrayLike], Index]': Return list of arrays, columns.\n# format(self, name: 'bool' = False, formatter: 'Callable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Render a string representation of the Index.\n# fget(self) -> 'list[Index]': Return a list of the row axis labels.\n# [end]\nimport pandas as pd\n\ndef get_row_index_values_as_list(df):\n    # Return the row-index values of the dataframe as a list\n    return df.index.tolist()", "error": "", "token_usage": [[286, 194]], "time_usage": [5.463636636734009]}
{"task_id": "PandasEval/80", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# transform(self, func: 'AggFuncType', axis: 'Axis' = 0, *args, **kwargs) -> 'DataFrame': Call ``func`` on self producing a DataFrame with transformed values.\n# to_numpy(self, dtype: 'NpDtype | None' = None, copy: 'bool' = False, na_value: 'Scalar' = <no_default>) -> 'np.ndarray': Convert to a NumPy Array.\n# ge(self, other, level=None, fill_value=None, axis=0): Return Greater than or equal to of series and other, element-wise (binary operator `ge`).\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'mycol':np.arange(5), 'dummy':np.arange(5)})\n# I find myself often having to check whether a column or row exists in a dataframe before trying to reference it.\n# Is there any way to do this more nicely? \n# For example on an arbitrary object I can do x = getattr(anobject, 'id', default) - is there anything similar to this in pandas? Really any way to achieve what I'm doing more gracefully?\n# Output the second row of data in `mycol` column if it exists, otherwise output NaN\nvalue = df['mycol'].iat[1] if 'mycol' in df.columns else np.nan", "error": "", "token_usage": [[461, 386]], "time_usage": [10.579336881637573]}
{"task_id": "PandasEval/81", "success": true, "generated_code": "# [start]\n# count(self, value, /): Return number of occurrences of value.\n# value_counts(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, dropna: 'bool' = True): Return a Series containing counts of unique values.\n# count_not_none(*args) -> 'int': Returns the count of arguments that are not None.\n# round(self, decimals=0, *args, **kwargs) -> 'Series': Round each value in a Series to the given number of decimals.\n# cov(self, other: 'Series', min_periods: 'int | None' = None, ddof: 'int | None' = 1) -> 'float': Compute covariance with Series, excluding missing values.\n# [end]\nimport pandas as pd\n\ndef counting_occurrences_of_a_value(series, value):\n    # Count the number of occurrences of a value in a series\n    # Return the count\n    return series.count(value)", "error": "", "token_usage": [[308, 216]], "time_usage": [6.730017185211182]}
{"task_id": "PandasEval/82", "success": true, "generated_code": "# [start]\n# searchsorted(self, value, side='left', sorter=None) -> 'np.ndarray': Find indices where elements should be inserted to maintain order.\n# rank(values: 'ArrayLike', axis: 'int' = 0, method: 'str' = 'average', na_option: 'str' = 'keep', ascending: 'bool' = True, pct: 'bool' = False) -> 'np.ndarray': Rank the values along a given axis.\n# nlargest(self, n, columns, keep: 'str' = 'first') -> 'DataFrame': Return the first `n` rows ordered by `columns` in descending order.\n# select_column(self, key: 'str', column: 'str', start: 'int | None' = None, stop: 'int | None' = None): return a single column from the table.\n# reorder_columns(ldesc: 'Sequence[Series]') -> 'list[Hashable]': Set a convenient order for rows for display.\n# [end]\nimport pandas as pd\n\ndef find_col_a_gt_col_b_rows(df, col_a, col_b):\n    # Find rows in df where col_a > col_b\n    # Return the rows\n    return df[df[col_a] > df[col_b]]", "error": "", "token_usage": [[355, 269]], "time_usage": [7.695075035095215]}
{"task_id": "PandasEval/83", "success": true, "generated_code": "# [start]\n# drop_duplicates(self, keep='first', inplace=False) -> 'Series | None': Return Series with duplicate values removed.\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# shift(self, periods=1, fill_value=None, axis=0): Shift values by desired number.\n# dropna(self, axis=0, inplace=False, how=None): Return a new Series with missing values removed.\n# pop(self, item: 'Hashable') -> 'Any': Return item and drops from series.\n# [end]\nimport pandas as pd\n\ndef drop_consecutive_duplicates(series):\n    # Drop consecutive duplicates\n    result = series.drop_duplicates(keep='first')\n    # Return the result\n    return result", "error": "", "token_usage": [[250, 167]], "time_usage": [4.768758773803711]}
{"task_id": "PandasEval/84", "success": true, "generated_code": "# [start]\n# round(self, decimals: 'int | dict[IndexLabel, int] | Series' = 0, *args, **kwargs) -> 'DataFrame': Round a DataFrame to a variable number of decimal places.\n# pivot(data: 'DataFrame', index: 'IndexLabel | None' = None, columns: 'IndexLabel | None' = None, values: 'IndexLabel | None' = None) -> 'DataFrame': Return reshaped DataFrame organized by given index / column values.\n# assign(self, **kwargs) -> 'DataFrame': Assign new columns to a DataFrame.\n# select_column(self, key: 'str', column: 'str', start: 'int | None' = None, stop: 'int | None' = None): return a single column from the table.\n# nlargest(self, n, columns, keep: 'str' = 'first') -> 'DataFrame': Return the first `n` rows ordered by `columns` in descending order.\n# [end]\nimport pandas as pd\n\ndef round_a_single_column(df):\n    # Round a single column `A`\n    # Return the dataframe\n    return df.round({'A': 0})", "error": "", "token_usage": [[335, 247]], "time_usage": [6.919435024261475]}
{"task_id": "PandasEval/85", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# pipe(self, func: 'Callable[..., T] | tuple[Callable[..., T], str]', *args, **kwargs) -> 'T': Apply func(self, \\*args, \\*\\*kwargs).\n# frame_apply(obj: 'DataFrame', func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type: 'str | None' = None, args=None, kwargs=None) -> 'FrameApply': construct and return a row or column based frame apply object.\n# to_feather(self, path: 'FilePathOrBuffer[AnyStr]', **kwargs) -> 'None': Write a DataFrame to the binary Feather format.\n# [end]\nimport pandas as pd\n\ndef add_zeros_to_string(df, col_name):\n    # Add Leading Zeros to Strings at `col_name` in Pandas Dataframe\n    # The maximum length of the string is 15\n    # Return the dataframe\n    df[col_name] = df[col_name].apply(lambda x: x.zfill(15))\n    return df", "error": "", "token_usage": [[392, 318]], "time_usage": [8.804664134979248]}
{"task_id": "PandasEval/86", "success": true, "generated_code": "# [start]\n# to_dict(self, orient: 'str' = 'dict', into=<class 'dict'>): Convert the DataFrame to a dictionary.\n# append(self, other: 'Index | Sequence[Index]') -> 'Index': Append a collection of Index options together.\n# frame_apply(obj: 'DataFrame', func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type: 'str | None' = None, args=None, kwargs=None) -> 'FrameApply': construct and return a row or column based frame apply object.\n# from_range(data: 'range', name=None, dtype: 'Dtype | None' = None) -> 'RangeIndex': Create RangeIndex from a range object.\n# view(self, dtype: 'Dtype | None' = None) -> 'Series': Create a new view of the Series.\n# [end]\nimport pandas as pd\n\ndef append_dict_to_df(df, dictionary):\n    # append dictionary to data frame\n    df = df.append(dictionary, ignore_index=True)\n    # return the data frame\n    return df", "error": "", "token_usage": [[321, 238]], "time_usage": [6.421739101409912]}
{"task_id": "PandasEval/87", "success": true, "generated_code": "# [start]\n# transform(self, func: 'AggFuncType', axis: 'Axis' = 0, *args, **kwargs) -> 'FrameOrSeriesUnion': Call ``func`` on self producing a Series with transformed values.\n# to_pydatetime(*args, **kwargs): Convert a Timestamp object to a native Python datetime object.\n# apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -> 'FrameOrSeriesUnion': Invoke function on values of Series.\n# replace(self, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=<class 'object'>, fold=None): Implements datetime.\n# slice_shift(self: 'FrameOrSeries', periods: 'int' = 1, axis=0) -> 'FrameOrSeries': Equivalent to `shift` without copying data.\n# [end]\nimport pandas as pd\n\ndef transform_timestamp_to_pydatetime(timestamp):\n    # transform timestamp to pydatetime object\n    return pd.to_pydatetime(timestamp)", "error": "", "token_usage": [[338, 241]], "time_usage": [6.915093183517456]}
{"task_id": "PandasEval/88", "success": true, "generated_code": "# [start]\n# asfreq(self, freq, method=None, how: 'str | None' = None, normalize: 'bool' = False, fill_value=None) -> 'Series': Convert time series to specified frequency.\n# value_counts(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, dropna: 'bool' = True): Return a Series containing counts of unique values.\n# mean(self, axis=0, *args, **kwargs): Mean of non-NA/null values.\n# round(self, decimals=0, *args, **kwargs) -> 'Series': Round each value in a Series to the given number of decimals.\n# period_range(start=None, end=None, periods: 'int | None' = None, freq=None, name=None) -> 'PeriodIndex': Return a fixed frequency PeriodIndex.\n# [end]\nimport pandas as pd\n\ndef get_percentage_of_each_gender(series):\n    # Given a pandas series that represents frequencies of a value, how can I turn those frequencies into percentages?\n    # Return the percentage of each gender.\n    total = series.sum()\n    percentages = (series / total) * 100\n    return percentages", "error": "", "token_usage": [[337, 261]], "time_usage": [7.112369537353516]}
{"task_id": "PandasEval/89", "success": true, "generated_code": "# [start]\n# divide(self, other, axis='columns', level=None, fill_value=None): Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n# div(self, other, axis='columns', level=None, fill_value=None): Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n# rdiv(self, other, axis='columns', level=None, fill_value=None): Get Floating division of dataframe and other, element-wise (binary operator `rtruediv`).\n# rfloordiv(self, other, axis='columns', level=None, fill_value=None): Get Integer division of dataframe and other, element-wise (binary operator `rfloordiv`).\n# truediv(self, other, axis='columns', level=None, fill_value=None): Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n# [end]\nimport pandas as pd\n\ndef divide_multiple_cols_by_first_col(df):\n    # I need to divide all ['B','C'] columns but the first column 'A' in a DataFrame by the first column.\n    # Return the result.\n    return df[['B', 'C']].divide(df['A'], axis='index')", "error": "", "token_usage": [[346, 266]], "time_usage": [7.090952157974243]}
{"task_id": "PandasEval/90", "success": true, "generated_code": "# [start]\n# floor(self, freq, ambiguous='raise', nonexistent='raise'): Return a new Timestamp floored to this resolution.\n# ceil(self, freq, ambiguous='raise', nonexistent='raise'): Return a new Timestamp ceiled to this resolution.\n# func(self) -> 'bool': Return True if the IntervalIndex is monotonic decreasing (only equal or decreasing values), else False.\n# pop(self, item: 'Hashable') -> 'Any': Return item and drops from series.\n# round(freq, ambiguous='raise', nonexistent='raise'): Round the Timestamp to the specified resolution.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef ceil_of_series(s):\n    # ceiling of a pandas series\n    # Return the result.\n    return np.ceil(s)", "error": "", "token_usage": [[257, 165]], "time_usage": [4.634165048599243]}
{"task_id": "PandasEval/91", "success": true, "generated_code": "# [start]\n# fillna(self, value=None, downcast=None): Fill NA/NaN values with the specified value.\n# delete(self, loc): Make new Index with passed location(-s) deleted.\n# dropna(self, how: 'str' = 'any') -> 'MultiIndex': Return Index without NA/NaN values.\n# isna(self) -> 'np.ndarray': Detect missing values.\n# isnull(self) -> 'np.ndarray': Detect missing values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef delete_all_nan_columns(df):\n    # Delete all columns that contain all NaN values\n    # Return the result.\n    return df.dropna(axis=1, how='all')", "error": "", "token_usage": [[238, 153]], "time_usage": [4.32499623298645]}
{"task_id": "PandasEval/92", "success": true, "generated_code": "# [start]\n# add(self, other, level=None, fill_value=None, axis=0): Return Addition of series and other, element-wise (binary operator `add`).\n# join(self, other, how: 'str' = 'left', level=None, return_indexers: 'bool' = False, sort: 'bool' = False): See Index.\n# radd(self, other, level=None, fill_value=None, axis=0): Return Addition of series and other, element-wise (binary operator `radd`).\n# take(self, indices: 'Sequence[int]', allow_fill: 'bool' = False, fill_value: 'Any' = None): Take elements from an array.\n# replace(self, to_replace=None, value=None, inplace: 'bool' = False, limit=None, regex: 'bool' = False, method: 'str' = 'pad'): Replace values given in `to_replace` with `value`.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'name': ['jon','sam','jane','bob'],\n           'age': [30,25,18,26],\n           'sex':['male','male','female','male']})\n\nrow = ['45', 'Dean', 'male']\n# add the row at top in df\ndf.loc[-1] = row\ndf.index = df.index + 1\n# resort the index by inplace\ndf.sort_index(inplace=True)", "error": "", "token_usage": [[394, 302]], "time_usage": [8.38658094406128]}
{"task_id": "PandasEval/93", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# assign(self, **kwargs) -> 'DataFrame': Assign new columns to a DataFrame.\n# shift(self, periods=1, freq: 'Frequency | None' = None, axis: 'Axis' = 0, fill_value=<no_default>) -> 'DataFrame': Shift index by desired number of periods with an optional time `freq`.\n# update(self, other, join: 'str' = 'left', overwrite: 'bool' = True, filter_func=None, errors: 'str' = 'ignore') -> 'None': Modify in place using non-NA values from another DataFrame.\n# isna(self) -> 'DataFrame': Detect missing values.\n# [end]\nimport pandas as pd\n\ndef set_value_to_entire_col(df, value):\n    # Set value to an entire column `B` of a pandas dataframe\n    # Return the changed dataframe.\n    df['B'] = value\n    return df", "error": "", "token_usage": [[332, 246]], "time_usage": [6.48500919342041]}
{"task_id": "PandasEval/94", "success": true, "generated_code": "intersection_result = s1.intersection(s2)", "error": "", "token_usage": [[329, 13]], "time_usage": [1.0380403995513916]}
{"task_id": "PandasEval/95", "success": true, "generated_code": "import pandas as pd\n\ndef get_first_n_rows(df, n):\n    # I would simply like to slice the Data Frame and take the first n rows.\n    # Return the result\n    return df.head(n)", "error": "", "token_usage": [[314, 48]], "time_usage": [1.910808801651001]}
{"task_id": "PandasEval/96", "success": true, "generated_code": "# [start]\n# sum(self, axis: 'int' = 0, min_count: 'int' = 0, *args, **kwargs) -> 'Scalar': Sum of non-NA/null values.\n# nansum(values: 'np.ndarray', *, axis: 'int | None' = None, skipna: 'bool' = True, min_count: 'int' = 0, mask: 'np.ndarray | None' = None) -> 'float': Sum the elements along an axis ignoring NaNs.\n# nanvar(values, *, axis=None, skipna=True, ddof=1, mask=None): Compute the variance along given axis while ignoring NaNs.\n# cumsum(self, axis=0, *args, **kwargs): Cumulative sum of non-NA/null values.\n# nanmean(values: 'np.ndarray', *, axis: 'int | None' = None, skipna: 'bool' = True, mask: 'np.ndarray | None' = None) -> 'float': Compute the mean of the element along an axis ignoring NaNs.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Apples': [2, 1, np.nan],\n              'Bananas': [3, 3, 7],\n              'Grapes': [np.nan, 2, 3],})\n\n# Add a new column named 'Fruit Total' that sums the values of the other columns\n# Note that igonring the NaN values\n\ndf['Fruit Total'] = df.sum(axis=1, skipna=True)", "error": "", "token_usage": [[418, 337]], "time_usage": [9.29599928855896]}
{"task_id": "PandasEval/97", "success": true, "generated_code": "# [start]\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# itertuples(self, index: 'bool' = True, name: 'str | None' = 'Pandas') -> 'Iterable[tuple[Any, ...]]': Iterate over DataFrame rows as namedtuples.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# eval(expr: 'str | BinOp', parser: 'str' = 'pandas', engine: 'str | None' = None, truediv=<no_default>, local_dict=None, global_dict=None, resolvers=(), level=0, target=None, inplace=False): Evaluate a Python expression as a string using various backends.\n# arrays_to_mgr(arrays, arr_names: 'Index', index, columns, *, dtype: 'DtypeObj | None' = None, verify_integrity: 'bool' = True, typ: 'str | None' = None, consolidate: 'bool' = True) -> 'Manager': Segregate Series based on type and coerce into matrices.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef find_non_numeric_rows(df):\n    # Finding non-numeric rows in dataframe in pandas\n    # Return the raws that contain non-numeric values\n    # So to get the subDataFrame of rouges, (Note: the negation, ~, of the above finds the ones which have at least one rogue non-numeric):\n    return df[~df.applymap(np.isreal).all(1)]", "error": "", "token_usage": [[468, 386]], "time_usage": [11.242918491363525]}
{"task_id": "PandasEval/98", "success": true, "generated_code": "# [start]\n# combine(self, other: 'DataFrame', func, fill_value=None, overwrite: 'bool' = True) -> 'DataFrame': Perform column-wise combine with another DataFrame.\n# merge_ordered(left: 'DataFrame', right: 'DataFrame', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_by=None, right_by=None, fill_method: 'str | None' = None, suffixes: 'Suffixes' = ('_x', '_y'), how: 'str' = 'outer') -> 'DataFrame': Perform merge with optional filling/interpolation.\n# join(self, other: 'FrameOrSeriesUnion', on: 'IndexLabel | None' = None, how: 'str' = 'left', lsuffix: 'str' = '', rsuffix: 'str' = '', sort: 'bool' = False) -> 'DataFrame': Join columns of another DataFrame.\n# combine_first(self, other: 'DataFrame') -> 'DataFrame': Update null elements with value in the same location in `other`.\n# merge(left: 'DataFrame | Series', right: 'DataFrame | Series', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'DataFrame': Merge DataFrame or named Series objects with a database-style join.\n# [end]\nimport pandas as pd\n\ndf1 = pd.DataFrame({'staff':[1,4], 'company':[100,301]})\ndf2 = pd.DataFrame({'person':[1,2], 'company':[100,300]})\n# merge the above two dataframes on column 'company'\nmerged_df = df1.merge(df2, on='company')", "error": "", "token_usage": [[543, 455]], "time_usage": [12.642623662948608]}
{"task_id": "PandasEval/99", "success": true, "generated_code": "# [start]\n# duplicated(self, keep='first') -> 'Series': Indicate duplicate Series values.\n# isna(self) -> 'Series': Detect missing values.\n# drop_duplicates(self, keep='first', inplace=False) -> 'Series | None': Return Series with duplicate values removed.\n# repeat(self, repeats, axis=None) -> 'Series': Repeat elements of a Series.\n# isnull(self) -> 'Series': Detect missing values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A':[1,4], 'B':[np.nan,301]})\n# # counting the number of missing/NaN in each column\n# Get a series with the number of missing/NaN in each column\ncount_series = df.isnull().sum()", "error": "", "token_usage": [[258, 167]], "time_usage": [4.935859680175781]}
{"task_id": "PandasEval/100", "success": true, "generated_code": "# [start]\n# Term(where, queryables: 'dict[str, Any] | None' = None, encoding=None, scope_level: 'int' = 0): Hold a pytables-like expression, comprised of possibly multiple 'terms'.\n# isna(self) -> 'np.ndarray': Boolean NumPy array indicating if each value is missing.\n# to_tuples(self, na_tuple=True) -> 'np.ndarray': Return an ndarray of tuples of the form (left, right).\n# kleene_or(left: 'bool | np.ndarray', right: 'bool | np.ndarray', left_mask: 'np.ndarray | None', right_mask: 'np.ndarray | None'): Boolean ``or`` using Kleene logic.\n# isnull(self) -> 'np.ndarray': Detect missing values.\n# [end]\nimport pandas as pd\ndf = pd.DataFrame({'col': [\"apple\",\n                           \"pear\",\n                           \"strawberry\"]})\ntargets = ['apple', 'banana']\n# Any word from `targets` are present in sentence.\nresult = df['col'].isin(targets)\n# [end]", "error": "", "token_usage": [[313, 226]], "time_usage": [6.435535430908203]}
{"task_id": "PandasEval/34", "success": true, "generated_code": "# [start]\n# groupby(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, dropna: 'bool' = True) -> 'DataFrameGroupBy': Group DataFrame using a mapper or by a Series of columns.\n# sum(self, axis: 'int' = 0, min_count: 'int' = 0, *args, **kwargs) -> 'Scalar': Sum of non-NA/null values.\n# aggregate(self, func=None, axis: 'Axis' = 0, *args, **kwargs): Aggregate using one or more operations over the specified axis.\n# agg(self, func=None, axis=0, *args, **kwargs): Aggregate using one or more operations over the specified axis.\n# dict_to_mgr(data: 'dict', index, columns, *, dtype: 'DtypeObj | None' = None, typ: 'str' = 'block', copy: 'bool' = True) -> 'Manager': Segregate Series based on type and coerce into matrices.\n# [end]\nimport pandas as pd\n\ndef f(x):\n    a = x['Value'].iat[2] - x['Value'].iat[1]\n    b = x['Value'].iat[3] - x['Value'].iat[0]\n    c = x['ID'].iat[2] + ' - ' + x['ID'].iat[1]\n    d = x['ID'].iat[3] + ' - ' + x['ID'].iat[0]\n    return pd.DataFrame({'Value': [a,b], 'ID':[c,d]})\n\ndef calculate_row_diff_groupwise(df):\n    # I need to calculate the difference between two rows groupwise using pandas.\n    # To calculate the sum I would use pandas.groupby('Group').sum(), but how do you calculate the difference between rows where the row ordering is important?\n    # I think we need custom function with apply which return DataFrame for each group, for select by position is used iat:\n    # Return the result\n    return df.groupby('Group').apply(f)", "error": "", "token_usage": [[576, 490]], "time_usage": [13.364935636520386]}
{"task_id": "PandasEval/27", "success": true, "generated_code": "# [start]\n# mean(self, axis=0, *args, **kwargs): Mean of non-NA/null values.\n# mad(self, axis=None, skipna=None, level=None): Return the mean absolute deviation of the values over the requested axis.\n# sem(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return unbiased standard error of the mean over requested axis.\n# std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return sample standard deviation over requested axis.\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# [end]\nimport pandas as pd\n\ndef normalize(df):\n    # Normalization using pandas\n    # We simply subtract the mean and divide by standard deviation on df.iloc[:,0,-1] obj with axis is zero.\n    # Return the normalized dataframe\n    return (df - df.mean()) / df.std()", "error": "", "token_usage": [[328, 242]], "time_usage": [6.384843111038208]}
