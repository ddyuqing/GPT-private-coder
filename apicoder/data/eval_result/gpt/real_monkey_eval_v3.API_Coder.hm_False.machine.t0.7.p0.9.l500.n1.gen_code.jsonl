{"task_id": "PandasEval/0", "success": true, "generated_code": "import monkey as mk\ndef compute_average_along_rows(kf):\n    # You can specify a new column named `average_along_rows` that contains the average of each row. You also need to compute the average along the rows, so use axis=1.\n    # Finally, return the knowledgeframe with the new column.\n    kf['average_along_rows'] = kf.mean(axis=1)\n    return kf", "error": "", "token_usage": [[170, 90]], "time_usage": [3.0934154987335205]}
{"task_id": "PandasEval/1", "success": true, "generated_code": "import monkey as mk\n\ndef select_rows_from_column(kf, col_name, values):\n    # How do I select rows from a KnowledgeFrame kf based on column values?\n    # Return rows whose column value named `col_name` is in an iterable `values`\n    \n    # Use the `isin` method of the column to check if values are present\n    mask = kf[col_name].isin(values)\n    \n    # Use the mask to filter the KnowledgeFrame\n    selected_rows = kf[mask]\n    \n    return selected_rows", "error": "", "token_usage": [[156, 112]], "time_usage": [3.2073071002960205]}
{"task_id": "PandasEval/2", "success": true, "generated_code": "import monkey as mk\n\ndef change_col_names_of_kf(kf, origin_names, new_names):\n    \"\"\"\n    How do I change the column labels of kf\uff1f\n    And return the knowledgeframe that has been renamed\n    \"\"\"\n    # Use the 'rename_columns' function from the 'mk' module to change the column labels of kf\n    renamed_kf = mk.rename_columns(kf, origin_names, new_names)\n    \n    # Return the renamed knowledgeframe\n    return renamed_kf", "error": "", "token_usage": [[147, 106]], "time_usage": [4.33617639541626]}
{"task_id": "PandasEval/3", "success": true, "generated_code": "import monkey as mk\n\ndef delete_column(kf, column_name):\n    # deleting a column from a Monkey KnowledgeFrame\n    # return the changed knowledgeframe\n    kf = kf.drop(column_name, axis=1)\n    return kf", "error": "", "token_usage": [[136, 53]], "time_usage": [2.0941321849823]}
{"task_id": "PandasEval/4", "success": true, "generated_code": "import monkey as mk\n\ndef select_multiple_columns(kf, columns):\n    # How do I select the given columns and return the new KnowledgeFrame?\n    new_kf = kf.select_columns(columns)\n    return new_kf", "error": "", "token_usage": [[132, 50]], "time_usage": [1.9762725830078125]}
{"task_id": "PandasEval/5", "success": true, "generated_code": "import monkey as mk\n\ndef get_row_count(kf):\n    \"\"\"\n    Return the row count of kf\n    \"\"\"\n    return kf.row_count()", "error": "", "token_usage": [[126, 34]], "time_usage": [1.523017406463623]}
{"task_id": "PandasEval/6", "success": true, "generated_code": "import monkey as mk\n\ndef get_list_from_knowledgeframe(kf):\n    # I want to get a list of the column headers from a Monkey KnowledgeFrame. \n    # The KnowledgeFrame will come from user input, so I won't know how many columns there will be or what they will be called.\n    # Return a list of the column headers.\n    return list(kf.columns)", "error": "", "token_usage": [[175, 84]], "time_usage": [2.8243470191955566]}
{"task_id": "PandasEval/7", "success": true, "generated_code": "import monkey as mk\n\ndef add_column_to_knowledgeframe(kf, column_name, column_data):\n    # How to add a new column to an existing KnowledgeFrame?\n    # I would like to add a new column data with the column name, to the existing knowledgeframe\n    kf[column_name] = column_data\n    return kf", "error": "", "token_usage": [[159, 74]], "time_usage": [2.501267671585083]}
{"task_id": "PandasEval/8", "success": true, "generated_code": "import monkey as mk\n\ndef change_all_cols_type(kf):\n    # Change all columns type of KnowledgeFrame to numeric\n    # And return the new KnowledgeFrame\n    # The code is:\n    kf = kf.astype(float)\n    return kf", "error": "", "token_usage": [[142, 55]], "time_usage": [2.106919288635254]}
{"task_id": "PandasEval/9", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef sip_rows_col_nan(kf, col_name):\n    # How to sip rows of Monkey KnowledgeFrame whose value in a certain column is NaN\n    return kf[kf[col_name].isna()]", "error": "", "token_usage": [[143, 55]], "time_usage": [2.083015203475952]}
{"task_id": "PandasEval/11", "success": true, "generated_code": "from typing import List\nimport monkey as mk\nimport numpy as np\n\ndef adding_in_knowledgeframe(kf, list_to_add, column_name_list) -> mk.KnowledgeFrame:\n    \"\"\"\n    Params:\n        kf: The knowledgeframe to add to.\n        list_to_add: The list to add.\n        column_name_list: The column names of the list to add.\n\n    Returns:\n        The knowledgeframe with the list added.\n    \"\"\"\n    # Convert the list_to_add to a numpy array\n    array_to_add = np.array(list_to_add)\n    \n    # Check if the number of columns in kf matches the number of columns in column_name_list\n    if kf.shape[1] != len(column_name_list):\n        raise ValueError(\"Number of columns in kf does not match the length of column_name_list\")\n    \n    # Create a new knowledgeframe with the array_to_add and column_name_list\n    new_kf = mk.KnowledgeFrame(array_to_add, column_name_list)\n    \n    # Append the new_kf to kf\n    kf.append(new_kf)\n    \n    # Return the updated knowledgeframe\n    return kf", "error": "", "token_usage": [[194, 237]], "time_usage": [6.797594308853149]}
{"task_id": "PandasEval/12", "success": true, "generated_code": "import monkey as mk\n\ndef extract_the_last_year(kf, column_name):\n    # I am trying to extract the last year (YY) of a fiscal date string in the format of YYYY-YY.\n    # e.g The last year of this '1999-00' would be 2000.\n    # I need a logic to include a case where if it is the end of the century then my employ method should add to the first two digits.\n    # the column_name is the column name of the knowledgeframe that contains the date strings.\n    # return the numerical Collections obj of the last year.\n\n    last_year = kf[column_name].apply(lambda x: int(x.split('-')[0][-2:]) + int(x.split('-')[1]))\n    return last_year", "error": "", "token_usage": [[227, 163]], "time_usage": [4.623055696487427]}
{"task_id": "PandasEval/13", "success": true, "generated_code": "import monkey as mk\n\ndef get_last_n_rows(kf, n):\n    # How to get the last N rows of a monkey KnowledgeFrame?\n    # We can use the tail() method of the KnowledgeFrame to get the last N rows.\n    # The tail() method returns the last N rows of the KnowledgeFrame.\n    # We can specify the number of rows to be returned using the 'n' parameter.\n    # Here, we will use the tail() method to get the last N rows and return the result.\n    return kf.tail(n)", "error": "", "token_usage": [[132, 117]], "time_usage": [3.4921071529388428]}
{"task_id": "PandasEval/14", "success": true, "generated_code": "import monkey as mk\n\ndef get_values_at_nth_rows(kf, n, column_name):\n    \"\"\"\n    how do I get the value at an nth row of a given column name in Monkey?\n    return the value\n    \"\"\"\n    return kf[column_name][n]", "error": "", "token_usage": [[149, 59]], "time_usage": [2.225402593612671]}
{"task_id": "PandasEval/15", "success": true, "generated_code": "import monkey as mk\n\ndef creating_kf_with_same_as_other(kf_original):\n    # creating a new knowledgeframe of all same with kf_original one, but no any rows\n    # return the new knowledgeframe\n    new_kf = mk.KnowledgeFrame(kf_original.columns)\n    return new_kf", "error": "", "token_usage": [[147, 69]], "time_usage": [2.5100302696228027]}
{"task_id": "PandasEval/20", "success": true, "generated_code": "new_kf = kf.groupby(['Country', 'Item_Code'])[['Y1961', 'Y1962', 'Y1963']].sum().reset_index()", "error": "", "token_usage": [[291, 37]], "time_usage": [1.7113268375396729]}
{"task_id": "PandasEval/10", "success": true, "generated_code": "import monkey as mk\n\n# creating a Collections from a list [56, 24, 421, 90]\nmy_collections = mk.Collections([56, 24, 421, 90])\n\n# printing the collection\nprint(my_collections)\n\n# getting the length of the collection\nlength = my_collections.length()\nprint(length)\n\n# getting the maximum value in the collection\nmaximum = my_collections.maximum()\nprint(maximum)\n\n# getting the minimum value in the collection\nminimum = my_collections.minimum()\nprint(minimum)\n\n# getting the sum of all values in the collection\nsum = my_collections.sum()\nprint(sum)\n\n# getting the average value of the collection\naverage = my_collections.average()\nprint(average)\n\n# checking if a specific value exists in the collection\nvalue_exists = my_collections.exists(56)\nprint(value_exists)\n\n# sorting the collection in ascending order\nmy_collections.sort_ascending()\n\n# printing the sorted collection\nprint(my_collections)\n\n# sorting the collection in descending order\nmy_collections.sort_descending()\n\n# printing the sorted collection\nprint(my_collections)\n\n# reversing the order of the collection\nmy_collections.reverse()\n\n# printing the reversed collection\nprint(my_collections)", "error": "", "token_usage": [[128, 244]], "time_usage": [7.555184841156006]}
{"task_id": "PandasEval/16", "success": true, "generated_code": "# Clip the values of `col_1` between -2 to 2 if `col_0` is `a`.\n# Using `clip` function in monkey.\nkf.loc[kf['col_0']=='a','col_1'] = kf.loc[kf['col_0']=='a','col_1'].clip(-2, 2)", "error": "", "token_usage": [[223, 77]], "time_usage": [3.0401790142059326]}
{"task_id": "PandasEval/17", "success": true, "generated_code": "kf = kf.apply(lambda col: col.where(col > col.mean(), np.nan), axis=0).dropna(how='all')", "error": "", "token_usage": [[231, 32]], "time_usage": [1.8191957473754883]}
{"task_id": "PandasEval/18", "success": true, "generated_code": "unionerd_collections = target_collections + source_collections", "error": "", "token_usage": [[173, 14]], "time_usage": [1.0497548580169678]}
{"task_id": "PandasEval/19", "success": true, "generated_code": "nan_kf = kf.select_rows(lambda row: np.isnan(row['x2']))", "error": "", "token_usage": [[214, 21]], "time_usage": [1.335705041885376]}
{"task_id": "PandasEval/21", "success": true, "generated_code": "import monkey as mk\n\na = [['a', '1.2'], ['b', '70'], ['x', '5']]\n# I want to convert a table, represented as a list of lists, into a monkey KnowledgeFrame.\n# The columns are ['one', 'two']\n# What is the best way to convert the columns to the appropriate types, in this case the 'two' column into floats?\nkf = mk.KnowledgeFrame(a, columns=['one', 'two'])\nkf['two'] = kf['two'].astype(float)", "error": "", "token_usage": [[188, 115]], "time_usage": [3.6372456550598145]}
{"task_id": "PandasEval/22", "success": true, "generated_code": "def change_dtype(kf, cols):\n    \"\"\"\n    Change the dtype of specified columns to float32.\n    \n    Parameters:\n    kf (KnowledgeFrame): The KnowledgeFrame object.\n    cols (list): A list of column names whose dtypes need to be changed.\n    \n    Returns:\n    KnowledgeFrame: The updated KnowledgeFrame object with changed dtypes.\n    \"\"\"\n    for col in cols:\n        kf[col] = kf[col].astype(np.float32)\n    return kf", "error": "", "token_usage": [[199, 101]], "time_usage": [3.5485987663269043]}
{"task_id": "PandasEval/23", "success": true, "generated_code": "import monkey as mk\nkf = mk.KnowledgeFrame({'col1': [1,2,3], 'col2': ['Jimmy','Tom','Jimmy']})\n# I have a knowledgeframe that has two columns, the second column is one of only a few values. \n# I want to return a knowledgeframe where only the rows where that col2 had a specific value 'Jimmy' are included.\nnew_kf = kf[kf['col2'] == 'Jimmy']", "error": "", "token_usage": [[189, 102]], "time_usage": [3.625670909881592]}
{"task_id": "PandasEval/24", "success": true, "generated_code": "# Importing the necessary module\nimport monkey as mk\n\n# Creating a KnowledgeFrame object\nkf = mk.KnowledgeFrame({'MSRA': [10, 11, 12], 'THU': [100, 110, 120]})\n\n# Resetting the index of the KnowledgeFrame\nkf = kf.reseting_index()  # make sure indexes pair with number of rows\n\n# Creating an empty dictionary to store the rows data\nrows_dict = {}  # {MSRA: THU, ...}\n\n# Traversing through each row in the KnowledgeFrame\nfor index, row in kf.traversal():\n    # Extracting the values for 'MSRA' and 'THU' columns\n    msra_value = row['MSRA']\n    thu_value = row['THU']\n    \n    # Adding the values to the rows_dict dictionary\n    rows_dict[msra_value] = thu_value", "error": "", "token_usage": [[235, 192]], "time_usage": [5.724344730377197]}
{"task_id": "PandasEval/25", "success": true, "generated_code": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1000, 765, 800], 'B': [10, 5, 7]})\n# I have a knowledgeframe in monkey where each column has different value range.\n# Any idea how I can normalize the columns of this knowledgeframe where each value is between 0 and 1?\n\ndef normalize(kf):\n    \"\"\"\n    Normalizes the columns of the given knowledge frame where each value is between 0 and 1.\n    \n    Args:\n        kf (KnowledgeFrame): The knowledge frame to be normalized.\n    \n    Returns:\n        KnowledgeFrame: The normalized knowledge frame.\n    \"\"\"\n    normalized_kf = kf.copy()\n    for column in normalized_kf.columns:\n        min_value = min(normalized_kf[column])\n        max_value = max(normalized_kf[column])\n        normalized_kf[column] = [(value - min_value) / (max_value - min_value) for value in normalized_kf[column]]\n    return normalized_kf\n\nnormalized_kf = normalize(kf)", "error": "", "token_usage": [[182, 222]], "time_usage": [6.878814458847046]}
{"task_id": "PandasEval/26", "success": true, "generated_code": "import monkey as mk\n\n# I want to create a knowledgeframe with one of the column as a list or array.\nkf = mk.KnowledgeFrame({'Name':['Juda','Pri']})\nemails = {'a@a.com','b@b.com'}\nkf['Email'] = ''\n# After you assign a list like or array like value to the columns, the column should be considered as type object\n# Now I want to assign the emails to first row and the 'Email' column\n\n# Complete the code to assign the emails to the 'Email' column of the first row\nkf['Email'][0] = emails", "error": "", "token_usage": [[201, 131]], "time_usage": [4.159596920013428]}
{"task_id": "PandasEval/28", "success": true, "generated_code": "import monkey as mk\n\ndef is_kf_exist(kf):\n    # In my code, I have several variables which can either contain a monkey KnowledgeFrame or nothing at all.\n    # Let's say I want to test and see if a certain KnowledgeFrame has been created yet or not.\n\n    if kf is None:\n        return False\n    else:\n        return True", "error": "", "token_usage": [[161, 80]], "time_usage": [2.947930097579956]}
{"task_id": "PandasEval/29", "success": true, "generated_code": "n_kf = kf.filter(lambda row: row['line_num'] != 0)", "error": "", "token_usage": [[187, 22]], "time_usage": [1.4649620056152344]}
{"task_id": "PandasEval/30", "success": true, "generated_code": "import monkey as mk\n\nweb_stats = {'Day': [1, 2, 3, 4, 2, 6],\n             'Visitors': [43, 43, 34, 23, 43, 23],\n             'Bounce_Rate': [3, 2, 4, 3, 5, 5]}\nkf = mk.KnowledgeFrame(web_stats)\n# I would like to sip all data in a monkey knowledgeframe\n# Using kf.index to sip all rows\n\ndef sip_all_data(kf):\n    \"\"\"\n    Sips all data in a monkey knowledgeframe.\n    \n    Parameters:\n        kf (KnowledgeFrame): The monkey knowledgeframe.\n        \n    Returns:\n        None\n    \"\"\"\n    kf.index\n\nsip_all_data(kf)", "error": "", "token_usage": [[211, 167]], "time_usage": [5.532770156860352]}
{"task_id": "PandasEval/31", "success": true, "generated_code": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n# I would like to add a new column C that is the sum value of A and B cell.\n\ndef add_column(kf):\n    kf['C'] = kf['A'] + kf['B']\n    return kf\n\nkf = add_column(kf)", "error": "", "token_usage": [[158, 92]], "time_usage": [3.1663622856140137]}
{"task_id": "PandasEval/32", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\nkf = mk.KnowledgeFrame({'A': [1, 4, 7, np.nan], 'B': [np.nan, 2, 5, np.nan], 'C': [np.nan, np.nan, 3, 6]})\n\n# Move next value to first empty row monkey\n# how do i move each value from a column to the first empty \"row/cell\" in monkey?\n# use sorted to align non NULL data at the top, use sipna to sip all rows with all NaN\n\nnew_kf = kf.apply(lambda x: sorted(x, key=lambda y: np.isnan(y)))\nnew_kf = new_kf.apply(lambda x: np.nan_to_num(x, nan=np.nan))", "error": "", "token_usage": [[224, 164]], "time_usage": [4.650510311126709]}
{"task_id": "PandasEval/33", "success": true, "generated_code": "import monkey as mk\n\ndef make_knowledgeframe_column_headers_lowercase(data):\n    # I want to make all column headers in my monkey data frame lower case\n    data.columns = [col.lower() for col in data.columns]", "error": "", "token_usage": [[134, 50]], "time_usage": [1.9416882991790771]}
{"task_id": "PandasEval/35", "success": true, "generated_code": "first_value = kf['a'].nsmallest(1).iloc[0]", "error": "", "token_usage": [[185, 21]], "time_usage": [1.71787428855896]}
{"task_id": "PandasEval/36", "success": true, "generated_code": "unique_ndarray = np.unique(kf.values.flat_underlying)", "error": "", "token_usage": [[227, 17]], "time_usage": [1.1557657718658447]}
{"task_id": "PandasEval/37", "success": true, "generated_code": "final_item_kf = kf.sort_values('date', ascending=False).groupby('id').first()", "error": "", "token_usage": [[305, 25]], "time_usage": [1.3739511966705322]}
{"task_id": "PandasEval/38", "success": true, "generated_code": "import monkey as mk\n\ndef sip2rows_zero(kf):\n    # i want to sip 2 rows in the knowledgeframe if zero comes in the column\n    # if 0 comes on odd index sip previous row as well as current row using monkey\n    # Assuming your knowledgeframe is indexed starting from 0\n    # Rows with column2 = 0 and on odd index\n    idx = kf[(kf['column2'] == 0) & (kf.index % 2 == 1)].index\n    # The rows above them\n    idx = idx.union(idx-1)\n    # A new knowledgeframe with those rows removed\n    new_kf = kf.drop(idx)\n    \n    return new_kf", "error": "", "token_usage": [[237, 154]], "time_usage": [4.815505743026733]}
{"task_id": "PandasEval/39", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef shift_column_up_by_one(kf):\n    # Shift column in monkey knowledgeframe up by one?\n    # In detail, in 'gdp' column, shift up by one and return knowledgeframe with the changed gdp column.\n    kf['gdp'] = np.roll(kf['gdp'], 1)\n    return kf", "error": "", "token_usage": [[160, 84]], "time_usage": [2.993245840072632]}
{"task_id": "PandasEval/40", "success": true, "generated_code": "new_kf = kf.select_dtypes(include=np.float64)", "error": "", "token_usage": [[182, 17]], "time_usage": [1.2957215309143066]}
{"task_id": "PandasEval/41", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef unioner_kf(kf1, kf2):\n    # How to unioner two knowledgeframes with different column names but same number of rows?\n    # I have two different data frames in monkey. Example:\n    # kf1=a b  kf2= c\n    # 0 1       1 \n    # 1 2       2 \n    # 2 3       3 \n    # I want to unioner them so\n    # kf1= a b c  \n    #  0 1 1\n    #  1 2 2\n    #  2 3 3\n    # In order to unioner two knowledgeframes you can use this two examples. Both returns the same goal\n    # Using unioner plus additional arguments instructing it to use the indexes\n    # Specially, we can set left_index and right_index to True\n    \n    # Example 1: Using the pandas merge function\n    kf_unioned = kf1.merge(kf2, left_index=True, right_index=True)\n    \n    # Example 2: Using the pandas concat function\n    kf_unioned = pd.concat([kf1, kf2], axis=1)\n    \n    return kf_unioned", "error": "", "token_usage": [[302, 274]], "time_usage": [8.555461168289185]}
{"task_id": "PandasEval/42", "success": true, "generated_code": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1, 2, 3],'B': [100, 300, 500],'C': list('abc')})\n\n# How can I delete multiple columns in one pass?\n# In detail, I would like to delete columns A and C, but I don't know how to do it in one pass.\nnew_kf = kf.drop(['A', 'C'], axis=1)", "error": "", "token_usage": [[185, 99]], "time_usage": [3.553060293197632]}
{"task_id": "PandasEval/43", "success": true, "generated_code": "import monkey as mk\n\ndef get_value_counts(kf):\n    # I want to get the counts of distinctive values of the knowledgeframe. count_values implements this however I want to use its output somewhere else. \n    # How can I convert .count_values output to a monkey knowledgeframe.\n    # Use renaming_axis('distinctive_values') for name ('counts') of column from index and reseting_index\n    # return the final knowledgeframe\n    \n    counts = kf.count_values()  # Get the counts of distinctive values\n    \n    counts = counts.rename_axis('distinctive_values').reset_index(name='counts')  # Rename the column from index and reset the index\n    \n    return mk.KnowledgeFrame(counts)  # Convert the output to a monkey knowledgeframe", "error": "", "token_usage": [[193, 160]], "time_usage": [4.576988935470581]}
{"task_id": "PandasEval/44", "success": true, "generated_code": "data.columns = ['a', 'b', 'c']", "error": "", "token_usage": [[175, 16]], "time_usage": [1.1053543090820312]}
{"task_id": "PandasEval/45", "success": true, "generated_code": "import monkey as mk\n\ndef make_kf_all_cols_lower(data):\n    # I want to make all column headers in my monkey data frame lower case\n    # Return the changed knowledgeframe\n    data.columns = [col.lower() for col in data.columns]\n    return data", "error": "", "token_usage": [[141, 61]], "time_usage": [2.2674341201782227]}
{"task_id": "PandasEval/46", "success": true, "generated_code": "sample_by_num = kf.group_by(\"section\").sample_by_num(n=50)", "error": "", "token_usage": [[256, 21]], "time_usage": [1.4001917839050293]}
{"task_id": "PandasEval/47", "success": true, "generated_code": "def remove_numbers_from_column(column):\n    \"\"\"\n    Removes all the numbers from the given column in the KnowledgeFrame.\n\n    Parameters:\n    column (str): The name of the column.\n\n    Returns:\n    None\n    \"\"\"\n    kf[column] = [name for name in kf[column] if not any(char.isdigit() for char in name)]", "error": "", "token_usage": [[225, 73]], "time_usage": [2.948509693145752]}
{"task_id": "PandasEval/48", "success": true, "generated_code": "new_kf = kf.groupby('Mt').apply(lambda x: x[x['num'] == x['num'].max()])", "error": "", "token_usage": [[292, 29]], "time_usage": [1.59639310836792]}
{"task_id": "PandasEval/49", "success": true, "generated_code": "# transfer column date to datetime type\n# when there is a string that is not capable of being turned into datetime format, skip that row,\n# use errors='coerce' for this\nkf['date'] = pd.to_datetime(kf['date'], errors='coerce')", "error": "", "token_usage": [[205, 61]], "time_usage": [2.3530635833740234]}
{"task_id": "PandasEval/50", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef if_any_value_is_nan(kf):\n    # How to check if any value is NaN in a Monkey KnowledgeFrame? Return the result.\n    result = kf.isnull().values.any()\n    return result", "error": "", "token_usage": [[141, 57]], "time_usage": [2.085405111312866]}
{"task_id": "PandasEval/51", "success": true, "generated_code": "import monkey as mk\n\ndef sorting_columns_based_on_column_name(kf):\n    # Sorting columns in monkey knowledgeframe based on column name\n    # Note that axis is one\n    kf = kf.sort_index(axis=1)\n    return kf", "error": "", "token_usage": [[138, 54]], "time_usage": [2.1601362228393555]}
{"task_id": "PandasEval/52", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef get_value_when_condition(kf):\n    # How can I get the values of column `A` when column `B`=3?\n    return kf[kf['B'] == 3]['A']", "error": "", "token_usage": [[140, 56]], "time_usage": [2.1553421020507812]}
{"task_id": "PandasEval/53", "success": true, "generated_code": "import monkey as mk\n\ndef get_average_in_column(kf, col_name):\n    # return the column average/mean\n    return kf[col_name].mean()", "error": "", "token_usage": [[127, 37]], "time_usage": [1.7055068016052246]}
{"task_id": "PandasEval/54", "success": true, "generated_code": "import monkey as mk\n\ndef combine_kf(kf1, kf2):\n    \"\"\"\n    How do I combine two knowledgeframes with ignore index? Return the concated knowledgeframe.\n    \"\"\"\n    return kf1.concat(kf2, ignore_index=True)", "error": "", "token_usage": [[138, 56]], "time_usage": [2.16623854637146]}
{"task_id": "PandasEval/55", "success": true, "generated_code": "repeated_x = mk.concat([x] * 5)", "error": "", "token_usage": [[180, 17]], "time_usage": [1.1994075775146484]}
{"task_id": "PandasEval/56", "success": true, "generated_code": "import monkey as mk\n\ndef knowledgeframe2list_of_dict(kf):\n    # Monkey KnowledgeFrame to List of Dictionaries\n    # Use kf.convert_dict() to solve it and return the result\n    return kf.convert_dict()", "error": "", "token_usage": [[143, 51]], "time_usage": [2.1248867511749268]}
{"task_id": "PandasEval/57", "success": true, "generated_code": "import monkey as mk\n\ndef convert_column_to_date(kf):\n    # Convert Column `Date` to Date Format using monkey function\n    # return the converted knowledgeframe\n    \n    kf[\"Date\"] = mk.to_date(kf[\"Date\"])\n    \n    return kf", "error": "", "token_usage": [[138, 58]], "time_usage": [2.2865397930145264]}
{"task_id": "PandasEval/58", "success": true, "generated_code": "import monkey as mk\n\ndef counting_consecutive_positive_values(y):\n    # Counting consecutive positive values in Python/monkey array\n    # I'm trying to count consecutive up days in equity return data; so if a positive day is 1 and a negative is 0, a list y=[0,0,1,1,1,0,0,1,0,1,1] should return z=[0,0,1,2,3,0,0,1,0,1,2].\n    # Initialize count and result list\n    count = 0\n    result = []\n    \n    # Iterate over the input list y\n    for value in y:\n        # Check if the value is positive\n        if value > 0:\n            # Increment count and append to result\n            count += 1\n            result.append(count)\n        else:\n            # Reset count to 0\n            count = 0\n            result.append(count)\n    \n    # Return the result\n    return result", "error": "", "token_usage": [[216, 215]], "time_usage": [6.7651917934417725]}
{"task_id": "PandasEval/59", "success": true, "generated_code": "import monkey as mk\n\ndef insert_row_at_arbitrary_in_knowledgeframe(kf, row_to_insert):\n    \"\"\"\n    Inserts a row into a knowledgeframe at a specified row with no ignore index, and sort & reset the index with sip=True.\n    Returns the new knowledgeframe.\n    \"\"\"\n    kf = kf.append(row_to_insert)\n    kf = kf.sort_index()\n    kf = kf.reset_index(drop=True)\n    return kf", "error": "", "token_usage": [[164, 93]], "time_usage": [3.057914972305298]}
{"task_id": "PandasEval/60", "success": true, "generated_code": "import monkey as mk\n\ndef get_data_frame_from_list(list_of_lists):\n    # list_of_lists format: [header, [row1], [row2], ...]\n    # header format: [column1, column2, ...]\n    # row format: [value1, value2, ...]\n    # How to convert list to knowledgeframe?\n    # Return the knowledgeframe\n    knowledgeframe = mk.KnowledgeFrame(list_of_lists[0], list_of_lists[1:])\n    return knowledgeframe", "error": "", "token_usage": [[182, 109]], "time_usage": [3.537055015563965]}
{"task_id": "PandasEval/61", "success": true, "generated_code": "unionerd_kf = mk.union(kf1, kf2, left_index=True, right_index=True)", "error": "", "token_usage": [[183, 26]], "time_usage": [1.4830734729766846]}
{"task_id": "PandasEval/62", "success": true, "generated_code": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'a': [0, 1], 'b': [5, 3]})\n# How to obtain monkey KnowledgeFrame without index\n# I want to print the whole knowledgeframe, but I don't want to print the index\nkf_string = kf.to_string(index=False)\nprint(kf_string)", "error": "", "token_usage": [[164, 77]], "time_usage": [2.844918966293335]}
{"task_id": "PandasEval/63", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef sip_all_nan_rows(kf):\n    # We will sip all Nan rows.\n    # Return the changed knowledgeframe.\n    kf = kf.dropna(how='all')\n    return kf", "error": "", "token_usage": [[137, 54]], "time_usage": [2.0023646354675293]}
{"task_id": "PandasEval/64", "success": true, "generated_code": "import monkey as mk\n\ndef is_contain_particular_value(collections, value):\n    # How to determine whether a Monkey Column contains a particular value?\n    # Return the result\n    return value in collections", "error": "", "token_usage": [[139, 47]], "time_usage": [1.9128496646881104]}
{"task_id": "PandasEval/65", "success": true, "generated_code": "import monkey as mk\n\ndef rename_column(kf, old_name, new_name):\n    # How would I rename the only one column header?\n    # return the changed knowledgeframe\n    kf.columns = [new_name if col == old_name else col for col in kf.columns]\n    return kf", "error": "", "token_usage": [[139, 65]], "time_usage": [2.4694294929504395]}
{"task_id": "PandasEval/66", "success": true, "generated_code": "import monkey as mk\n\ndef remove_duplicates_by_column(kf, col1, col2):\n    # I have a knowledgeframe with repeat values in column `col1`. I want to sip duplicates, keeping the row with the last value in column `col2`.\n    # How would I do that?\n    # return the final knowledgeframe\n    \n    # Sort the knowledgeframe by column col1 in descending order\n    kf = kf.sort_values(col1, ascending=False)\n    \n    # Drop duplicates based on column col1, keeping the last occurrence\n    kf = kf.drop_duplicates(subset=col1, keep='last')\n    \n    # Return the modified knowledgeframe\n    return kf", "error": "", "token_usage": [[172, 145]], "time_usage": [4.040646553039551]}
{"task_id": "PandasEval/67", "success": true, "generated_code": "import monkey as mk\n\ndef create_empty_kf(col_names):\n    # Monkey create empty KnowledgeFrame with only column names\n    # Return: KnowledgeFrame\n    return mk.KnowledgeFrame(col_names)", "error": "", "token_usage": [[134, 45]], "time_usage": [1.9069578647613525]}
{"task_id": "PandasEval/68", "success": true, "generated_code": "import monkey as mk\n\ndef delete_first_n_rows(kf, n):\n    # Delete first n rows of a knowledgeframe\n    # Input:\n    #   kf: KnowledgeFrame\n    #   n: int\n    # Return:\n    #   KnowledgeFrame\n    kf = kf[n:]\n    return kf", "error": "", "token_usage": [[157, 69]], "time_usage": [2.760610580444336]}
{"task_id": "PandasEval/69", "success": true, "generated_code": "import monkey as mk\n\ndef remove_duplicates_by_col_names(kf):\n    \"\"\"\n    Here's a one solution to remove columns based on duplicate column names:\n    Return the duplicated knowledgeframe\n    \n    :param kf: the input knowledgeframe\n    :type kf: monkey.KnowledgeFrame\n    :return: the knowledgeframe with removed duplicate columns\n    :rtype: monkey.KnowledgeFrame\n    \"\"\"\n    # Get the column names of the knowledgeframe\n    col_names = kf.get_column_names()\n    \n    # Check for duplicate column names\n    duplicates = set([col_name for col_name in col_names if col_names.count(col_name) > 1])\n    \n    # Remove the duplicate columns\n    kf.remove_columns(duplicates)\n    \n    # Return the modified knowledgeframe\n    return kf", "error": "", "token_usage": [[142, 166]], "time_usage": [5.021798849105835]}
{"task_id": "PandasEval/70", "success": true, "generated_code": "import monkey as mk\n\ndef convert_bool_to_int(kf, col_name):\n    # How can I map True/False to 1/0 in a Monkey KnowledgeFrame?\n    # return the knowledgeframe with the column converted to int\n    kf[col_name] = kf[col_name].astype(int)\n    return kf", "error": "", "token_usage": [[151, 70]], "time_usage": [2.5684053897857666]}
{"task_id": "PandasEval/71", "success": true, "generated_code": "import monkey as mk\n\ndef get_number_columns(kf):\n    # How do I retrieve the number of columns in a Monkey data frame?\n    # Return the number of columns in the knowledgeframe\n    return kf.get_number_of_columns()", "error": "", "token_usage": [[142, 52]], "time_usage": [2.044696569442749]}
{"task_id": "PandasEval/72", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef find_columns_name_lists(kf):\n    # How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?\n    # Return a list of the column names containing NaNs\n    \n    # Get the column names\n    column_names = kf.columns.tolist()\n    \n    # Check for NaN values in each column\n    nan_columns = [col for col in column_names if kf[col].isnull().any()]\n    \n    return nan_columns", "error": "", "token_usage": [[161, 115]], "time_usage": [3.8193161487579346]}
{"task_id": "PandasEval/73", "success": true, "generated_code": "def get_last_n_rows(kf, n):\n    \"\"\"\n    Returns the last N rows of a monkey KnowledgeFrame.\n    \n    Args:\n        kf (monkey.KnowledgeFrame): The monkey KnowledgeFrame.\n        n (int): The number of rows to retrieve.\n    \n    Returns:\n        monkey.KnowledgeFrame: The last N rows of the monkey KnowledgeFrame.\n    \"\"\"\n    return kf.tail(n)\n    \nresult = get_last_n_rows(kf, N)", "error": "", "token_usage": [[169, 97]], "time_usage": [3.371652126312256]}
{"task_id": "PandasEval/74", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef replacing_blank_with_nan(kf):\n    # replace field that's entirely space (or empty) with NaN using regex\n    kf = kf.replace(r'^\\s*$', np.nan, regex=True)\n    # return the result\n    return kf", "error": "", "token_usage": [[143, 66]], "time_usage": [2.3895390033721924]}
{"task_id": "PandasEval/75", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef fill_none_with_zero(kf, col_names):\n    # Monkey knowledgeframe fillnone() only some columns in place\n    # This function fills all columns with 0\n    # Return the changed knowledgeframe\n    \n    # Fill none values with 0 for specified columns\n    kf.fillnone(col_names, value=0)\n    \n    # Return the modified knowledgeframe\n    return kf", "error": "", "token_usage": [[156, 94]], "time_usage": [3.216063976287842]}
{"task_id": "PandasEval/76", "success": true, "generated_code": "import monkey as mk\n\ndef concating_kf(kf1, kf2):\n    # Given that all the knowledgeframes have the same columns, you can simply concat them:\n    return mk.concat([kf1, kf2])", "error": "", "token_usage": [[147, 51]], "time_usage": [2.170625925064087]}
{"task_id": "PandasEval/77", "success": true, "generated_code": "import monkey as mk\n\ndef extract_first_and_last_kf(kf):\n    \"\"\"\n    Extracts the first and last row of a knowledgeframe in monkey.\n    Returns the knowledgeframe with the first and last row.\n    \"\"\"\n    first_row = kf.iloc[0]\n    last_row = kf.iloc[-1]\n    extracted_kf = kf.iloc[[0, -1]]\n    \n    return extracted_kf", "error": "", "token_usage": [[144, 88]], "time_usage": [3.0716376304626465]}
{"task_id": "PandasEval/78", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef display_rows_with_gt_1_nan(kf):\n    # Return the knowledgeframe with the rows with one or more NaN values\n    \n    # Check if kf is a KnowledgeFrame object\n    if not isinstance(kf, mk.KnowledgeFrame):\n        raise TypeError(\"kf must be a KnowledgeFrame object\")\n    \n    # Find the rows with one or more NaN values\n    rows_with_nan = np.isnan(kf.data).any(axis=1)\n    \n    # Return the knowledgeframe with the rows with NaN values\n    return kf[rows_with_nan]", "error": "", "token_usage": [[139, 127]], "time_usage": [3.9281716346740723]}
{"task_id": "PandasEval/79", "success": true, "generated_code": "import monkey as mk\n\ndef get_row_index_values_as_list(kf):\n    # Return the row-index values of the knowledgeframe as a list\n    return list(kf.index)", "error": "", "token_usage": [[132, 41]], "time_usage": [2.079699754714966]}
{"task_id": "PandasEval/80", "success": true, "generated_code": "# I find myself often having to check whether a column or row exists in a knowledgeframe before trying to reference it.\n# Is there any way to do this more nicely? \n# For example on an arbitrary object I can do x = getattr(anobject, 'id', default) - is there anything similar to this in monkey? Really any way to achieve what I'm doing more gracefully?\n\n# Output the second row of data in `mycol` column if it exists, otherwise output NaN\nif 'mycol' in kf.columns:\n    value = kf['mycol'].iloc[1]\nelse:\n    value = np.nan", "error": "", "token_usage": [[237, 133]], "time_usage": [4.661703109741211]}
{"task_id": "PandasEval/81", "success": true, "generated_code": "import monkey as mk\n\ndef counting_occurrences_of_a_value(collections, value):\n    # Count the number of occurrences of a value in a collections\n    # Return the count\n    count = 0\n    for item in collections:\n        if item == value:\n            count += 1\n    return count", "error": "", "token_usage": [[139, 69]], "time_usage": [2.686042308807373]}
{"task_id": "PandasEval/82", "success": true, "generated_code": "import monkey as mk\n\ndef find_col_a_gt_col_b_rows(kf, col_a, col_b):\n    # Find rows in kf where col_a > col_b\n    # Return the rows\n    rows = []\n    for row in kf:\n        if row[col_a] > row[col_b]:\n            rows.append(row)\n    return rows", "error": "", "token_usage": [[143, 75]], "time_usage": [2.521223306655884]}
{"task_id": "PandasEval/83", "success": true, "generated_code": "import monkey as mk\n\ndef sip_consecutive_duplicates(collections):\n    # Drop consecutive duplicates\n    collections = [x for i, x in enumerate(collections) if i == 0 or x != collections[i-1]]\n    # Return the result\n    return collections", "error": "", "token_usage": [[127, 60]], "time_usage": [2.452413320541382]}
{"task_id": "PandasEval/84", "success": true, "generated_code": "import monkey as mk\n\ndef value_round_a_single_column(kf):\n    # Round a single column `A`\n    # Return the knowledgeframe\n    kf['A'] = kf['A'].round()\n    return kf", "error": "", "token_usage": [[132, 50]], "time_usage": [2.112717628479004]}
{"task_id": "PandasEval/85", "success": true, "generated_code": "import monkey as mk\n\ndef add_zeros_to_string(kf, col_name):\n    # Add Leading Zeros to Strings at `col_name` in Monkey Dataframe\n    # The maximum length of the string is 15\n    # Return the knowledgeframe\n    kf[col_name] = kf[col_name].apply(lambda x: str(x).zfill(15))\n    return kf", "error": "", "token_usage": [[155, 83]], "time_usage": [2.9373440742492676]}
{"task_id": "PandasEval/86", "success": true, "generated_code": "import monkey as mk\n\ndef adding_dict_to_kf(kf, dictionary):\n    # adding dictionary to data frame\n    kf.update(dictionary)\n    return kf", "error": "", "token_usage": [[133, 37]], "time_usage": [1.777036190032959]}
{"task_id": "PandasEval/87", "success": true, "generated_code": "import monkey as mk\n\ndef transform_timestamp_to_pydatetime(timestamp):\n    # transform timestamp to pydatetime object\n    # return pydatetime object\n    pass", "error": "", "token_usage": [[131, 36]], "time_usage": [1.7536799907684326]}
{"task_id": "PandasEval/88", "success": true, "generated_code": "import monkey as mk\n\ndef getting_percentage_of_each_gender(collections):\n    # Given a monkey collections that represents frequencies of a value, how can I turn those frequencies into percentages?\n    # Return the percentage of each gender.\n    \n    total = sum(collections.values())\n    percentages = {}\n    for key, value in collections.items():\n        percentages[key] = (value / total) * 100\n        \n    return percentages", "error": "", "token_usage": [[147, 89]], "time_usage": [3.219604969024658]}
{"task_id": "PandasEval/89", "success": true, "generated_code": "import monkey as mk\n\ndef divide_multiple_cols_by_first_col(kf):\n    # I need to divide all ['B','C'] columns but the first column 'A' in a KnowledgeFrame by the first column.\n    # Return the result.\n    \n    kf[['B', 'C']] = kf[['B', 'C']].div(kf['A'], axis=0)\n    \n    return kf", "error": "", "token_usage": [[154, 86]], "time_usage": [3.038682699203491]}
{"task_id": "PandasEval/90", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef ceiling_of_collections(s):\n    # ceiling of a monkey collections\n    # Return the result.\n    return np.ceil(s)", "error": "", "token_usage": [[132, 40]], "time_usage": [1.5949246883392334]}
{"task_id": "PandasEval/91", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef delete_all_nan_columns(kf):\n    # Delete all columns that contain all NaN values\n    # Return the result.\n    kf = kf.dropna(axis=1, how='all')\n    return kf", "error": "", "token_usage": [[137, 57]], "time_usage": [2.5266995429992676]}
{"task_id": "PandasEval/92", "success": true, "generated_code": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'name': ['jon','sam','jane','bob'],\n           'age': [30,25,18,26],\n           'sex':['male','male','female','male']})\n\nrow = ['45', 'Dean', 'male']\n# add the row at top in kf\nkf.loc[-1] = row\nkf.index = kf.index + 1\n# resort the index by inplace\nkf.sort_index(inplace=True)", "error": "", "token_usage": [[196, 104]], "time_usage": [3.498368740081787]}
{"task_id": "PandasEval/93", "success": true, "generated_code": "import monkey as mk\n\ndef set_value_to_entire_col(kf, value):\n    # Set value to an entire column `B` of a monkey knowledgeframe\n    # Return the changed knowledgeframe.\n    kf['B'] = value\n    return kf", "error": "", "token_usage": [[144, 58]], "time_usage": [2.3192849159240723]}
{"task_id": "PandasEval/94", "success": true, "generated_code": "# Finding the intersection between two collections\n# In detail, first we create two sets, one for each collections.\n# Then we find the intersection of the two sets.\ns1, s2 = set(s1), set(s2)\nintersection_result = s1.intersection(s2)", "error": "", "token_usage": [[185, 61]], "time_usage": [2.421992540359497]}
{"task_id": "PandasEval/95", "success": true, "generated_code": "import monkey as mk\n\ndef getting_first_n_rows(kf, n):\n    # I would simply like to slice the Data Frame and take the first n rows.\n    # Return the result\n    return kf[:n]", "error": "", "token_usage": [[141, 49]], "time_usage": [2.130424737930298]}
{"task_id": "PandasEval/96", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'Apples': [2, 1, np.nan],\n              'Bananas': [3, 3, 7],\n              'Grapes': [np.nan, 2, 3],})\n\n# Add a new column named 'Fruit Total' that sums the values of the other columns\n# Note that ignoring the NaN values\nkf['Fruit Total'] = kf.sum(axis=1, skipna=True)\n\nkf", "error": "", "token_usage": [[192, 111]], "time_usage": [3.600957155227661]}
{"task_id": "PandasEval/97", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\ndef find_non_numeric_rows(kf):\n    # Finding non-numeric rows in knowledgeframe in monkey\n    # Return the raws that contain non-numeric values\n    # So to get the subKnowledgeFrame of rouges, (Note: the negation, ~, of the above finds the ones which have at least one rogue non-numeric):\n    non_numeric_rows = kf[~np.isfinite(kf).all(1)]\n    return non_numeric_rows", "error": "", "token_usage": [[184, 110]], "time_usage": [3.6651196479797363]}
{"task_id": "PandasEval/98", "success": true, "generated_code": "import monkey as mk\n\nkf1 = mk.KnowledgeFrame({'staff':[1,4], 'company':[100,301]})\nkf2 = mk.KnowledgeFrame({'person':[1,2], 'company':[100,300]})\n# unioner the above two knowledgeframes on column 'company'\nunionerd_kf = mk.union(kf1, kf2, 'company')", "error": "", "token_usage": [[167, 81]], "time_usage": [3.1467676162719727]}
{"task_id": "PandasEval/99", "success": true, "generated_code": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'A':[1,4], 'B':[np.nan,301]})\n# # counting the number of missing/NaN in each column\n# Get a collections with the number of missing/NaN in each column\ncount_collections = kf.isnull().sum()", "error": "", "token_usage": [[164, 72]], "time_usage": [2.4967682361602783]}
{"task_id": "PandasEval/100", "success": true, "generated_code": "import monkey as mk\nkf = mk.KnowledgeFrame({'col': [\"apple\",\n                           \"pear\",\n                           \"strawberry\"]})\ntargets = ['apple', 'banana']\n# Any word from `targets` are present in sentence.\nresult = kf.search(targets)", "error": "", "token_usage": [[152, 58]], "time_usage": [2.3532261848449707]}
{"task_id": "PandasEval/34", "success": true, "generated_code": "import monkey as mk\n\ndef f(x):\n    a = x['Value'].iat[2] - x['Value'].iat[1]\n    b = x['Value'].iat[3] - x['Value'].iat[0]\n    c = x['ID'].iat[2] + ' - ' + x['ID'].iat[1]\n    d = x['ID'].iat[3] + ' - ' + x['ID'].iat[0]\n    return mk.KnowledgeFrame({'Value': [a,b], 'ID':[c,d]})\n\ndef calculate_row_diff_groupwise(kf):\n    # I need to calculate the difference between two rows groupwise using monkey.\n    # To calculate the total_sum I would use monkey.grouper('Group').total_sum(), but how do you calculate the difference between rows where the row ordering is important?\n    # I think we need custom function with employ which return KnowledgeFrame for each group, for select by position is used iat:\n    # Return the result\n    return kf.groupby('Group').apply(f)", "error": "", "token_usage": [[313, 225]], "time_usage": [6.900988340377808]}
{"task_id": "PandasEval/27", "success": true, "generated_code": "import monkey as mk\n\ndef normalize(kf):\n    # Normalization using monkey\n    # We simply subtract the average and divide by standard deviation on kf.iloc[:,0,-1] obj with axis is zero.\n    # Return the normalized knowledgeframe\n    \n    normalized_kf = kf.copy()  # create a copy of the knowledgeframe\n    \n    avg = kf.iloc[:, 0, -1].mean()  # calculate the average\n    std_dev = kf.iloc[:, 0, -1].std()  # calculate the standard deviation\n    \n    normalized_kf.iloc[:, 0, -1] = (kf.iloc[:, 0, -1] - avg) / std_dev  # normalize the values\n    \n    return normalized_kf", "error": "", "token_usage": [[153, 156]], "time_usage": [4.551119804382324]}
