{"task_id": "PandasEval/0", "success": true, "generated_code": "# [start]\n# mean(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the mean of the values over the requested axis.\n# [end]\nimport pandas as pd\n\ndef compute_mean_along_rows(df):\n    # You can specify a new column named `mean_along_rows` that contains the mean of each row. You also need to compute the mean along the rows, so use axis=1.\n    # Finally, return the dataframe with the new column. \n    df['mean_along_rows'] = df.mean(axis=1)\n    return df", "error": "", "token_usage": [[208, 129]], "time_usage": [4.342997312545776]}
{"task_id": "PandasEval/1", "success": true, "generated_code": "# [start]\n# isin(self, values) -> 'np.ndarray': Compute boolean array of whether each value is found in the passed set of values.\n# [end]\nimport pandas as pd\n\ndef select_rows_from_column(df, col_name, values):\n    # How do I select rows from a DataFrame df based on column values?\n    # Return rows whose column value named `col_name` is in an iterable `values`\n    return df[df[col_name].isin(values)]", "error": "", "token_usage": [[190, 102]], "time_usage": [3.5858302116394043]}
{"task_id": "PandasEval/2", "success": true, "generated_code": "# [start]\n# rename(self, name, inplace=False): Alter Index or MultiIndex name.\n# [end]\nimport pandas as pd\n\ndef change_col_names_of_df(df, origin_names, new_names):\n    # How do I change the column labels of df\uff1f\n    # And return the dataframe that has been renamed\n    df.rename(columns=dict(zip(origin_names, new_names)), inplace=True)\n    return df", "error": "", "token_usage": [[168, 89]], "time_usage": [3.066427707672119]}
{"task_id": "PandasEval/3", "success": true, "generated_code": "import pandas as pd\n\ndef delete_column(df, column_name):\n    \"\"\"\n    deleting a column from a Pandas DataFrame\n    return the changed dataframe\n    \"\"\"\n    df = df.drop(column_name, axis=1)\n    return df", "error": "", "token_usage": [[172, 53]], "time_usage": [1.8254635334014893]}
{"task_id": "PandasEval/4", "success": true, "generated_code": "import pandas as pd\n\ndef select_multiple_columns(df, columns):\n    # How do I select the given columns and return the new DataFrame?\n    return df[columns]", "error": "", "token_usage": [[130, 38]], "time_usage": [1.7546274662017822]}
{"task_id": "PandasEval/5", "success": true, "generated_code": "import pandas as pd\n\ndef get_row_count(df):\n    \"\"\"\n    Return the row count of df\n    \"\"\"\n    return len(df)", "error": "", "token_usage": [[125, 32]], "time_usage": [1.4499554634094238]}
{"task_id": "PandasEval/6", "success": true, "generated_code": "# [start]\n# tolist(self, *args, **kwargs): Return a list of the values.\n# [end]\nimport pandas as pd\n\ndef get_list_from_dataframe(df):\n    # I want to get a list of the column headers from a Pandas DataFrame. \n    # The DataFrame will come from user input, so I won't know how many columns there will be or what they will be called.\n    # Return a list of the column headers.\n    \n    return df.columns.tolist()", "error": "", "token_usage": [[197, 106]], "time_usage": [3.064007043838501]}
{"task_id": "PandasEval/7", "success": true, "generated_code": "import pandas as pd\n\ndef add_column_to_dataframe(df, column_name, column_data):\n    # How to add a new column to an existing DataFrame?\n    # I would like to add a new column data with the column name, to the existing dataframe\n    df[column_name] = column_data\n    return df", "error": "", "token_usage": [[154, 69]], "time_usage": [2.325746774673462]}
{"task_id": "PandasEval/8", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# to_numeric(arg, errors='raise', downcast=None): Convert argument to a numeric type.\n# [end]\nimport pandas as pd\n\ndef change_all_cols_type(df):\n    # Change all columns type of DataFrame to numeric\n    # And return the new DataFrame\n    # The code is:\n    return df.apply(pd.to_numeric)", "error": "", "token_usage": [[216, 126]], "time_usage": [3.955561876296997]}
{"task_id": "PandasEval/9", "success": true, "generated_code": "# [start]\n# dropna(self): Return ExtensionArray without NA values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef drop_rows_col_nan(df, col_name):\n    # How to drop rows of Pandas DataFrame whose value in a certain column is NaN\n    df = df.dropna(subset=[col_name])\n    return df", "error": "", "token_usage": [[162, 79]], "time_usage": [2.420865058898926]}
{"task_id": "PandasEval/11", "success": true, "generated_code": "# [start]\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# append(self, other: 'Index | Sequence[Index]') -> 'Index': Append a collection of Index options together.\n# [end]\nfrom typing import List\nimport pandas as pd\nimport numpy as np\n\ndef append_in_dataframe(df, list_to_append, column_name_list) -> pd.DataFrame:\n    \"\"\"    \n    Params:\n        df: The dataframe to append to.\n        list_to_append: The list to append.\n        column_name_list: The column names of the list to append.\n\n    Returns:\n        The dataframe with the list appended.\n    \"\"\"\n    df_to_append = pd.DataFrame(list_to_append, columns=column_name_list)\n    return df.append(df_to_append)", "error": "", "token_usage": [[279, 206]], "time_usage": [5.630925178527832]}
{"task_id": "PandasEval/12", "success": true, "generated_code": "def extract_the_last_year(df, column_name):\n    # I am trying to extract the last year (YY) of a fiscal date string in the format of YYYY-YY.\n    # e.g The last year of this '1999-00' would be 2000.\n    # I need a logic to include a case where if it is the end of the century then my apply method should add to the first two digits.\n    # the column_name is the column name of the dataframe that contains the date strings.\n    # return the numerical Series obj of the last year.\n    \n    df[column_name] = pd.to_numeric(df[column_name].str[:4])\n    df[column_name] = df[column_name] + 1\n    \n    return df[column_name]", "error": "", "token_usage": [[253, 160]], "time_usage": [4.931089162826538]}
{"task_id": "PandasEval/13", "success": true, "generated_code": "# [start]\n# tail(self: 'FrameOrSeries', n: 'int' = 5) -> 'FrameOrSeries': Return the last `n` rows.\n# [end]\nimport pandas as pd\n\ndef get_last_n_rows(df, n):\n    # How to get the last N rows of a pandas DataFrame?\n    return df.tail(n)", "error": "", "token_usage": [[170, 78]], "time_usage": [2.5078015327453613]}
{"task_id": "PandasEval/14", "success": true, "generated_code": "import pandas as pd\n\ndef get_values_at_nth_rows(df, n, column_name):\n    \"\"\"\n    how do I get the value at an nth row of a given column name in Pandas?\n    return the value\n    \"\"\"\n    return df[column_name].iloc[n]", "error": "", "token_usage": [[149, 60]], "time_usage": [2.099065065383911]}
{"task_id": "PandasEval/15", "success": true, "generated_code": "# [start]\n# copy(self: '_IndexT', name: 'Hashable | None' = None, deep: 'bool' = False, dtype: 'Dtype | None' = None, names: 'Sequence[Hashable] | None' = None) -> '_IndexT': Make a copy of this object.\n# [end]\nimport pandas as pd\n\ndef creating_df_with_same_as_other(df_original):\n    # creating a new dataframe of all same with df_original one, but no any rows\n    new_df = pd.DataFrame(columns=df_original.columns)\n    return new_df", "error": "", "token_usage": [[215, 126]], "time_usage": [3.71772837638855]}
{"task_id": "PandasEval/20", "success": true, "generated_code": "new_df = df.groupby(['Country', 'Item_Code'])[['Y1961', 'Y1962', 'Y1963']].sum()", "error": "", "token_usage": [[435, 33]], "time_usage": [1.5576298236846924]}
{"task_id": "PandasEval/10", "success": true, "generated_code": "# [start]\n# Series(data=None, index=None, dtype: 'Dtype | None' = None, name=None, copy: 'bool' = False, fastpath: 'bool' = False): One-dimensional ndarray with axis labels (including time series).\n# [end]\nimport pandas as pd\n\n# creating a Series from a list [56, 24, 421, 90]\nmy_series = pd.Series([56, 24, 421, 90])", "error": "", "token_usage": [[186, 103]], "time_usage": [3.696420669555664]}
{"task_id": "PandasEval/16", "success": true, "generated_code": "import pandas as pd\n\ndata = {'col_0': ['a', 'a', 'a', 'a', 'b','b','b'], 'col_1': [-2, -7, 6, 8, -5, 2, 6]}\ndf = pd.DataFrame(data)\n# What I want is to clip the values of `col_1` between -2 to 2 if `col_0` is `a`.\n# # Using `clip` function in pandas.\ndf.loc[df['col_0']=='a','col_1'] = df.loc[df['col_0']=='a','col_1'].clip(-2, 2)", "error": "", "token_usage": [[220, 144]], "time_usage": [4.695363759994507]}
{"task_id": "PandasEval/17", "success": true, "generated_code": "df = df.apply(lambda x: x.where(x > x.mean()).dropna())", "error": "", "token_usage": [[400, 21]], "time_usage": [1.209975242614746]}
{"task_id": "PandasEval/18", "success": true, "generated_code": "target_series = target_series.append(source_series, ignore_index=True)", "error": "", "token_usage": [[234, 17]], "time_usage": [1.002760648727417]}
{"task_id": "PandasEval/19", "success": true, "generated_code": "# Selecting rows where column x2 is NaN \nnan_df = df[df['x2'].isnull()]", "error": "", "token_usage": [[238, 28]], "time_usage": [1.3744385242462158]}
{"task_id": "PandasEval/21", "success": true, "generated_code": "df = pd.DataFrame(a, columns=['one', 'two'])\ndf['two'] = df['two'].astype(float)", "error": "", "token_usage": [[281, 29]], "time_usage": [1.332871675491333]}
{"task_id": "PandasEval/22", "success": true, "generated_code": "cols = my_df.select_dtypes(include=['float64']).columns\nmy_df[cols] = my_df[cols].astype('float32')", "error": "", "token_usage": [[266, 34]], "time_usage": [1.4797568321228027]}
{"task_id": "PandasEval/23", "success": true, "generated_code": "import pandas as pd\n\ndf = pd.DataFrame({'col1': [1,2,3], 'col2': ['Jimmy','Tom','Jimmy']})\n\n# I have a dataframe that has two columns, the second column is one of only a few values.\n# I want to return a dataframe where only the rows where that col2 had a specific value 'Jimmy' are included.\n\ndef filter_dataframe(df):\n    new_df = df[df['col2'] == 'Jimmy']\n    return new_df\n\nnew_df = filter_dataframe(df)", "error": "", "token_usage": [[184, 113]], "time_usage": [3.678253412246704]}
{"task_id": "PandasEval/24", "success": true, "generated_code": "# [start]\n# iterrows(self) -> 'Iterable[tuple[Hashable, Series]]': Iterate over DataFrame rows as (index, Series) pairs.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'MSRA': [10, 11, 12], 'THU': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n# (for index, row in DataFrame.iterrows) is a generator which yields both the index and row (as a Series)\n# for each row in the DataFrame, we need put the row['MSRA'] (as key) and row['THU'] (as value) into a rows_dict\nrows_dict = {} # {MSRA: THU, ...}\n\nfor index, row in df.iterrows():\n    rows_dict[row['MSRA']] = row['THU']\n#", "error": "", "token_usage": [[267, 193]], "time_usage": [6.061615943908691]}
{"task_id": "PandasEval/25", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -> 'FrameOrSeriesUnion': Invoke function on values of Series.\n# min(self, *, skipna=True, **kwargs): The minimum value of the object.\n# max(self, axis=None, skipna: 'bool' = True, *args, **kwargs): Return the maximum value of the Index.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1000, 765, 800], 'B': [10, 5, 7]})\n# I have a dataframe in pandas where each column has different value range.\n# Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?\n\nnormalized_df = df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))", "error": "", "token_usage": [[282, 204]], "time_usage": [7.412308216094971]}
{"task_id": "PandasEval/26", "success": true, "generated_code": "# Now I want to assign the emails to first row and the 'Email' column\ndf.loc[0, 'Email'] = emails", "error": "", "token_usage": [[226, 33]], "time_usage": [1.4290225505828857]}
{"task_id": "PandasEval/28", "success": true, "generated_code": "import pandas as pd\n\ndef is_df_exist(df):\n    # In my code, I have several variables which can either contain a pandas DataFrame or nothing at all.\n    # Let's say I want to test and see if a certain DataFrame has been created yet or not.\n    \n    if isinstance(df, pd.DataFrame):\n        print(\"The DataFrame exists.\")\n    else:\n        print(\"The DataFrame does not exist.\")", "error": "", "token_usage": [[157, 87]], "time_usage": [3.0113914012908936]}
{"task_id": "PandasEval/29", "success": true, "generated_code": "n_df = df[df['line_num'] != 0]", "error": "", "token_usage": [[184, 17]], "time_usage": [1.011028528213501]}
{"task_id": "PandasEval/30", "success": true, "generated_code": "# [start]\n# drop(self, labels, errors: 'str_t' = 'raise') -> 'Index': Make new Index with passed list of labels deleted.\n# [end]\nimport pandas as pd\n\nweb_stats = {'Day': [1, 2, 3, 4, 2, 6],\n             'Visitors': [43, 43, 34, 23, 43, 23],\n             'Bounce_Rate': [3, 2, 4, 3, 5, 5]}\ndf = pd.DataFrame(web_stats)\n# I would like to drop all data in a pandas dataframe\n# Using df.index to drop all rows\n\n# [start]\ndef drop(self, labels, errors: 'str_t' = 'raise') -> 'Index':\n    \"\"\"\n    Make new Index with passed list of labels deleted.\n    \n    Parameters:\n    - labels: A list of labels to be deleted.\n    - errors: Determines how to handle when an element to be deleted does not exist.\n    \n    Returns:\n    - Index: A new Index object with passed labels deleted.\n    \"\"\"\n    new_index = self.copy()\n    try:\n        for label in labels:\n            new_index._data.remove(label)\n    except:\n        if errors == 'raise':\n            raise ValueError(f\"labels {labels} not contained in axis\")\n        elif errors == 'ignore':\n            pass\n    return new_index\n# [end]", "error": "", "token_usage": [[246, 305]], "time_usage": [8.80715799331665]}
{"task_id": "PandasEval/31", "success": true, "generated_code": "import pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n# I would like to add a new column C that is the sum value of A and B cell.\n\ndf['C'] = df['A'] + df['B']", "error": "", "token_usage": [[156, 72]], "time_usage": [2.568545341491699]}
{"task_id": "PandasEval/32", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# isnull(self) -> 'Series': Detect missing values.\n# dropna(self): Return ExtensionArray without NA values.\n# [end]\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': [1, 4, 7, np.nan], 'B': [np.nan, 2, 5, np.nan], 'C': [np.nan, np.nan, 3, 6]})\n# Move next value to first empty row pandas\n# how do i move each value from a column to the first empty \"row/cell\" in pandas?\n# use sorted to align non NULL data at the top, use dropna to drop all rows with all NaN\nnew_df = df.apply(lambda x: sorted(x, key=pd.isnull))\nnew_df = new_df.dropna()", "error": "", "token_usage": [[303, 226]], "time_usage": [6.445942640304565]}
{"task_id": "PandasEval/33", "success": true, "generated_code": "# [start]\n# map(self, mapper, na_action=None): Map values using input correspondence (a dict, Series, or function).\n# [end]\nimport pandas as pd\n\ndef make_dataframe_column_headers_lowercase(data):\n    # I want to make all column headers in my pandas data frame lower case\n    data.columns = map(str.lower, data.columns)", "error": "", "token_usage": [[164, 77]], "time_usage": [2.574498414993286]}
{"task_id": "PandasEval/35", "success": true, "generated_code": "first_value = df['a'].nlargest(1).iloc[0]", "error": "", "token_usage": [[215, 20]], "time_usage": [1.0711159706115723]}
{"task_id": "PandasEval/36", "success": true, "generated_code": "# [start]\n# unique(self: '_IndexT', level: 'Hashable | None' = None) -> '_IndexT': Return unique values in the index.\n# ravel(self, order='C'): Return an ndarray of the flattened values of the underlying data.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(0,10,size=100).reshape(10,10))\n# I have a Pandas dataframe and I want to find all the unique values in that dataframe...irrespective of row/columns. \n# If I have a 10 x 10 dataframe, and suppose they have 84 unique values, I need to find them - Not the count.\n# Using xx.values.ravel to get the flattened array of the dataframe\n# Getting the unique values by numpy.unique\nunique_ndarray = np.unique(df.values.ravel())", "error": "", "token_usage": [[280, 188]], "time_usage": [5.475114822387695]}
{"task_id": "PandasEval/37", "success": true, "generated_code": "# [start]\n# sort_values(self, return_indexer: 'bool' = False, ascending: 'bool' = True, na_position: 'str_t' = 'last', key: 'Callable | None' = None): Return a sorted copy of the index.\n# groupby(self, values) -> 'PrettyDict[Hashable, np.ndarray]': Group the index labels by a given array of values.\n# last(self: 'FrameOrSeries', offset) -> 'FrameOrSeries': Select final periods of time series data based on a date offset.\n# [end]\nimport pandas as pd\ndf = pd.DataFrame({\n    'id': [220, 220, 220, 826, 826, 826, 901, 901, 901],\n    'product': [6647, 6647, 6647, 3380, 3380, 3380, 4555, 4555, 4555],\n    'date': ['2014-09-01', '2014-09-03', '2014-10-16', '2014-11-11', '2014-12-09', '2015-05-19', '2014-09-01', '2014-10-05', '2014-11-01']\n})\n\n# How to group values of pandas dataframe and select the latest by date from each group?\n# Sorting values by `date` (ascending is True), and then grouping by `id`\nlast_df = df.sort_values('date', ascending=True).groupby('id').last()", "error": "", "token_usage": [[420, 338]], "time_usage": [8.611716270446777]}
{"task_id": "PandasEval/38", "success": true, "generated_code": "# [start]\n# drop(self, labels, errors: 'str_t' = 'raise') -> 'Index': Make new Index with passed list of labels deleted.\n# [end]\nimport pandas as pd\n\ndef drop2rows_zero(df):\n    # i want to drop 2 rows in the dataframe if zero comes in the column\n    # if 0 comes on odd index drop previous row as well as current row using pandas\n    # Assuming your dataframe is indexed starting from 0\n    # Rows with column2 = 0 and on odd index\n    idx = df[(df['column2'] == 0) & (df.index % 2 == 1)].index\n    # The rows above them\n    idx = idx.append(idx-1)\n    # A new dataframe with those rows removed\n    df = df.drop(idx, errors='ignore')\n    # [start]\n    # drop(self, labels, errors: 'str_t' = 'raise') -> 'Index': Make new Index with passed list of labels deleted.\n    # [end]\n    return df", "error": "", "token_usage": [[270, 228]], "time_usage": [6.301217079162598]}
{"task_id": "PandasEval/39", "success": true, "generated_code": "# [start]\n# shift(self, periods=1, freq=None): Shift index by desired number of time frequency increments.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef shift_column_up_by_one(df):\n    # Shift column in pandas dataframe up by one?\n    # In detail, in 'gdp' column, shift up by one and return dataframe with the changed gdp column.\n    df['gdp'] = df['gdp'].shift(periods=1)\n    return df", "error": "", "token_usage": [[186, 110]], "time_usage": [3.393763303756714]}
{"task_id": "PandasEval/40", "success": true, "generated_code": "# Select only float64 columns from a DataFrame\nnew_df = df.select_dtypes(include='float64')", "error": "", "token_usage": [[218, 26]], "time_usage": [1.1481835842132568]}
{"task_id": "PandasEval/41", "success": true, "generated_code": "# [start]\n# merge(self, right: 'FrameOrSeriesUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'DataFrame': Merge DataFrame or named Series objects with a database-style join.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef merge_df(df1, df2):\n    # How to merge two dataframes with different column names but same number of rows?\n    # I have two different data frames in pandas. Example:\n    # df1=a b  df2= c\n    # 0 1       1 \n    # 1 2       2 \n    # 2 3       3 \n    # I want to merge them so\n    # df1= a b c  \n    #  0 1 1\n    #  1 2 2\n    #  2 3 3\n    # In order to merge two dataframes you can use this two examples. Both returns the same goal\n    # Using merge plus additional arguments instructing it to use the indexes\n    df1.merge(df2, left_index=True, right_index=True)\n    # Using concat and setting axis=1 to concatenate columns\n    pd.concat([df1, df2], axis=1)", "error": "", "token_usage": [[442, 372]], "time_usage": [9.416991949081421]}
{"task_id": "PandasEval/42", "success": true, "generated_code": "# [start]\n# drop(self, codes, level=None, errors='raise'): Make new MultiIndex with passed list of codes deleted.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 3],'B': [100, 300, 500],'C': list('abc')})\n\n# How can I delete multiple columns in one pass?\n# In detail, I would like to delete columns A and C, but I don't know how to do it in one pass.\nnew_df = df.drop(['A', 'C'], axis=1)", "error": "", "token_usage": [[214, 128]], "time_usage": [3.643371105194092]}
{"task_id": "PandasEval/43", "success": true, "generated_code": "# [start]\n# value_counts(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, dropna: 'bool' = True): Return a Series containing counts of unique values.\n# rename_axis(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False): Set the name of the axis for the index or columns.\n# reset_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, drop: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'DataFrame | None': Reset the index, or a level of it.\n# [end]\nimport pandas as pd\n\ndef get_value_counts(df):\n    # I want to get the counts of unique values of the dataframe. count_values implements this however I want to use its output somewhere else. \n    # How can I convert .count_values output to a pandas dataframe.\n    # Use rename_axis('unique_values') for name ('counts') of column from index and reset_index\n    # return the final dataframe\n    counts = df.value_counts().rename_axis('unique_values').reset_index(name='counts')\n    return counts", "error": "", "token_usage": [[354, 279]], "time_usage": [7.660427093505859]}
{"task_id": "PandasEval/44", "success": true, "generated_code": "data.columns = ['a', 'b', 'c']", "error": "", "token_usage": [[172, 16]], "time_usage": [1.0435125827789307]}
{"task_id": "PandasEval/45", "success": true, "generated_code": "# [start]\n# map(self, mapper, na_action=None): Map values using input correspondence (a dict, Series, or function).\n# [end]\nimport pandas as pd\n\ndef make_df_all_cols_lower(data):\n    # I want to make all column headers in my pandas data frame lower case\n    # Return the changed dataframe\n    return data.rename(columns=str.lower)", "error": "", "token_usage": [[171, 81]], "time_usage": [2.597585678100586]}
{"task_id": "PandasEval/46", "success": true, "generated_code": "sample = df.groupby(\"section\").sample(n=50)", "error": "", "token_usage": [[363, 16]], "time_usage": [1.1093025207519531]}
{"task_id": "PandasEval/47", "success": true, "generated_code": "# [start]\n# replace(old, new, count=-1, /): Return a copy with all occurrences of substring old replaced by new.\n# [end]\nimport pandas as pd\n\n# Example DataFrame\ndf = pd.DataFrame.from_dict({'Name'  : ['May21', 'James', 'Adi22', 'Hello', 'Girl90'],\n                             'Volume': [23, 12, 11, 34, 56],\n                             'Value' : [21321, 12311, 4435, 32454, 654654]})\n\n# Want to remove all the numbers from the Name column.\n# Any idea how to do it in a better way at the series/dataframe level.\ndf['Name'] = df['Name'].str.replace('\\d+', '', regex=True)", "error": "", "token_usage": [[254, 169]], "time_usage": [5.227912664413452]}
{"task_id": "PandasEval/48", "success": true, "generated_code": "new_df = df.groupby('Mt').apply(lambda x: x[x['num'] == x['num'].max()])", "error": "", "token_usage": [[414, 28]], "time_usage": [1.524193525314331]}
{"task_id": "PandasEval/49", "success": true, "generated_code": "# [start]\n# to_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, format: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_format: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Series | DatetimeScalar | NaTType | None': Convert argument to datetime.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({\n'date': [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"friday\"],\n'value': [1, 2, 3, 4]\n})\n\n# transfer column date to datetime type\n# when there is a string that is not capable of beeing turned into datetime format, skip that row,\n# use errors='coerce' for this\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')", "error": "", "token_usage": [[332, 246]], "time_usage": [7.346067667007446]}
{"task_id": "PandasEval/50", "success": true, "generated_code": "# [start]\n# isnull(self) -> 'np.ndarray': Detect missing values Missing values (-1 in.\n# any(self, *args, **kwargs): Return whether any element is Truthy.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef if_any_value_is_nan(df):\n    # How to check if any value is NaN in a Pandas DataFrame? Return the result.\n    return df.isnull().any().any()", "error": "", "token_usage": [[185, 97]], "time_usage": [2.8593664169311523]}
{"task_id": "PandasEval/51", "success": true, "generated_code": "# [start]\n# reindex(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create index with target's values (move/add/delete values as necessary).\n# [end]\nimport pandas as pd\n\ndef sorting_columns_based_on_column_name(df):\n    # Sorting columns in pandas dataframe based on column name\n    # Note that axis is one\n    df = df.reindex(sorted(df.columns), axis=1)\n    return df", "error": "", "token_usage": [[190, 110]], "time_usage": [3.8271217346191406]}
{"task_id": "PandasEval/52", "success": true, "generated_code": "import pandas as pd\nimport numpy as np\n\ndef get_value_when_condition(df):\n    # How can I get the values of column `A` when column `B`=3?\n    values = df[df['B'] == 3]['A']\n    return values", "error": "", "token_usage": [[139, 59]], "time_usage": [2.3171021938323975]}
{"task_id": "PandasEval/53", "success": true, "generated_code": "# [start]\n# mean(self, *args, **kwargs): Return the mean value of the Array.\n# [end]\nimport pandas as pd\n\ndef get_mean_in_column(df, col_name):\n    # return the column average/mean\n    return df[col_name].mean()", "error": "", "token_usage": [[152, 62]], "time_usage": [2.1546950340270996]}
{"task_id": "PandasEval/54", "success": true, "generated_code": "# [start]\n# append(self, other: 'Index | Sequence[Index]') -> 'Index': Append a collection of Index options together.\n# [end]\nimport pandas as pd\n\ndef combine_df(df1, df2):\n    # How do I combine two dataframes with ignore index? Return the concated dataframe.\n    return pd.concat([df1, df2], ignore_index=True)", "error": "", "token_usage": [[168, 85]], "time_usage": [3.0992796421051025]}
{"task_id": "PandasEval/55", "success": true, "generated_code": "# [start]\n# concat(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, copy: 'bool' = True) -> 'FrameOrSeriesUnion': Concatenate pandas objects along a particular axis with optional set logic along the other axes.\n# [end]\nimport pandas as pd\n\n# This is my DataFrame that should be repeated for 5 times:\nx = pd.DataFrame({'a':1,'b':2}, index = range(1))\n# I haven't found anything practical, including those like np.repeat ---- it just doesn't work on a DataFrame.\n# You can use the concat function:\nrepeated_x = pd.concat([x] * 5)", "error": "", "token_usage": [[279, 190]], "time_usage": [5.8584301471710205]}
{"task_id": "PandasEval/56", "success": true, "generated_code": "def dataframe2list_of_dict(df):\n    \"\"\"\n    Pandas DataFrame to List of Dictionaries\n    Use df.to_dict() to solve it and return the result\n    \"\"\"\n    return df.to_dict(orient='records')", "error": "", "token_usage": [[175, 50]], "time_usage": [1.8868837356567383]}
{"task_id": "PandasEval/57", "success": true, "generated_code": "# [start]\n# to_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, format: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_format: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Series | DatetimeScalar | NaTType | None': Convert argument to datetime.\n# [end]\nimport pandas as pd\n\ndef convert_column_to_date(df):\n    # Convert Column `Date` to Date Format using pandas function\n    return pd.to_datetime(df['Date'])", "error": "", "token_usage": [[265, 168]], "time_usage": [5.515223741531372]}
{"task_id": "PandasEval/58", "success": true, "generated_code": "# [start]\n# groupby(self, values) -> 'PrettyDict[Hashable, np.ndarray]': Group the index labels by a given array of values.\n# shift(self, periods=1, freq=None): Shift index by desired number of time frequency increments.\n# cumsum(self, axis=None, skipna=True, *args, **kwargs): Return cumulative sum over a DataFrame or Series axis.\n# [end]\nimport pandas as pd\n\ndef counting_consecutive_positive_values(y):\n    # Counting consecutive positive values in Python/pandas array\n    # I'm trying to count consecutive up days in equity return data; so if a positive day is 1 and a negative is 0, a list y=[0,0,1,1,1,0,0,1,0,1,1] should return z=[0,0,1,2,3,0,0,1,0,1,2].\n    z = [0] * len(y) # Initialize the result list with zeros\n    count = 0 # Initialize the count variable\n    for i in range(len(y)):\n        if y[i] == 1: # If the value is positive\n            count += 1 # Increment the count\n            z[i] = count # Assign the count to the corresponding index in the result list\n        else:\n            count = 0 # Reset the count if the value is not positive\n    return z # Return the result", "error": "", "token_usage": [[303, 308]], "time_usage": [9.109736919403076]}
{"task_id": "PandasEval/59", "success": true, "generated_code": "# [start]\n# append(self, to_append, ignore_index: 'bool' = False, verify_integrity: 'bool' = False): Concatenate two or more Series.\n# sort_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'last', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Sort object by labels (along an axis).\n# reset_index(self, level=None, drop=False, name=None, inplace=False): Generate a new DataFrame or Series with the index reset.\n# [end]\nimport pandas as pd\n\ndef insert_row_at_arbitrary_in_dataframe(df, row_to_insert):\n    \"\"\"\n    Inserts a row into a dataframe at a specified row with no ignore index, and sort & reset the index with drop=True.\n    Returns the new dataframe.\n    \"\"\"\n    df = df.append(row_to_insert, ignore_index=False)\n    df.sort_index(inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    return df", "error": "", "token_usage": [[334, 268]], "time_usage": [7.451244831085205]}
{"task_id": "PandasEval/60", "success": true, "generated_code": "# [start]\n# DataFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None): Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n# [end]\nimport pandas as pd\n\ndef get_data_frame_from_list(list_of_lists):\n    # list_of_lists format: [header, [row1], [row2], ...]\n    # header format: [column1, column2, ...]\n    # row format: [value1, value2, ...]\n    # How to convert list to dataframe?\n    df = pd.DataFrame(data=list_of_lists[1:], columns=list_of_lists[0])\n    # Return the dataframe\n    return df", "error": "", "token_usage": [[247, 172]], "time_usage": [5.3366875648498535]}
{"task_id": "PandasEval/61", "success": true, "generated_code": "df1 = pd.DataFrame({'a': [0, 1], 'b': [5, 3]})\ndf2 = pd.DataFrame({'c': [0, 1], 'd': [10, 20]})\n\n# How do I merge two dataframes by index?\n# Set left&right indexs to True\nmerged_df = df1.merge(df2, left_index=True, right_index=True)", "error": "", "token_usage": [[323, 89]], "time_usage": [3.126157283782959]}
{"task_id": "PandasEval/62", "success": true, "generated_code": "# [start]\n# to_string(self, buf: 'FilePathOrBuffer[str] | None' = None, columns: 'Sequence[str] | None' = None, col_space: 'int | None' = None, header: 'bool | Sequence[str]' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatters: 'fmt.FormattersType | None' = None, float_format: 'fmt.FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, max_rows: 'int | None' = None, min_rows: 'int | None' = None, max_cols: 'int | None' = None, show_dimensions: 'bool' = False, decimal: 'str' = '.', line_width: 'int | None' = None, max_colwidth: 'int | None' = None, encoding: 'str | None' = None) -> 'str | None': Render a DataFrame to a console-friendly tabular output.\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [0, 1], 'b': [5, 3]})\n# How to obtain pandas DataFrame without index\n# I want to print the whole dataframe, but I don't want to print the index\ndf_string = df.to_string(index=False)", "error": "", "token_usage": [[397, 305]], "time_usage": [8.631400108337402]}
{"task_id": "PandasEval/63", "success": true, "generated_code": "# [start]\n# dropna(self: '_IndexT', how: 'str_t' = 'any') -> '_IndexT': Return Index without NA/NaN values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef drop_all_nan_rows(df):\n    # We will drop all Nan rows.\n    # Return the changed dataframe.\n    return df.dropna(how='all')", "error": "", "token_usage": [[175, 87]], "time_usage": [3.339409828186035]}
{"task_id": "PandasEval/64", "success": true, "generated_code": "# [start]\n# unique(self: '_IndexT', level: 'Hashable | None' = None) -> '_IndexT': Return unique values in the index.\n# [end]\nimport pandas as pd\n\ndef is_contain_particular_value(series, value):\n    # How to determine whether a Pandas Column contains a particular value?\n    # Return the result\n    return value in series.unique()", "error": "", "token_usage": [[178, 87]], "time_usage": [3.1071298122406006]}
{"task_id": "PandasEval/65", "success": true, "generated_code": "# [start]\n# rename(self, name, inplace=False): Alter Index or MultiIndex name.\n# [end]\nimport pandas as pd\n\ndef rename_column(df, old_name, new_name):\n    # How would I rename the only one column header?\n    df.rename(columns={old_name: new_name}, inplace=True)\n    return df", "error": "", "token_usage": [[161, 74]], "time_usage": [2.513259172439575]}
{"task_id": "PandasEval/66", "success": true, "generated_code": "# [start]\n# drop_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Return Index with duplicate values removed.\n# [end]\nimport pandas as pd\n\ndef remove_duplicates_by_column(df, col1, col2):\n    # I have a dataframe with repeat values in column `col1`. I want to drop duplicates, keeping the row with the last value in column `col2`.\n    df.drop_duplicates(subset=col1, keep='last', inplace=True)\n    return df", "error": "", "token_usage": [[210, 118]], "time_usage": [3.7748262882232666]}
{"task_id": "PandasEval/67", "success": true, "generated_code": "def create_empty_df(col_names):\n    # Pandas create empty DataFrame with only column names\n    # Return: DataFrame\n    return pd.DataFrame(columns=col_names)", "error": "", "token_usage": [[199, 38]], "time_usage": [1.6822311878204346]}
{"task_id": "PandasEval/68", "success": true, "generated_code": "import pandas as pd\n\ndef delete_first_n_rows(df, n):\n    # Delete first n rows of a dataframe\n    # Input:\n    #   df: DataFrame\n    #   n: int\n    # Return:\n    #   DataFrame\n    return df[n:]", "error": "", "token_usage": [[153, 60]], "time_usage": [2.1128921508789062]}
{"task_id": "PandasEval/69", "success": true, "generated_code": "# [start]\n# duplicated(self, keep: \"Literal[('first', 'last', False)]\" = 'first') -> 'np.ndarray': Indicate duplicate index values.\n# [end]\nimport pandas as pd\n\ndef remove_duplicates_by_col_names(df):\n    \"\"\"\n    Here's a one solution to remove columns based on duplicate column names:\n    Return the duplicated dataframe\n    \"\"\"\n    return df.loc[:, ~df.columns.duplicated()]", "error": "", "token_usage": [[181, 94]], "time_usage": [3.0914359092712402]}
{"task_id": "PandasEval/70", "success": true, "generated_code": "# [start]\n# astype(self, dtype, copy: 'bool' = True) -> 'Index': Create an Index with values cast to dtypes.\n# [end]\nimport pandas as pd\n\ndef convert_bool_to_int(df, col_name):\n    # How can I map True/False to 1/0 in a Pandas DataFrame?\n    # return the dataframe with the column converted to int\n    df[col_name] = df[col_name].astype(int)\n    return df", "error": "", "token_usage": [[186, 105]], "time_usage": [3.3668081760406494]}
{"task_id": "PandasEval/71", "success": true, "generated_code": "# [start]\n# len(self): Compute the length of each element in the Series/Index.\n# [end]\nimport pandas as pd\n\ndef get_number_columns(df):\n    # How do I retrieve the number of columns in a Pandas data frame?\n    # Return the number of columns in the dataframe\n    return len(df.columns)", "error": "", "token_usage": [[164, 72]], "time_usage": [2.547563076019287]}
{"task_id": "PandasEval/72", "success": true, "generated_code": "# [start]\n# isna(self) -> 'np.ndarray': Detect missing values.\n# any(self, *args, **kwargs): Return whether any element is Truthy.\n# tolist(self): Return a list of the values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef find_columns_name_lists(df):\n    # How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?\n    # Return a list of the column names containing NaNs\n    return df.columns[df.isna().any()].tolist()", "error": "", "token_usage": [[212, 126]], "time_usage": [3.7383036613464355]}
{"task_id": "PandasEval/73", "success": true, "generated_code": "result = df.tail(N)", "error": "", "token_usage": [[206, 88]], "time_usage": [2.95591139793396]}
{"task_id": "PandasEval/74", "success": true, "generated_code": "# [start]\n# replace(self, to_replace, value, inplace: 'bool' = False): Replaces all instances of one value with another.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef replacing_blank_with_nan(df):\n    # replace field that's entirely space (or empty) with NaN using regex\n    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n    # return the result\n    return df", "error": "", "token_usage": [[177, 101]], "time_usage": [3.278339147567749]}
{"task_id": "PandasEval/75", "success": true, "generated_code": "def fill_none_with_zero(df, col_names):\n    # Pandas dataframe fillna() only some columns in place\n    # This function fills all columns with 0\n    # Return the changed dataframe\n    df[col_names] = df[col_names].fillna(0)\n    return df", "error": "", "token_usage": [[184, 64]], "time_usage": [2.3192338943481445]}
{"task_id": "PandasEval/76", "success": true, "generated_code": "# [start]\n# concat(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, copy: 'bool' = True) -> 'FrameOrSeriesUnion': Concatenate pandas objects along a particular axis with optional set logic along the other axes.\n# [end]\nimport pandas as pd\n\ndef concat_df(df1, df2):\n    # Given that all the dataframes have the same columns, you can simply concat them:\n    return pd.concat([df1, df2])", "error": "", "token_usage": [[247, 152]], "time_usage": [4.740549802780151]}
{"task_id": "PandasEval/77", "success": true, "generated_code": "import pandas as pd\n\ndef extract_first_and_last_df(df):\n    # Extract first and last row of a dataframe in pandas\n    # Return the dataframe with the first and last row\n    \n    first_row = df.head(1)\n    last_row = df.tail(1)\n    \n    return pd.concat([first_row, last_row])", "error": "", "token_usage": [[140, 72]], "time_usage": [2.619107961654663]}
{"task_id": "PandasEval/78", "success": true, "generated_code": "# [start]\n# isna(obj): Detect missing values for an array-like object.\n# any(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs): Return whether any element is True, potentially over an axis.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef display_rows_with_gt_1_nan(df):\n    # Return the dataframe with the rows with one or more NaN values\n    return df[df.isna().any(axis=1)]", "error": "", "token_usage": [[193, 107]], "time_usage": [3.4402480125427246]}
{"task_id": "PandasEval/79", "success": true, "generated_code": "def get_row_index_values_as_list(df):\n    # Return the row-index values of the dataframe as a list\n    return df.index.tolist()", "error": "", "token_usage": [[150, 33]], "time_usage": [1.4319777488708496]}
{"task_id": "PandasEval/80", "success": true, "generated_code": "# [start]\n# get(self, i): Extract element from each component at specified position.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'mycol':np.arange(5), 'dummy':np.arange(5)})\n# I find myself often having to check whether a column or row exists in a dataframe before trying to reference it.\n# Is there any way to do this more nicely? \n# For example on an arbitrary object I can do x = getattr(anobject, 'id', default) - is there anything similar to this in pandas? Really any way to achieve what I'm doing more gracefully?\n# Output the second row of data in `mycol` column if it exists, otherwise output NaN\nvalue = df['mycol'].get(1, np.nan)", "error": "", "token_usage": [[257, 171]], "time_usage": [5.14042854309082]}
{"task_id": "PandasEval/81", "success": true, "generated_code": "# [start]\n# value_counts(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, dropna: 'bool' = True): Return a Series containing counts of unique values.\n# [end]\nimport pandas as pd\n\ndef counting_occurrences_of_a_value(series, value):\n    # Count the number of occurrences of a value in a series\n    # Return the count\n    return series.value_counts()[value]", "error": "", "token_usage": [[196, 106]], "time_usage": [3.116183042526245]}
{"task_id": "PandasEval/82", "success": true, "generated_code": "import pandas as pd\n\ndef find_col_a_gt_col_b_rows(df, col_a, col_b):\n    # Find rows in df where col_a > col_b\n    # Return the rows\n    return df[df[col_a] > df[col_b]]", "error": "", "token_usage": [[142, 56]], "time_usage": [2.048164129257202]}
{"task_id": "PandasEval/83", "success": true, "generated_code": "import pandas as pd\n\ndef drop_consecutive_duplicates(series):\n    # Drop consecutive duplicates\n    series = series.shift(periods=1, freq=None)\n    # Return the result\n    return series", "error": "", "token_usage": [[155, 45]], "time_usage": [1.7693214416503906]}
{"task_id": "PandasEval/84", "success": true, "generated_code": "# [start]\n# round(freq, ambiguous='raise', nonexistent='raise'): Round the Timestamp to the specified resolution.\n# [end]\nimport pandas as pd\n\ndef round_a_single_column(df):\n    # Round a single column `A`\n    # Return the dataframe\n    df['A'] = df['A'].round()\n    return df", "error": "", "token_usage": [[157, 75]], "time_usage": [2.662729024887085]}
{"task_id": "PandasEval/85", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -> 'FrameOrSeriesUnion': Invoke function on values of Series.\n# format(self, name: 'bool' = False, formatter: 'Callable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Render a string representation of the Index.\n# [end]\nimport pandas as pd\n\ndef add_zeros_to_string(df, col_name):\n    # Add Leading Zeros to Strings at `col_name` in Pandas Dataframe\n    # The maximum length of the string is 15\n    # Return the dataframe\n    df[col_name] = df[col_name].apply(lambda x: str(x).zfill(15))\n    return df", "error": "", "token_usage": [[258, 186]], "time_usage": [5.0968780517578125]}
{"task_id": "PandasEval/86", "success": true, "generated_code": "# [start]\n# append(self, other: 'Index | Sequence[Index]') -> 'Index': Append a collection of Index options together.\n# [end]\nimport pandas as pd\n\ndef append_dict_to_df(df, dictionary):\n    # append dictionary to data frame\n    # return the data frame\n    return df.append(dictionary, ignore_index=True)", "error": "", "token_usage": [[164, 76]], "time_usage": [2.621367931365967]}
{"task_id": "PandasEval/87", "success": true, "generated_code": "# [start]\n# to_pydatetime(*args, **kwargs): Convert a Timestamp object to a native Python datetime object.\n# [end]\nimport pandas as pd\n\ndef transform_timestamp_to_pydatetime(timestamp):\n    # transform timestamp to pydatetime object\n    return timestamp.to_pydatetime()", "error": "", "token_usage": [[160, 62]], "time_usage": [2.1981966495513916]}
{"task_id": "PandasEval/88", "success": true, "generated_code": "# [start]\n# value_counts(self, dropna: 'bool' = True): Return a Series containing counts of unique values.\n# [end]\nimport pandas as pd\n\ndef get_percentage_of_each_gender(series):\n    # Given a pandas series that represents frequencies of a value, how can I turn those frequencies into percentages?\n    # Return the percentage of each gender.\n    \n    total_count = series.sum()\n    percentage_series = series / total_count * 100\n    \n    return percentage_series", "error": "", "token_usage": [[177, 104]], "time_usage": [3.236023187637329]}
{"task_id": "PandasEval/89", "success": true, "generated_code": "# [start]\n# div(self, other, axis='columns', level=None, fill_value=None): Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n# [end]\nimport pandas as pd\n\ndef divide_multiple_cols_by_first_col(df):\n    # I need to divide all ['B','C'] columns but the first column 'A' in a DataFrame by the first column.\n    # Return the result.\n    df[['B', 'C']] = df[['B', 'C']].div(df['A'], axis='index')\n    return df", "error": "", "token_usage": [[194, 125]], "time_usage": [4.378662586212158]}
{"task_id": "PandasEval/90", "success": true, "generated_code": "# [start]\n# ceil(self, *args, **kwargs): Perform ceil operation on the data to the specified `freq`.\n# [end]\nimport pandas as pd\nimport numpy as np\ndef ceil_of_series(s):\n    # ceiling of a pandas series\n    # Return the result.\n    return np.ceil(s)", "error": "", "token_usage": [[162, 70]], "time_usage": [2.261467456817627]}
{"task_id": "PandasEval/91", "success": true, "generated_code": "# [start]\n# dropna(self): Return ExtensionArray without NA values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef delete_all_nan_columns(df):\n    # Delete all columns that contain all NaN values\n    # Return the result.\n    return df.dropna(axis=1, how='all')", "error": "", "token_usage": [[156, 71]], "time_usage": [2.3152647018432617]}
{"task_id": "PandasEval/92", "success": true, "generated_code": "# [start]\n# sort_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'last', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Sort object by labels (along an axis).\n# [end]\nimport pandas as pd\n\ndf = pd.DataFrame({'name': ['jon','sam','jane','bob'],\n           'age': [30,25,18,26],\n           'sex':['male','male','female','male']})\n\nrow = ['45', 'Dean', 'male']\n# add the row at top in df\ndf.loc[-1] = row\ndf.index = df.index + 1\n# resort the index by inplace\ndf.sort_index(inplace=True)", "error": "", "token_usage": [[308, 216]], "time_usage": [5.82009220123291]}
{"task_id": "PandasEval/93", "success": true, "generated_code": "# [start]\n# assign(self, **kwargs) -> 'DataFrame': Assign new columns to a DataFrame.\n# [end]\nimport pandas as pd\n\ndef set_value_to_entire_col(df, value):\n    # Set value to an entire column `B` of a pandas dataframe\n    # Return the changed dataframe.\n    df['B'] = value\n    return df", "error": "", "token_usage": [[167, 81]], "time_usage": [2.900379180908203]}
{"task_id": "PandasEval/94", "success": true, "generated_code": "# [start]\n# intersection(self, other, sort=False): Form the intersection of two Index objects.\n# [end]\nimport pandas as pd\n\ns1 = pd.Series([3,4,5])\ns2 = pd.Series([1,2,3,5])\n# Finding the intersection between two series\n# In detail, first we create two sets, one for each series.\n# Then we find the intersection of the two sets.\ns1, s2 = set(s1), set(s2)\nintersection_result = s1.intersection(s2)", "error": "", "token_usage": [[209, 117]], "time_usage": [3.8330020904541016]}
{"task_id": "PandasEval/95", "success": true, "generated_code": "# [start]\n# head(self: 'FrameOrSeries', n: 'int' = 5) -> 'FrameOrSeries': Return the first `n` rows.\n# [end]\nimport pandas as pd\n\ndef get_first_n_rows(df, n):\n    # I would simply like to slice the Data Frame and take the first n rows.\n    # Return the result\n    return df.head(n)", "error": "", "token_usage": [[180, 88]], "time_usage": [2.814082384109497]}
{"task_id": "PandasEval/96", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# sum(self, axis: 'int' = 0, min_count: 'int' = 0, *args, **kwargs) -> 'Scalar': Sum of non-NA/null values.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Apples': [2, 1, np.nan],\n              'Bananas': [3, 3, 7],\n              'Grapes': [np.nan, 2, 3],})\n\n# Add a new column named 'Fruit Total' that sums the values of the other columns\n# Note that igonring the NaN values\ndf['Fruit Total'] = df.sum(axis=1, skipna=True)", "error": "", "token_usage": [[288, 207]], "time_usage": [5.707400321960449]}
{"task_id": "PandasEval/97", "success": true, "generated_code": "# [start]\n# applymap(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame': Apply a function to a Dataframe elementwise.\n# all(self, *args, **kwargs): Return whether all elements are Truthy.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndef find_non_numeric_rows(df):\n    # Finding non-numeric rows in dataframe in pandas\n    # Return the raws that contain non-numeric values\n    # So to get the subDataFrame of rouges, (Note: the negation, ~, of the above finds the ones which have at least one rogue non-numeric):\n    return df.select_dtypes(exclude=[np.number])", "error": "", "token_usage": [[247, 161]], "time_usage": [4.654635906219482]}
{"task_id": "PandasEval/98", "success": true, "generated_code": "# [start]\n# merge(self, right: 'FrameOrSeriesUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'DataFrame': Merge DataFrame or named Series objects with a database-style join.\n# [end]\nimport pandas as pd\n\ndf1 = pd.DataFrame({'staff':[1,4], 'company':[100,301]})\ndf2 = pd.DataFrame({'person':[1,2], 'company':[100,300]})\n# merge the above two dataframes on column 'company'\nmerged_df = df1.merge(df2, on='company')", "error": "", "token_usage": [[307, 219]], "time_usage": [5.922564268112183]}
{"task_id": "PandasEval/99", "success": true, "generated_code": "# [start]\n# isnull(self) -> 'np.ndarray': Detect missing values.\n# sum(self, axis=None, skipna=None, level=None, numeric_only=None, min_count=0, **kwargs): Return the sum of the values over the requested axis.\n# [end]\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A':[1,4], 'B':[np.nan,301]})\n# # counting the number of missing/NaN in each column\n# Get a series with the number of missing/NaN in each column\ncount_series = df.isnull().sum()", "error": "", "token_usage": [[221, 130]], "time_usage": [3.941473960876465]}
{"task_id": "PandasEval/100", "success": true, "generated_code": "result = df['col'].isin(targets)", "error": "", "token_usage": [[181, 14]], "time_usage": [1.0167655944824219]}
{"task_id": "PandasEval/34", "success": true, "generated_code": "# [start]\n# groupby(self, values) -> 'PrettyDict[Hashable, np.ndarray]': Group the index labels by a given array of values.\n# apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -> 'FrameOrSeriesUnion': Invoke function on values of Series.\n# reset_index(self, level=None, drop=False, name=None, inplace=False): Generate a new DataFrame or Series with the index reset.\n# [end]\nimport pandas as pd\n\ndef f(x):\n    a = x['Value'].iat[2] - x['Value'].iat[1]\n    b = x['Value'].iat[3] - x['Value'].iat[0]\n    c = x['ID'].iat[2] + ' - ' + x['ID'].iat[1]\n    d = x['ID'].iat[3] + ' - ' + x['ID'].iat[0]\n    return pd.DataFrame({'Value': [a,b], 'ID':[c,d]})\n\ndef calculate_row_diff_groupwise(df):\n    # I need to calculate the difference between two rows groupwise using pandas.\n    # To calculate the sum I would use pandas.groupby('Group').sum(), but how do you calculate the difference between rows where the row ordering is important?\n    # I think we need custom function with apply which return DataFrame for each group, for select by position is used iat:\n    # Return the result\n    return df.groupby('Group').apply(f)", "error": "", "token_usage": [[419, 331]], "time_usage": [9.501155376434326]}
{"task_id": "PandasEval/27", "success": true, "generated_code": "# [start]\n# apply(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Apply a function along an axis of the DataFrame.\n# mean(self, *args, **kwargs): Return the mean value of the Array.\n# std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return sample standard deviation over requested axis.\n# [end]\nimport pandas as pd\n\ndef normalize(df):\n    # Normalization using pandas\n    # We simply subtract the mean and divide by standard deviation on df.iloc[:,0,-1] obj with axis is zero.\n    # Return the normalized dataframe\n    return df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)", "error": "", "token_usage": [[260, 183]], "time_usage": [5.783015012741089]}
