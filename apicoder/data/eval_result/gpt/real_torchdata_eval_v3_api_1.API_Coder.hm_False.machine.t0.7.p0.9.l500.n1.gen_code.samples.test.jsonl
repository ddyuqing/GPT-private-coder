{"task_id": "TorchDataEval/0", "completion": " datapipe.flatmap(lambda x: [x] * 6)"}
{"task_id": "TorchDataEval/1", "completion": " enumerate(dp)"}
{"task_id": "TorchDataEval/2", "completion": " next(iter(batch_dp))"}
{"task_id": "TorchDataEval/4", "completion": "# [start]\n# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\n# [end]\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef odd_or_even(n):\n    return n % 2\n\nsource_dp = IterableWrapper(range(5))\n\n# Split into 2 sub-datapipes by the odd_or_even function\ndp1 = IterableWrapper(filter(lambda x: odd_or_even(x) == 1, source_dp))\ndp2 = IterableWrapper(filter(lambda x: odd_or_even(x) == 0, source_dp))"}
{"task_id": "TorchDataEval/5", "completion": " fork(source_dp, 2)\n\ndef fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000):\n    \"\"\"\n    Creates multiple instances of the same Iterable DataPipe.\n    \n    Args:\n        datapipe (torch.utils.data.dataset.IterDataPipe): The Iterable DataPipe to be cloned.\n        num_instances (int): The number of clones to create.\n        buffer_size (int): The buffer size for each clone. Default is 1000.\n    \n    Returns:\n        tuple: A tuple containing the cloned Iterable DataPipes.\n    \"\"\"\n    cloned_dps = []\n    for _ in range(num_instances):\n        cloned_dps.append(IterableWrapper(datapipe, buffer_size))\n    return tuple(cloned_dps)"}
{"task_id": "TorchDataEval/6", "completion": " dp1.concat(dp2, key=itemgetter(0), merge_fn=merge_fn)"}
{"task_id": "TorchDataEval/7", "completion": " map(dp1, itemgetter(0), input_col=1, output_col=1)"}
{"task_id": "TorchDataEval/9", "completion": " SampleMultiplexer(*weights.items(), seed=0)"}
{"task_id": "TorchDataEval/10", "completion": " source_dp.unzip(3)"}
{"task_id": "TorchDataEval/11", "completion": " IterableWrapper(range(10))\n# Divide datapipes into 3 batches and discard if the last batch is not reached.\ndp ="}
{"task_id": "TorchDataEval/12", "completion": " batch(source_dp, batch_size=3, drop_last=True, wrapper_class=List)"}
{"task_id": "TorchDataEval/14", "completion": " source_dp.groupby(group_fn, buffer_size=3, group_size=3, guaranteed_group_size=2, drop_remaining=False)"}
{"task_id": "TorchDataEval/16", "completion": " IterableWrapper([file_url]) >> HttpReader()"}
{"task_id": "TorchDataEval/17", "completion": " source_dp.flatmap(multiple_fn)"}
{"task_id": "TorchDataEval/18", "completion": "# [start]\n# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\n# [end]\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef add_one(x):\n    return x + 1\n\ndp = IterableWrapper(range(10))\n\n# Method 1\nmap_dp_1 = dp.map(add_one)  # Invocation via functional form is preferred\n\n# Method 2\n# We discourage the usage of `lambda` functions as they are not serializable with `pickle`\n# Using `lambda` to implement add_two rather than add_one that is mentioned in above.\nmap_dp_2 = dp.map(lambda x: x + 2)"}
{"task_id": "TorchDataEval/19", "completion": " filter(dp, is_even)"}
{"task_id": "TorchDataEval/20", "completion": " dp.header(limit=3)"}
{"task_id": "TorchDataEval/21", "completion": " batch(dp, batch_size=1, drop_last=False, wrapper_class=List)\n\ndef batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List):\n    \"\"\"\n    Creates mini-batches of data.\n    \n    Args:\n        datapipe: The input DataPipe.\n        batch_size: The size of each mini-batch.\n        drop_last: Whether to drop the last incomplete batch if it's smaller than batch_size.\n        wrapper_class: The class to wrap each mini-batch.\n    \n    Returns:\n        A new DataPipe that produces mini-batches of data.\n    \"\"\"\n    pass"}
{"task_id": "TorchDataEval/23", "completion": " map_dp_1.batch(2)"}
{"task_id": "TorchDataEval/24", "completion": " HttpReader(URL)"}
{"task_id": "TorchDataEval/25", "completion": " HttpReader(URL) | map(lambda_func_)"}
{"task_id": "TorchDataEval/26", "completion": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data."}
{"task_id": "TorchDataEval/27", "completion": " IterableWrapper(range(3))\n# Augument the datapipe with repeat three times and sample the data.\ndp = dp.repeat(3).sample(Sampler(3))"}
{"task_id": "TorchDataEval/28", "completion": " dp1.concat(dp2).repeat(3)"}
{"task_id": "TorchDataEval/29", "completion": " zip_with_iter(dp1, dp2, itemgetter(0), itemgetter(0), True, 10000, merge_fn)"}
{"task_id": "TorchDataEval/30", "completion": " zip_with_iter(dp1, dp2, itemgetter(0), itemgetter(0), True, merge_fn=merge_fn)\nfor i, item in enumerate(res_dp):\n    print(i, item)"}
{"task_id": "TorchDataEval/31", "completion": " zip_with_iter(dp1, dp2, itemgetter(0), itemgetter(0), True, merge_fn=merge_fn)\nres_dp = res_dp.as_list(1)[0]"}
{"task_id": "TorchDataEval/32", "completion": " zip_with_iter(dp1, mapdp, itemgetter(0), itemgetter(0), False, 10000, merge_fn)"}
{"task_id": "TorchDataEval/33", "completion": " zip_with_map(dp1, mapdp, itemgetter(0), merge_fn)\nres_dp = res_dp.repeat(3)\nres_dp = res_dp.as_list()\nres_dp = list(map(itemgetter(1), res_dp))"}
{"task_id": "TorchDataEval/34", "completion": " zip_with_iter(dp1, mapdp, itemgetter(0), keep_key=True, merge_fn=merge_fn)\nres_dp = res_dp.repeat(3)\nres_dp = res_dp.sample(Sampler(buffer_size=buffer_size))\nres_dp = res_dp.as_list()\nres_dp = [x[2] for x in res_dp]"}
{"task_id": "TorchDataEval/35", "completion": " groupby(source_dp, group_fn)"}
{"task_id": "TorchDataEval/37", "completion": " collate(ds, collate_fn)"}
{"task_id": "TorchDataEval/38", "completion": " collate(ds, collate_fn)"}
{"task_id": "TorchDataEval/40", "completion": " demux(source_dp, 2, great_than_5)"}
{"task_id": "TorchDataEval/41", "completion": " SampleMultiplexer(*weight_)"}
{"task_id": "TorchDataEval/42", "completion": " zip(*raw_dp)"}
{"task_id": "TorchDataEval/43", "completion": " IterableWrapper([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"])\n# Make a batch operation on the datapipe `dp` of size 3 with droping last batch if it is not full.\n# And then get the first two batches.\ndp = batch(dp, 3, True)\nbatch_1 = next(dp)\nbatch_2 = next(dp)"}
{"task_id": "TorchDataEval/44", "completion": " dp1.batch(4, drop_last=True, wrapper_class=List).take(2) + dp2"}
{"task_id": "TorchDataEval/45", "completion": " concat(dp_source_1, dp_source_2).enumerate(start=1).rename_fields('Ids')"}
{"task_id": "TorchDataEval/46", "completion": " dp_source_1.concat(dp_source_2).enumerate(start=1, key='Ids').tee(3)"}
{"task_id": "TorchDataEval/47", "completion": " dp_source_1.concat(dp_source_2, dp_source_3).enumerate()"}
{"task_id": "TorchDataEval/48", "completion": " source_dp.header().flatmap(flatted_func)"}
{"task_id": "TorchDataEval/49", "completion": " HttpReader([AG_NEWS_CSV_URL])"}
{"task_id": "TorchDataEval/3", "completion": " dp_one.concat(dp_two)"}
{"task_id": "TorchDataEval/8", "completion": " list(dp1.header(3)) + list(dp2.header(3))"}
{"task_id": "TorchDataEval/13", "completion": " collate(ds, collate_fn=int2tensor)"}
{"task_id": "TorchDataEval/15", "completion": " unbatch(source_dp)"}
{"task_id": "TorchDataEval/22", "completion": " StreamReader(dp, chunk=1)"}
{"task_id": "TorchDataEval/39", "completion": " [remove_final_number(s) for s in dp_dog] + [remove_final_number(s) for s in dp_cat]"}
{"task_id": "TorchDataEval/36", "completion": " groupby(source_dp, group_key_fn=group_fn)"}
