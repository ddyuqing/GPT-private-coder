{"task_id": "PandasEval/0", "completion": "\n    kf['average_along_rows'] = kf.mean(axis=1)\n    return kf"}
{"task_id": "PandasEval/1", "completion": "\n    return kf[kf[col_name].isin(values)]"}
{"task_id": "PandasEval/2", "completion": "\n# [start]\n# renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.\n# [end]\nimport monkey as mk\n\ndef change_col_names_of_kf(kf, origin_names, new_names):\n    \"\"\"\n    Change the column labels of kf and return the renamed knowledgeframe.\n    \n    Args:\n        kf (KnowledgeFrame): The knowledgeframe whose column labels need to be changed.\n        origin_names (list): A list of original column labels.\n        new_names (list): A list of new column labels.\n        \n    Returns:\n        KnowledgeFrame: The knowledgeframe with renamed column labels.\n    \"\"\"\n    kf.renaming(dict(zip(origin_names, new_names)))\n    return kf"}
{"task_id": "PandasEval/3", "completion": "\n    kf.drop(column_name, axis=1, inplace=True)\n    return kf"}
{"task_id": "PandasEval/4", "completion": "\n    return kf.select_columns(columns)"}
{"task_id": "PandasEval/5", "completion": "\n    return kf.row_count()"}
{"task_id": "PandasEval/6", "completion": "\n    return kf.columns.tolist()"}
{"task_id": "PandasEval/7", "completion": "\n    kf[column_name] = column_data\n    return kf"}
{"task_id": "PandasEval/8", "completion": "\n    kf.employ(mk.to_num, axis=1)\n    return kf"}
{"task_id": "PandasEval/9", "completion": " kf[kf[col_name].isna()].sipna()"}
{"task_id": "PandasEval/11", "completion": "\n    # Adding the list to the knowledgeframe\n    kf.add(list_to_add, columns=column_name_list)\n    \n    # Returning the updated knowledgeframe\n    return kf"}
{"task_id": "PandasEval/12", "completion": "\n    return mk.to_num(kf[column_name].str.split('-').str[0]) + 1"}
{"task_id": "PandasEval/13", "completion": "\n    return kf.last_tail(n)"}
{"task_id": "PandasEval/14", "completion": "\n    return kf[column_name][n]"}
{"task_id": "PandasEval/15", "completion": "\n    return kf_original.clone()"}
{"task_id": "PandasEval/20", "completion": " kf.grouper(by=['Country', 'Item_Code']).total_sum(numeric_only=True, axis=1, skipna=True)"}
{"task_id": "PandasEval/10", "completion": " mk.Collections([56, 24, 421, 90])"}
{"task_id": "PandasEval/16", "completion": " kf.loc[kf['col_0']=='a','col_1'].clip(-2, 2)"}
{"task_id": "PandasEval/17", "completion": " mk.KnowledgeFrame({'a': [4, 1, 7, 3], 'b': [5, 2, 9, 6], 'c': [6, 3, 2, 8]})\n\n# I would like to create a new KnowledgeFrame out of the old one in a way that there will only be values that exceed the average value of the column.\n# We can compare values and then add NaNs by indexing or `where`.\n# We want to remove NaNs also in first rows, add custom function with `sipna`.\n\n# Calculate the average value of each column\navg_values = kf.average()\n\n# Create a new KnowledgeFrame with values that exceed the average value of each column\nnew_kf = kf.where(kf > avg_values)\n\n# Remove NaN values from the new KnowledgeFrame\nnew_kf = new_kf.sipna()"}
{"task_id": "PandasEval/18", "completion": " target_collections.adding(source_collections)"}
{"task_id": "PandasEval/19", "completion": " kf.ifnull('x2')"}
{"task_id": "PandasEval/21", "completion": " mk.KnowledgeFrame(a, columns=['one', 'two'])\nkf['two'] = kf['two'].astype(float)"}
{"task_id": "PandasEval/22", "completion": " my_kf.choose_dtypes(include='float64').columns.tolist()\nmy_kf[cols] = my_kf[cols].totype(np.float32)"}
{"task_id": "PandasEval/23", "completion": " kf[kf['col2'] == 'Jimmy']"}
{"task_id": "PandasEval/24", "completion": "\n# Complete the code\nfor index, row in kf.traversal():\n    rows_dict[row['MSRA']] = row['THU']"}
{"task_id": "PandasEval/25", "completion": " kf.copy()\n    for column in normalized_kf.columns:\n        min_val = kf[column].get_min()\n        max_val = kf[column].get_max()\n        normalized_kf[column] = (kf[column] - min_val) / (max_val - min_val)\n    return normalized_kf\n\nnormalized_kf = normalize_columns(kf)"}
{"task_id": "PandasEval/26", "completion": "\n# Assigning emails to first row and 'Email' column\nkf['Email'][0] = emails"}
{"task_id": "PandasEval/28", "completion": "\n    if kf is not None:\n        return True\n    else:\n        return False"}
{"task_id": "PandasEval/29", "completion": " kf[kf['line_num'] != 0]"}
{"task_id": "PandasEval/30", "completion": "\nkf.sip(kf.index)"}
{"task_id": "PandasEval/31", "completion": "\nkf['C'] = kf['A'] + kf['B']"}
{"task_id": "PandasEval/32", "completion": " kf.employ(lambda x: sorted(x, key=lambda y: pd.isnull(y)), axis=0).sipna()"}
{"task_id": "PandasEval/33", "completion": "\n    data.columns = map(str.lower, data.columns)"}
{"task_id": "PandasEval/35", "completion": " kf.nbiggest(1).iloc[0]['a']"}
{"task_id": "PandasEval/36", "completion": " np.unique(kf.values.flat_underlying())"}
{"task_id": "PandasEval/37", "completion": " kf.sort_the_values(ascending=True, key='date').grouper(by='id').final_item(-1)"}
{"task_id": "PandasEval/38", "completion": "\n    new_kf = kf.drop(idx)\n    return new_kf"}
{"task_id": "PandasEval/39", "completion": "\n    kf['gdp'] = kf['gdp'].shift(periods=1)\n    return kf"}
{"task_id": "PandasEval/40", "completion": " kf.choose_dtypes(include=np.float64)"}
{"task_id": "PandasEval/41", "completion": "\n    # Using the unioner function with the 'how' parameter set to 'outer'\n    # This will perform a full outer join, including all rows from both knowledgeframes\n    kf_union = kf1.unioner(kf2, how='outer')\n    \n    return kf_union"}
{"task_id": "PandasEval/42", "completion": " kf.drop(['A', 'C'], axis=1)"}
{"task_id": "PandasEval/43", "completion": "\n    counts = kf.count_values()\n    counts_kf = mk.KnowledgeFrame(counts)\n    counts_kf.renaming_axis('distinctive_values', columns='counts', inplace=True)\n    final_kf = counts_kf.reseting_index(inplace=False)\n    return final_kf"}
{"task_id": "PandasEval/44", "completion": " ['a', 'b', 'c']"}
{"task_id": "PandasEval/45", "completion": "\n    return data.mapping(lambda x: x.lower(), na_action=None)"}
{"task_id": "PandasEval/46", "completion": " kf.grouper(by='section').sample_by_num(n=50)"}
{"task_id": "PandasEval/47", "completion": " kf['Name'].str.replace('\\d+', '', regex=True)"}
{"task_id": "PandasEval/48", "completion": " kf.grouper(by='Mt').employ(kf.get_max, axis=0)"}
{"task_id": "PandasEval/49", "completion": " pd.to_datetime(kf['date'], errors='coerce')"}
{"task_id": "PandasEval/50", "completion": "\n    return kf.ifnull().whatever().any()"}
{"task_id": "PandasEval/51", "completion": "\n    kf = kf.reindex(sorted(kf.columns), axis=1)\n    return kf"}
{"task_id": "PandasEval/52", "completion": "\n    # You can use the following code to achieve this:\n    values = kf[kf['B'] == 3]['A']\n    return values"}
{"task_id": "PandasEval/53", "completion": "\n    return kf[col_name].mean()"}
{"task_id": "PandasEval/54", "completion": "\n    return kf1.concat(kf2, ignore_index=True)"}
{"task_id": "PandasEval/55", "completion": " mk.concatenating([x]*5, axis=0, ignore_index=True)"}
{"task_id": "PandasEval/56", "completion": "\n    return kf.convert_dict()"}
{"task_id": "PandasEval/57", "completion": "\n    mk.convert_datetime(kf['Date'], formating='%Y-%m-%d')\n    return kf"}
{"task_id": "PandasEval/58", "completion": "\n# [start]\n# grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.\n# shifting(self, periods=1, freq=None): Increase the number of time frequency increments by the required number.\n# cumulative_sum(self, axis=None, skipna=True, *args, **kwargs): Return the cumulative total of an axis in the KnowledgeFrame or Collections.\n# [end]\nimport monkey as mk\n\ndef counting_consecutive_positive_values(y):\n    # Counting consecutive positive values in Python/monkey array\n    # I'm trying to count consecutive up days in equity return data; so if a positive day is 1 and a negative is 0, a list y=[0,0,1,1,1,0,0,1,0,1,1] should return z=[0,0,1,2,3,0,0,1,0,1,2].\n    z = []\n    count = 0\n    for value in y:\n        if value > 0:\n            count += 1\n        else:\n            count = 0\n        z.append(count)\n    return z"}
{"task_id": "PandasEval/59", "completion": "\n    # Insert the row into the knowledgeframe\n    kf = kf.append(row_to_insert, ignore_index=False)\n    \n    # Sort the knowledgeframe by labels along the specified axis\n    kf.sorting_index(inplace=True)\n    \n    # Reset the index of the knowledgeframe and use the default one instead\n    kf.reseting_index(sip=True, inplace=True)\n    \n    return kf"}
{"task_id": "PandasEval/60", "completion": "\n    return mk.KnowledgeFrame(data=list_of_lists[1:], columns=list_of_lists[0])"}
{"task_id": "PandasEval/61", "completion": " kf1.unioner(kf2, left_index=True, right_index=True)"}
{"task_id": "PandasEval/62", "completion": " kf.convert_string(index=False)"}
{"task_id": "PandasEval/63", "completion": "\n    return kf.sipna()"}
{"task_id": "PandasEval/64", "completion": "\n    return value in collections"}
{"task_id": "PandasEval/65", "completion": "\n# [start]\n# renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.\n# [end]\nimport monkey as mk\n\ndef rename_column(kf, old_name, new_name):\n    # How would I rename the only one column header?\n    kf.renaming({old_name: new_name}, inplace=True)\n    return kf"}
{"task_id": "PandasEval/66", "completion": "\n# [start]\n# remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index.\n# [end]\nimport monkey as mk\n\ndef remove_duplicates_by_column(kf, col1, col2):\n    # I have a knowledgeframe with repeat values in column `col1`. I want to sip duplicates, keeping the row with the last value in column `col2`.\n    # How would I do that?\n    kf.drop_duplicates(subset=col1, keep='last', inplace=True)\n    return kf"}
{"task_id": "PandasEval/67", "completion": "\n    return mk.KnowledgeFrame(columns=col_names)"}
{"task_id": "PandasEval/68", "completion": "\n    #   n: int\n    # Return:\n    #   KnowledgeFrame\n    return kf.drop(kf.index[:n])"}
{"task_id": "PandasEval/69", "completion": "\n    # Get the column names\n    col_names = kf.columns\n    \n    # Find the duplicate column names\n    duplicate_cols = set([x for x in col_names if col_names.count(x) > 1])\n    \n    # Remove the duplicate columns\n    kf = kf.drop(columns=duplicate_cols)\n    \n    return kf"}
{"task_id": "PandasEval/70", "completion": "\n    kf[col_name] = kf[col_name].astype(int)\n    return kf"}
{"task_id": "PandasEval/71", "completion": "\n    return kf.length()"}
{"task_id": "PandasEval/72", "completion": "\n    return kf.columns[kf.isna().any()].tolist()"}
{"task_id": "PandasEval/73", "completion": " kf.last_tail(N)"}
{"task_id": "PandasEval/74", "completion": "\n    return kf.replace(r'^\\s*$', np.nan, regex=True)"}
{"task_id": "PandasEval/75", "completion": "\n    kf.fillnone(0, downcast=None)\n    return kf"}
{"task_id": "PandasEval/76", "completion": "\n# [start]\n# concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.\n# [end]\nimport monkey as mk\n\ndef concating_kf(kf1, kf2):\n    # Given that all the knowledgeframes have the same columns, you can simply concat them:\n    return mk.concat(kf1, kf2, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, clone=True)"}
{"task_id": "PandasEval/77", "completion": "\n    first_row = kf.iloc[0]\n    last_row = kf.iloc[-1]\n    return kf.iloc[[0, -1]]"}
{"task_id": "PandasEval/78", "completion": "\n    return kf.ifna().whatever().sum(axis=1) > 1"}
{"task_id": "PandasEval/79", "completion": "\n    return kf.index.tolist()"}
{"task_id": "PandasEval/80", "completion": " kf.getting(1) if 'mycol' in kf.columns() else np.nan"}
{"task_id": "PandasEval/81", "completion": "\ns of distinctive values.\n# [end]\nimport monkey as mk\n\ndef counting_occurrences_of_a_value(collections, value):\n    # Count the number of occurrences of a value in a collections\n    # Return the count\n    count = collections.count(value)\n    return count"}
{"task_id": "PandasEval/82", "completion": "\n    return kf[kf[col_a] > kf[col_b]]"}
{"task_id": "PandasEval/83", "completion": "\n    return collections"}
{"task_id": "PandasEval/84", "completion": "\n    return mk.value_round(kf, ambiguous='raise', nonexistent='raise')"}
{"task_id": "PandasEval/85", "completion": "\n    kf[col_name] = kf[col_name].apply(lambda x: x.zfill(15))\n    return kf"}
{"task_id": "PandasEval/86", "completion": "\n    kf.update(dictionary)\n    return kf"}
{"task_id": "PandasEval/87", "completion": "\n    return mk.convert_pydatetime(timestamp)"}
{"task_id": "PandasEval/88", "completion": "\n    # Calculate the total count of all genders\n    total_count = sum(collections.values())\n    \n    # Calculate the percentage of each gender\n    percentages = {gender: (count / total_count) * 100 for gender, count in collections.items()}\n    \n    return percentages"}
{"task_id": "PandasEval/89", "completion": "\n    return kf[['B', 'C']].div(kf['A'], axis='columns')"}
{"task_id": "PandasEval/90", "completion": "\n    return mk.ceiling(s)"}
{"task_id": "PandasEval/91", "completion": "\n    kf = kf.dropna(axis=1, how='all')\n    return kf"}
{"task_id": "PandasEval/92", "completion": "\nkf.sort_index(inplace=True)"}
{"task_id": "PandasEval/93", "completion": "\n    kf['B'] = value\n    return kf"}
{"task_id": "PandasEval/94", "completion": " s1.intersection(s2)"}
{"task_id": "PandasEval/95", "completion": "\n    return kf.header_num(n)"}
{"task_id": "PandasEval/96", "completion": "\n# [start]\n# employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.\n# total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis.\n# [end]\nimport monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'Apples': [2, 1, np.nan],\n              'Bananas': [3, 3, 7],\n              'Grapes': [np.nan, 2, 3],})\n\n# Add a new column named 'Fruit Total' that sums the values of the other columns\n# Note that ignoring the NaN values\n\ndef total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs):\n    pass\n\nkf.total_sum = total_sum"}
{"task_id": "PandasEval/97", "completion": "\n    return kf.total_all(~np.isfinite(kf.conduct_map(np.isreal)))"}
{"task_id": "PandasEval/98", "completion": " kf1.unioner(kf2, on='company')"}
{"task_id": "PandasEval/99", "completion": " kf.ifnull().total_sum()"}
{"task_id": "PandasEval/100", "completion": " kf.incontain(targets)"}
{"task_id": "PandasEval/34", "completion": "\n    return kf.grouper('Group').employ(f)"}
{"task_id": "PandasEval/27", "completion": "\n    kf.employ(mk.average, axis=0)\n    kf.employ(mk.standard, axis=0)\n    \n    return kf"}
