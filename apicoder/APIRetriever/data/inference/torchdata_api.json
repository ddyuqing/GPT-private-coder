{"text_id": 1400000, "text": "batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data."}
{"text_id": 1400001, "text": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data."}
{"text_id": 1400002, "text": "bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket."}
{"text_id": 1400003, "text": "parse_csv_as_dict(*args, **kwds): Accepts a DataPipe consists of tuples of file name and CSV data stream, reads and returns the contents within the CSV files one row at a time."}
{"text_id": 1400004, "text": "parse_csv(source_datapipe: IterDataPipe[Tuple[str, IO]], *, skip_lines: int = 0, decode: bool = True, encoding: str = 'utf-8', errors: str = 'ignore', return_path: bool = False, **fmtparams): Accepts a DataPipe consists of tuples of file name and CSV data stream, reads and returns the contents within the CSV files one row at a time."}
{"text_id": 1400005, "text": "collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function."}
{"text_id": 1400006, "text": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes."}
{"text_id": 1400007, "text": "concat(*args, **kwds): Concatenate multiple Map DataPipes."}
{"text_id": 1400008, "text": "cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times."}
{"text_id": 1400009, "text": "DataFrameMaker(source_dp: torch.utils.data.dataset.IterDataPipe[~T_co], dataframe_size: int = 1000, dtype=None, columns: Union[List[str], NoneType] = None, device: str = ''): Takes rows of data, batches a number of them together and creates `TorchArrow` DataFrames (functional name: ``dataframe``)."}
{"text_id": 1400010, "text": "Decompressor(*args, **kwds): Takes tuples of path and compressed stream of data, and returns tuples of path and decompressed stream of data (functional name: ``decompress``)."}
{"text_id": 1400011, "text": "types(value, names=None, *, module=None, qualname=None, type=None, start=1): An enumeration."}
{"text_id": 1400012, "text": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function."}
{"text_id": 1400013, "text": "EndOnDiskCacheHolder(datapipe, mode='wb', filepath_fn=None, *, same_filepath_fn=False, skip_read=False): Indicates when the result of prior DataPipe will be saved local files specified by ``filepath_fn`` (functional name: ``end_caching``)."}
{"text_id": 1400014, "text": "enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default."}
{"text_id": 1400015, "text": "Extractor(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Tuple[str, io.IOBase]], file_type: Union[str, torchdata.datapipes.iter.util.decompressor.CompressionType, NoneType] = None): Please use ``Decompressor`` or ``."}
{"text_id": 1400016, "text": "FSSpecFileLister(*args, **kwds): Lists the contents of the directory at the provided ``root`` pathname or URL, and yields the full pathname or URL for each file within the directory."}
{"text_id": 1400017, "text": "FSSpecFileOpener(*args, **kwds): Opens files from input datapipe which contains `fsspec` paths and yields a tuple of pathname and opened file stream (functional name: ``open_file_by_fsspec``)."}
{"text_id": 1400018, "text": "FSSpecSaver(*args, **kwds): Takes in a DataPipe of tuples of metadata and data, saves the data to the target path (generated by the filepath_fn and metadata), and yields the resulting `fsspec` path (functional name: ``save_by_fsspec``)."}
{"text_id": 1400019, "text": "FileLister(*args, **kwds): Given path(s) to the root directory, yields file pathname(s) (path + filename) of files within the root directory."}
{"text_id": 1400020, "text": "FileOpener(*args, **kwds): Given pathnames, opens files and yield pathname and file stream in a tuple."}
{"text_id": 1400021, "text": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``."}
{"text_id": 1400022, "text": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe."}
{"text_id": 1400023, "text": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe."}
{"text_id": 1400024, "text": "GDriveReader(*args, **kwds): Takes URLs pointing at GDrive files, and yields tuples of file name and IO stream."}
{"text_id": 1400025, "text": "groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined."}
{"text_id": 1400026, "text": "HashChecker(*args, **kwds): Computes and checks the hash of each file, from an input DataPipe of tuples of file name and data/stream (functional name: ``check_hash``)."}
{"text_id": 1400027, "text": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit."}
{"text_id": 1400028, "text": "HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream."}
{"text_id": 1400029, "text": "InMemoryCacheHolder(*args, **kwds): Stores elements from the source DataPipe in memory, up to a size limit if specified (functional name: ``in_memory_cache``)."}
{"text_id": 1400030, "text": "add_index(*args, **kwds): Adds an index to an existing Iterable DataPipe with."}
{"text_id": 1400031, "text": "IoPathFileLister(*args, **kwds): Lists the contents of the directory at the provided ``root`` pathname or URL, and yields the full pathname or URL for each file within the directory."}
{"text_id": 1400032, "text": "IoPathFileOpener(*args, **kwds): Opens files from input datapipe which contains pathnames or URLs, and yields a tuple of pathname and opened file stream (functional name: ``open_file_by_iopath``)."}
{"text_id": 1400033, "text": "IoPathSaver(*args, **kwds): Takes in a DataPipe of tuples of metadata and data, saves the data to the target path which is generated by the ``filepath_fn`` and metadata, and yields the resulting path in `iopath` format (functional name: ``save_by_iopath``)."}
{"text_id": 1400034, "text": "IterDataPipe(*args, **kwds): Iterable-style DataPipe."}
{"text_id": 1400035, "text": "zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key."}
{"text_id": 1400036, "text": "IterableWrapper(iterable, deepcopy=True): Wraps an iterable object to create an IterDataPipe."}
{"text_id": 1400037, "text": "JsonParser(*args, **kwds): Reads from JSON data streams and yields a tuple of file name and JSON data (functional name: ``parse_json_files``)."}
{"text_id": 1400038, "text": "LineReader(*args, **kwds): Accepts a DataPipe consisting of tuples of file name and string data stream, and for each line in the stream, yields a tuple of file name and the line (functional name: ``readlines``)."}
{"text_id": 1400039, "text": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe."}
{"text_id": 1400040, "text": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe."}
{"text_id": 1400041, "text": "map(*args, **kwds): Apply the input function over each item from the source DataPipe."}
{"text_id": 1400042, "text": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes."}
{"text_id": 1400043, "text": "OnDiskCacheHolder(*args, **kwds): Caches the outputs of multiple DataPipe operations to local files, which are typically performance bottleneck such download, decompress, and etc (functional name: ``on_disk_cache``)."}
{"text_id": 1400044, "text": "OnlineReader(*args, **kwds): Takes file URLs (can be HTTP URLs pointing to files or URLs to GDrive files), and yields tuples of file URL and IO stream."}
{"text_id": 1400045, "text": "ParagraphAggregator(*args, **kwds): Aggregates lines of text from the same file into a single paragraph (functional name: ``lines_to_paragraphs``)."}
{"text_id": 1400046, "text": "ParquetDataFrameLoader(*args, **kwds): Takes in paths to Parquet files and return a `TorchArrow` DataFrame for each row group within a Parquet file (functional name: ``load_parquet_as_df``)."}
{"text_id": 1400047, "text": "RarArchiveLoader(*args, **kwds): Decompresses rar binary streams from input Iterable Datapipes which contains tuples of path name and rar binary stream, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_rar``)."}
{"text_id": 1400048, "text": "RoutedDecoder(*args, **kwds): Decodes binary streams from input DataPipe, yields pathname and decoded data in a tuple (functional name: ``routed_decode``)."}
{"text_id": 1400049, "text": "rows2columnar(source_datapipe: IterDataPipe[List[Union[Dict, List]]], column_names: Optional[List[str]] = None): Accepts an input DataPipe with batches of data, and processes one batch at a time and yields a Dict for each batch, with ``column_names`` as keys and lists of corresponding values from each row as values."}
{"text_id": 1400050, "text": "SampleMultiplexer(*args, **kwds): Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights."}
{"text_id": 1400051, "text": "Sampler(*args, **kwds): Generates sample elements using the provided ``Sampler`` (defaults to :class:`SequentialSampler`)."}
{"text_id": 1400052, "text": "Saver(*args, **kwds): Takes in a DataPipe of tuples of metadata and data, saves the data to the target path generated by the ``filepath_fn`` and metadata, and yields file path on local file system (functional name: ``save_to_disk``)."}
{"text_id": 1400053, "text": "ShardingFilter(*args, **kwds): Wrapper that allows DataPipe to be sharded (functional name: ``sharding_filter``)."}
{"text_id": 1400054, "text": "Shuffler(*args, **kwds): Shuffles the input DataPipe with a buffer (functional name: ``shuffle``)."}
{"text_id": 1400055, "text": "Shuffler(*args, **kwds): Shuffle the input DataPipe via its indices (functional name: ``shuffle``)."}
{"text_id": 1400056, "text": "StreamReader(datapipe, chunk=None): Given IO streams and their label names, yields bytes with label name in a tuple."}
{"text_id": 1400057, "text": "TarArchiveLoader(*args, **kwds): Opens/decompresses tar binary streams from an Iterable DataPipe which contains tuples of path name and tar binary stream, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_tar``)."}
{"text_id": 1400058, "text": "TarArchiveReader(datapipe: Iterable[Tuple[str, io.BufferedIOBase]], mode: str = 'r:*', length: int = -1): Please use ``TarArchiveLoader`` or ``."}
{"text_id": 1400059, "text": "unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data."}
{"text_id": 1400060, "text": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence."}
{"text_id": 1400061, "text": "XzFileLoader(*args, **kwds): Decompresses xz (lzma) binary streams from an Iterable DataPipe which contains tuples of path name and xy binary streams, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_xz``)."}
{"text_id": 1400062, "text": "XzFileReader(datapipe: Iterable[Tuple[str, io.BufferedIOBase]], length: int = -1): Please use ``XzFileLoader`` or ``."}
{"text_id": 1400063, "text": "ZipArchiveLoader(*args, **kwds): Opens/decompresses zip binary streams from an Iterable DataPipe which contains a tuple of path name and zip binary stream, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_zip``)."}
{"text_id": 1400064, "text": "ZipArchiveReader(datapipe: Iterable[Tuple[str, io.BufferedIOBase]], length: int = -1): Please use ``ZipArchiveLoader`` or ``."}
{"text_id": 1400065, "text": "Zipper(*args, **kwds): Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``)."}
{"text_id": 1400066, "text": "Zipper(*args, **kwds): Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``)."}
{"text_id": 1400067, "text": "IterToMapConverter(*args, **kwds): Lazily load data from ``IterDataPipe`` to construct a ``MapDataPipe`` with the key-value pair generated by ``key_value_fn`` (functional name: ``to_map_datapipe``)."}
{"text_id": 1400068, "text": "MapDataPipe(*args, **kwds): Map-style DataPipe."}
{"text_id": 1400069, "text": "SequenceWrapper(*args, **kwds): Wraps a sequence object into a MapDataPipe."}
{"text_id": 1400070, "text": "StreamWrapper(file_obj): StreamWrapper is introduced to wrap file handler generated by DataPipe operation like `FileOpener`."}
{"text_id": 1400071, "text": "StreamWrapper(file_obj): StreamWrapper is introduced to wrap file handler generated by DataPipe operation like `FileOpener`."}
{"text_id": 1400072, "text": "Tuple(*args, **kwargs): Tuple type; Tuple[X, Y] is the cross-product type of X and Y."}
{"text_id": 1400073, "text": "capitalize(): Return a capitalized version of the string."}
{"text_id": 1400074, "text": "casefold(): Return a version of the string suitable for caseless comparisons."}
{"text_id": 1400075, "text": "center(width, fillchar=' ', /): Return a centered string of length width."}
{"text_id": 1400076, "text": "encode(encoding='utf-8', errors='strict'): Encode the string using the codec registered for encoding."}
{"text_id": 1400077, "text": "expandtabs(tabsize=8): Return a copy where all tab characters are expanded using spaces."}
{"text_id": 1400078, "text": "isalnum(): Return True if the string is an alpha-numeric string, False otherwise."}
{"text_id": 1400079, "text": "isalpha(): Return True if the string is an alphabetic string, False otherwise."}
{"text_id": 1400080, "text": "isascii(): Return True if all characters in the string are ASCII, False otherwise."}
{"text_id": 1400081, "text": "isdecimal(): Return True if the string is a decimal string, False otherwise."}
{"text_id": 1400082, "text": "isdigit(): Return True if the string is a digit string, False otherwise."}
{"text_id": 1400083, "text": "isidentifier(): Return True if the string is a valid Python identifier, False otherwise."}
{"text_id": 1400084, "text": "islower(): Return True if the string is a lowercase string, False otherwise."}
{"text_id": 1400085, "text": "isnumeric(): Return True if the string is a numeric string, False otherwise."}
{"text_id": 1400086, "text": "isprintable(): Return True if the string is printable, False otherwise."}
{"text_id": 1400087, "text": "isspace(): Return True if the string is a whitespace string, False otherwise."}
{"text_id": 1400088, "text": "istitle(): Return True if the string is a title-cased string, False otherwise."}
{"text_id": 1400089, "text": "isupper(): Return True if the string is an uppercase string, False otherwise."}
{"text_id": 1400090, "text": "join(iterable, /): Concatenate any number of strings."}
{"text_id": 1400091, "text": "ljust(width, fillchar=' ', /): Return a left-justified string of length width."}
{"text_id": 1400092, "text": "lower(): Return a copy of the string converted to lowercase."}
{"text_id": 1400093, "text": "lstrip(chars=None, /): Return a copy of the string with leading whitespace removed."}
{"text_id": 1400094, "text": "partition(sep, /): Partition the string into three parts using the given separator."}
{"text_id": 1400095, "text": "replace(old, new, count=-1, /): Return a copy with all occurrences of substring old replaced by new."}
{"text_id": 1400096, "text": "rjust(width, fillchar=' ', /): Return a right-justified string of length width."}
{"text_id": 1400097, "text": "rpartition(sep, /): Partition the string into three parts using the given separator."}
{"text_id": 1400098, "text": "rsplit(sep=None, maxsplit=-1): Return a list of the words in the string, using sep as the delimiter string."}
{"text_id": 1400099, "text": "rstrip(chars=None, /): Return a copy of the string with trailing whitespace removed."}
{"text_id": 1400100, "text": "split(sep=None, maxsplit=-1): Return a list of the words in the string, using sep as the delimiter string."}
{"text_id": 1400101, "text": "splitlines(keepends=False): Return a list of the lines in the string, breaking at line boundaries."}
{"text_id": 1400102, "text": "strip(chars=None, /): Return a copy of the string with leading and trailing whitespace removed."}
{"text_id": 1400103, "text": "swapcase(): Convert uppercase characters to lowercase and lowercase characters to uppercase."}
{"text_id": 1400104, "text": "title(): Return a version of the string where each word is titlecased."}
{"text_id": 1400105, "text": "translate(table, /): Replace each character in the string using the given translation table."}
{"text_id": 1400106, "text": "upper(): Return a copy of the string converted to uppercase."}
{"text_id": 1400107, "text": "zfill(width, /): Pad a numeric string with zeros on the left, to fill a field of the given width."}
